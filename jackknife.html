<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Jackknife Methods | Lewis Cole Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Jackknife Methods" />
<meta name="author" content="Lewis Cole (2020)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog post outlines a general procedure for estimating how accurate a statistic generated by a stochastic model is. By accuracy here we mean model or sampling error as opposed to whether the model is correct." />
<meta property="og:description" content="This blog post outlines a general procedure for estimating how accurate a statistic generated by a stochastic model is. By accuracy here we mean model or sampling error as opposed to whether the model is correct." />
<link rel="canonical" href="https://www.lewiscoleblog.com/jackknife" />
<meta property="og:url" content="https://www.lewiscoleblog.com/jackknife" />
<meta property="og:site_name" content="Lewis Cole Blog" />
<meta property="og:image" content="https://github.com/lewiscoleblog/blog/raw/master/images/Jackknife/Mondrian.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-28T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Lewis Cole (2020)"},"description":"This blog post outlines a general procedure for estimating how accurate a statistic generated by a stochastic model is. By accuracy here we mean model or sampling error as opposed to whether the model is correct.","@type":"BlogPosting","headline":"Jackknife Methods","dateModified":"2020-01-28T00:00:00-06:00","datePublished":"2020-01-28T00:00:00-06:00","image":"https://github.com/lewiscoleblog/blog/raw/master/images/Jackknife/Mondrian.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.lewiscoleblog.com/jackknife"},"url":"https://www.lewiscoleblog.com/jackknife","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.lewiscoleblog.com/feed.xml" title="Lewis Cole Blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Jackknife Methods | Lewis Cole Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Jackknife Methods" />
<meta name="author" content="Lewis Cole (2020)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog post outlines a general procedure for estimating how accurate a statistic generated by a stochastic model is. By accuracy here we mean model or sampling error as opposed to whether the model is correct." />
<meta property="og:description" content="This blog post outlines a general procedure for estimating how accurate a statistic generated by a stochastic model is. By accuracy here we mean model or sampling error as opposed to whether the model is correct." />
<link rel="canonical" href="https://www.lewiscoleblog.com/jackknife" />
<meta property="og:url" content="https://www.lewiscoleblog.com/jackknife" />
<meta property="og:site_name" content="Lewis Cole Blog" />
<meta property="og:image" content="https://github.com/lewiscoleblog/blog/raw/master/images/Jackknife/Mondrian.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-28T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Lewis Cole (2020)"},"description":"This blog post outlines a general procedure for estimating how accurate a statistic generated by a stochastic model is. By accuracy here we mean model or sampling error as opposed to whether the model is correct.","@type":"BlogPosting","headline":"Jackknife Methods","dateModified":"2020-01-28T00:00:00-06:00","datePublished":"2020-01-28T00:00:00-06:00","image":"https://github.com/lewiscoleblog/blog/raw/master/images/Jackknife/Mondrian.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.lewiscoleblog.com/jackknife"},"url":"https://www.lewiscoleblog.com/jackknife","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://www.lewiscoleblog.com/feed.xml" title="Lewis Cole Blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Lewis Cole Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/welcome/">Welcome</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Jackknife Methods</h1><p class="page-description">This blog post outlines a general procedure for estimating how accurate a statistic generated by a stochastic model is. By accuracy here we mean model or sampling error as opposed to whether the model is correct.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-28T00:00:00-06:00" itemprop="datePublished">
        Jan 28, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Lewis Cole (2020)</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#computational-statistics">computational-statistics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#computation">computation</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#confidence-interval">confidence-interval</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Justification-of-the-Method">Justification of the Method </a></li>
<li class="toc-entry toc-h2"><a href="#An-Example">An Example </a></li>
<li class="toc-entry toc-h2"><a href="#Notes-on-Implementation">Notes on Implementation </a></li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-01-28-Jackknife.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this blog post we are concerned with a specific problem: we have a Monte-Carlo type model that produces some simulated output. With this we want to estimate an arbitrary statistic (for example percentiles, expected shortfall or more complicated statistics relating to many variables). We know however that calculated in this way the calculated statistic is just one realisation of a distribution of outcomes. We would like to be able to say something about this distribution, in particular we would like to have some idea of the variability in the statistic.</p>
<p>The "obvious" (and most accurate) way to do this would be to re-run the model many times with different random seeds and create an empirical distribution of the statistic. However if the model is particularly complex this might mean a lot of compute time. Instead we would like to find an approximate method that does not require us to re-run the model at all. To do this in a general way we will introduce the Jackknife method.</p>
<p>It is worth noting that in certain situations other methods can be easier to implement/more accurate, for example if we only wanted to estimate the 90th percentile we can construct an argument using a binomial distribution (and normal approximation thereof) - however this method will not generalise to (for example) expected shortfall or even more complicated statistics.</p>
<h2 id="Justification-of-the-Method">
<a class="anchor" href="#Justification-of-the-Method" aria-hidden="true"><span class="octicon octicon-link"></span></a>Justification of the Method<a class="anchor-link" href="#Justification-of-the-Method"> </a>
</h2>
<p>We begin by supposing we have $N$ un-ordered observations $\{ X_i \}_{i=1}^{N}$ from our Monte-Carlo model. We use these observations to create an estimate of a statistic $\hat{Q}$. We denote the estimate from the model $\hat{Q}_{\{1:N\}}$, which itself is a random variable. Through a Taylor expansion we can note:

$$ \mathbb{E}\left(\hat{Q}_{\{1:N\}}\right) = \hat{Q} + \frac{a_1}{N} + \frac{a_2}{N^2} + ...$$

For some constants $a_x$.</p>
<p>If we now consider partitioning the $N$ observations as: $N = mk$ for integers $m$ and $k$ - that is we create $m$ collections of $k$ obervations ($k \gg m$). We denote a set: $A_i = \{ X_j | \quad  j &lt; (i-1) k , \quad j \geq i k  \}$ to contain all observations bar $k$, each set removes a different set of $k$ observations. Each has $|A_i| = (m-1)k$. We can then write:

$$ \mathbb{E}\left(\hat{Q}_{A_i}\right) \approx \hat{Q} + \frac{a_1}{(m-1)k} + \frac{a_2}{(m-1)^2k^2} $$

Via a second order approximation.</p>
<p>If we then define a new variable: $\hat{q}_i = m\hat{Q}_N - (m-1)\hat{Q}_{A_i}$. Then via a second-order approximation we have:
$$\mathbb{E}\left( \hat{q}_i \right) \approx m\left( \hat{Q} + \frac{a_1}{N} + \frac{a_2}{N^2}\right) - (m-1)\left(\hat{Q} + \frac{a_1}{(m-1)k} + \frac{a_2}{(m-1)^2k^2}\right)$$ 
Through some simplification this becomes:

$$\mathbb{E}\left( \hat{q}_i \right) \approx \hat{Q} - \frac{a_2}{m(m-1)k^2} $$

Asymptotically this has bias $O(N^{-2})$. If the estimator only has bias $O(N^{-1})$ (i.e. $a_i =0$ for $i \geq 2$) then the approximation is unbiased.</p>
<p>We can now define two new variables: <br>
$\hat{\hat{Q}}_N = \frac{1}{m} \sum_{i=1}^{m} \hat{q}_i$ <br>
$\hat{\hat{V}}_N =  \frac{1}{m-1} \sum_{i=1}^{m} \left( \hat{q}_i - \hat{\hat{Q}}_N \right)^2$ <br>
Then via CLT we have that $\hat{\hat{Q}}_N \sim \mathcal{N}(\hat{Q}, \hat{V})$ for some unknown variance $\hat{V}$. We can thus create an $X\%$ confidence interval as:<br>
$\hat{Q} \in \left[ \hat{\hat{Q}}_N - t \sqrt{\frac{\hat{\hat{V}}_N}{m}}, \hat{\hat{Q}}_N + t \sqrt{\frac{\hat{\hat{V}}_N}{m}} \right] $ <br>
With $t$ as the $(1-X)\%$ point of a double-tail student-t distribution with $(m-1)$ degrees of freedom.</p>
<p>We can see from the construction of this confidence interval there has been no restriction on the type of statistic $\hat{Q}$ used, in this sense this is a generic method.</p>
<p>(Note this is strongly related to a bootstrap method, in fact it is a first order approximation to a bootstrap)</p>
<h2 id="An-Example">
<a class="anchor" href="#An-Example" aria-hidden="true"><span class="octicon octicon-link"></span></a>An Example<a class="anchor-link" href="#An-Example"> </a>
</h2>
<p>The explanation above is quite notation dense, it will be easier to look at an example. In this case we will take one of the simplest examples of a Monte-Carlo model: estimating the value of $\pi$. To do this we will take a unit square and a unit quarter circle inside it:</p>
<p><img src="https://github.com/lewiscoleblog/blog/raw/master/images/Jackknife/circle.jpg" alt=""></p>
<p>We will simulate random points within the square and calculate the proportion $p$ of points landing within the quarter circle (red). If we simulate $N$ points we get an estimate $\pi \approx \frac{4p}{N}$. A simple vectorised numpy for this can be seen below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Estimating pi using Monte-Carlo</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">points_in_circle</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Returns an array that contains 1 if a random point (x,y)</span>
<span class="sd">    is within the unit circle of 0 otherwise</span>
<span class="sd">    </span>
<span class="sd">    N: Number of simulations (int)</span>
<span class="sd">    </span>
<span class="sd">    Random seed fixed for repeatability</span>
<span class="sd">    """</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">*</span><span class="mi">1</span>

<span class="k">def</span> <span class="nf">est_pi</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Return an estimate of pi using the output of points_in_circle</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span>

<span class="n">SIMS</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">pts</span> <span class="o">=</span> <span class="n">points_in_circle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Estimate of pi:"</span><span class="p">,</span> <span class="n">est_pi</span><span class="p">(</span><span class="n">pts</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Estimate of pi: 3.1456
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use the jackknife method to now construct a confidence interval. (Obviously in this case we have the sample estimator of an average and so CLT applies, in practice we wouldn't use a jackknife here. But for this example I wanted something simple as to not distract attention from the jackknife method itself.) We know what the result "should" be in this example, however we shall pretend we don't have access to np.pi (or similar).</p>
<p>We can code up an implementation of the jackknife method as:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span>

<span class="k">def</span> <span class="nf">jackknife_pi</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">pi_fn</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    This function implements the jackknife method outlined above</span>
<span class="sd">    The function takes an array (arr) and an estimate function (pi_fn)</span>
<span class="sd">    and a number of discrete buckets (m) - in this implementation m </span>
<span class="sd">    needs to divide size(arr) exactly</span>
<span class="sd">    </span>
<span class="sd">    The function returns Q-double hat, V-double hat, (m-1)</span>
<span class="sd">    """</span>
    <span class="n">Qn</span> <span class="o">=</span> <span class="n">pi_fn</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">itr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">N</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">ID</span> <span class="o">=</span> <span class="p">(</span><span class="n">itr</span> <span class="o">&lt;</span> <span class="n">i</span><span class="o">*</span><span class="n">k</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">itr</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">k</span><span class="p">)</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">ID</span><span class="p">]</span>
        <span class="n">q</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">Qn</span> <span class="o">-</span> <span class="p">(</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">pi_fn</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
    
    <span class="n">Qjk</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">-</span> <span class="n">Qjk</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">Vjk</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">Qjk</span><span class="p">,</span> <span class="n">Vjk</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">conf_int</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dof</span><span class="p">):</span>
    <span class="n">tpt</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">X</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">dof</span><span class="p">)</span>
    <span class="n">up</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">+</span> <span class="n">tpt</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V</span><span class="o">/</span><span class="p">(</span><span class="n">dof</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">down</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">-</span> <span class="n">tpt</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V</span><span class="o">/</span><span class="p">(</span><span class="n">dof</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="mi">100</span><span class="p">),</span><span class="s2">"</span><span class="si">% c</span><span class="s2">onfidence interval: ["</span><span class="p">,</span><span class="n">down</span><span class="p">,</span><span class="s2">","</span><span class="p">,</span><span class="n">up</span><span class="p">,</span><span class="s2">"]"</span><span class="p">)</span>

<span class="n">jk_pi</span> <span class="o">=</span> <span class="n">jackknife_pi</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="n">est_pi</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">Qtest</span> <span class="o">=</span> <span class="n">jk_pi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Vtest</span> <span class="o">=</span> <span class="n">jk_pi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pct</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">dof</span> <span class="o">=</span> <span class="n">jk_pi</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">conf_int</span><span class="p">(</span><span class="n">Qtest</span><span class="p">,</span> <span class="n">Vtest</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">dof</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>95 % confidence interval: [ 3.103649740420917 , 3.187550259579082 ]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the 95% confidence interval range is fairly large (around 3.10 to 3.19) in this case. We will now show that by using a "better" method we can reduce this range. We start by reconsidering the square-quarter-circle: we notice that we can add 2 additional squares to this setup:</p>
<p><img src="https://github.com/lewiscoleblog/blog/raw/master/images/Jackknife/Mondrian.jpg" alt=""></p>
<p>Apart from looking like a Mondrian painting we can notice that all points generated in the yellow area will add "1" to the estimator array and all points in the black area will add "0". The only areas of "contention" are the blue/red rectangles, if we focus only in generating points in these areas we will increase the accuracy of the estimator. By symmetry these 2 rectangles are identical, we only need to generate points within the rectangle: $\left\{\left(1, \frac{1}{\sqrt{2}}\right), \left( 1,0\right), \left(\frac{1}{\sqrt{2}},0\right), \left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)\right\}$. This has the area: $\frac{\sqrt{2}-1}{2}$ or both rectangles together having total area: $\sqrt{2}-1$. Therefore generating $N$ points within these rectangles is equivalent to generating: $\frac{N}{\sqrt{2}-1}$ points in the original scheme (approximately 2.5 times as many). This should reduce the standard deviation of the estimate by about a third (by CLT). We can code this estimator up in a similar way to before:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">points_in_rect</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Returns an array that contains 1 if a random point (x,y)</span>
<span class="sd">    is within the unit circle of 0 otherwise</span>
<span class="sd">    </span>
<span class="sd">    This uses the "imporved" method</span>
<span class="sd">    </span>
<span class="sd">    N: Number of simulations (int)</span>
<span class="sd">    </span>
<span class="sd">    Random seed fixed for repeatability</span>
<span class="sd">    """</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">*</span><span class="mi">1</span>

<span class="k">def</span> <span class="nf">est_pi_2</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Return an estimate of pi using the output of points_in_rect</span>
<span class="sd">    This applies a correction since points are only simulated</span>
<span class="sd">    in smaller rectangles</span>
<span class="sd">    """</span>
    <span class="n">pct</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">approx_pi</span> <span class="o">=</span> <span class="p">(</span><span class="n">pct</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="mi">4</span>
    <span class="k">return</span> <span class="n">approx_pi</span>

<span class="n">SIMS</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">pts</span> <span class="o">=</span> <span class="n">points_in_rect</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Estimate of pi:"</span><span class="p">,</span> <span class="n">est_pi_2</span><span class="p">(</span><span class="n">pts</span><span class="p">))</span>

<span class="n">jk_pi</span> <span class="o">=</span> <span class="n">jackknife_pi</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="n">est_pi_2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">Qtest</span> <span class="o">=</span> <span class="n">jk_pi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Vtest</span> <span class="o">=</span> <span class="n">jk_pi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pct</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">dof</span> <span class="o">=</span> <span class="n">jk_pi</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">conf_int</span><span class="p">(</span><span class="n">Qtest</span><span class="p">,</span> <span class="n">Vtest</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">dof</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Estimate of pi: 3.144554915549336
95 % confidence interval: [ 3.1247314678035196 , 3.164378363295143 ]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As expected we can see the confidence interval has decreased significantly through an improved estimator.</p>
<h2 id="Notes-on-Implementation">
<a class="anchor" href="#Notes-on-Implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notes on Implementation<a class="anchor-link" href="#Notes-on-Implementation"> </a>
</h2>
<p>As we have seen the jackknife is a general tool. In certain situations other tools exist. There are a couple of points we have to keep in mind when implementing these methods:</p>
<ul>
<li>
<strong>The selection of m:</strong> this is fairly arbitrary, if the statistic being analysed is very cumbersome to calculate then a smaller choice of m is helpful (or if we wish to run this method over very many statistics).</li>
<li>
<strong>Properties of the statistic:</strong> in some instances we know the statistic must be bounded (e.g. a correlation coefficient must be between $[-1,1]$) This additional information can and should be used to improve the confidence interval.
It often becomes more art than science when deciding how to present the results of this method.</li>
</ul>
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
<p>In this post we have seen what a jackknife method is, why it works and a basic implementation. Hopefully now it is obvious the power these methods hold for reporting on the results of a Monte-Carlo simulation. In more sophisticated situations it can really give insight into which parts of a model suffer most from simulation error and also how confident we should be with an estimate it produces.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/jackknife" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog about maths, probability, modelling and computing.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/lewiscoleblog" title="lewiscoleblog"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jazzcoffeestuff" title="jazzcoffeestuff"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
