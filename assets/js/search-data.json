{
  
    
        "post0": {
            "title": "Cellular Automata and Iterative Algorithms",
            "content": "Cellular automata, despite the gradiose name, are very simple rule-based programs. Essentially we have a number of cells that can take various states, through time the cell&#39;s state will update according to some simple rule. We will begin with the simplest case. . 1D Cellular Automata . We start by looking at 1-dimensional cellular automata (elementary cellular automata). We imagine this consisting of a &quot;line&quot; of cells lined up 1 next to the other. Each cell can take values $0$ or $1$ (at least initially). The cells update their value in time based on a set of rules based on the previous value the cell has taken and the previous values taken by its two adjacent neighbours (left and right). At each step all cells update in unison (synchronous updates - to avoid any &quot;cascading&quot; type effects). . If we colour code the cells such that $0$ is equivalent to white and $1$ is equivalent to black we could describe update rules in tabular form: . . For example the top entry in the table gives the sub-rule that if the left neighbour is white, the cell is white and the right neighbour is white then the cells value in the next timestep is white. The second entry states that if the left neighbour and previous cell is white while the right neighbour is black then the cell updates to black. And so on for all the other entries in the table. There are 8 sub-rules total to describe this automata since each cell can take only 2 values and $2^3 = 8$. The collection of 8 sub-rules constitutes a rule. For convenience I refer to the &quot;input&quot; to the cell by a binary value (left most cell representing $4$, above $2$ and right $1$ in a binary representation). So a binary input of $001$ means the output of the update step will be $1$ (black) - the second row in the table. To classify the rules we also use binary, the right most column in the table shows how to do this, in this case the rule shown is rule $30$ ($2+4+8+16=30$). . That is all there is to it. On the face of it this does not seem very interesting but it turns out that cellular automata are deceptively complex even in this simple 1d form. To see this let&#39;s look at how rule 30 behaves, starting with a single black cell. . We will make a 1d cellular automata function to help us plot the automata. The state of the cells will evolve &quot;downwards&quot; on the plot, so the first row will represent the initial condition and the second will represent the state after one update, and so on. We will then call the function to generate images of the rules we wish to examine. For convenience we wil assume any &quot;cells&quot; outside of the frame are set to a $0$ state (white) - we could just as easily take a periodic boundary condition so that the left most cell is connected to the right most cell. In taking the $0$ assumption we can introduce &quot;edge effects&quot; for some rules, however this is fine for our purpose. . The function can be seen below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix Random seed np.random.seed(123) def CA_1D(N, rules, initial): # Initialize array image = np.zeros((N+1, 2*N+1)) image[0, :] = initial # Loop over total time steps for i in range(1,N+1): # Loop over cells per time step for j in range(2*N + 1): # if cell 0 set left neighbour to 0 if j == 0: left = 0 else: left = image[i-1, j-1] above = image[i-1,j] # If cell 2N+1 set right neigbour to 0 if j == 2*N: right = 0 else: right = image[i-1, j+1] # Convert to binary binary = right + 2*above + 4*left # Update cell image[i,j] = rules[7 - int(binary)] # Plot image of automata plt.figure(figsize=(10,15), frameon=False) plt.imshow(image, cmap=&#39;binary&#39;) plt.xticks([]) plt.yticks([]) plt.axis(&#39;off&#39;) plt.show() . We can now use this to plot rule 30, we will start with a single black cell in a row and evolve from there: . # Rule 30 N = 250 rules = np.array([0,0,0,1,1,1,1,0]) initial = np.zeros(2*N+1) initial[N] = 1 CA_1D(N, rules, initial) . This is somewhat counter to our intuition, we would not expect a simple set of rules to be able to generate such complicated, seemingly random behaviour. Instead we would expect something much more &quot;regular&quot; and &quot;predictable&quot;. . From our construction we know that there are &quot;only&quot; $256$ rules ($2^8$) so in this case it is possible to iterate through them all. Stephen Wolfram did this in the 1980s and observed 4 distinct classes of behaviour: . Class 1 - The automata quickly converges to a stable state and does not change | Class 2 - The automata quickly converges to a regular repetitive pattern | Class 3 - The automata quickly converges to a seemingly random or chaotic pattern (rule 30 is one such example) | Class 4 - The automata quickly converges to a number of distinct patterns that &quot;interact&quot; with each other in unpredictable ways | . We would have likely expected to observe class 1 and class 2 behaviour for all rules, the 3rd and 4th classes are interesting - they are not merely &quot;anamolies&quot; there are reasonably large numbers of rules in each class. These classes are some of the simplest examples of emergent behaviour - from seemingly simple rules comes complex global behaviours that look like they are specifically engineered. Stephen Wolfram landed on the phrase &quot;A New Kind of Science&quot; (and wrote a book under the same name) to represent this approach. Whereas in the past to engineer a solution we would have looked at large scale properties and try to &quot;construct&quot; a solution. The &quot;emergence&quot; approach suggests instead that we can find a &quot;universe&quot; of possible global behaviours generated by simple rules and then select the best (or at least a suitable) one. This very much mimics the evolutionary process. . As of 2020 the team at Wolfram have embarked on framing the physics problem of a &quot;theory of everything&quot; in terms of elementary computation (in some sense a generalization of the cellular automata presented here). They suggest exploring the space of fundamental computations in hope of finding rules that are able to generate all our physical laws (e.g. the strong and weak nuclear forces, general relativity, quantum theory etc). You can read more about this at: https://www.wolframphysics.org/ . Despite the apparent simplicity there are still many unanswered theoretical questions about 1d automata. Wolfram have offered a $30k prize for solving any one of the following open problems in relation to rule 30 as shown above: . Does the centre column remain non-periodic? (i.e. no repeating patterns) | Does the centre column (on average) contain equally many black and white cells? | Does computation of the Nth cell in the centre column require at least $O(N)$ effort? | Point 3 above alludes to the concept of computational irreducibility. If the computation requires at least $O(N)$ effort then (essentially) the cellular automata rule-based procedure is the &quot;most efficient&quot; way to compute these sequences and it is not possible to find a &quot;shortcut&quot; that allows you to predict what the result of some rule set more quickly. . If the centre column (or other rules) are capable of producing non-periodic random numbers this has quite a profound impact on the nature of randomness. What we percieve as randomness in the real world could infact be a deterministic process, moreover the rules generating it could be remarkably simple! From a more practical standpoint it could pave the way for better pseudo-random number generators for our stochastic models, cryptography and so on. . We will now look at examples from each of the 4 rule classes. For each one we will start with a random initial condition. . Class 1 (e.g. rule 160): . # Rule 160 # Class 1 Behaviour # Converges to stable state N = 50 rules = np.array([1,1,0,0,0,0,0,0]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules,initial) . Class 2 (e.g. rule 3): . # Rule 3 # Class 2 Behaviour # Periodic behaviour N = 50 rules = np.array([0,0,0,0,0,1,0,1]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules, initial) . Class 3 (e.g. rule 126): . # Rule 126 # Class 3 Behaviour # Seemingly random behaviour N = 250 rules = np.array([0,1,1,1,1,1,1,0]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules, initial) . Class 4 (e.g. rule 110): . # Rule 110 # Class 4 Behaviour # Localised repetitive structures that interact with each other N = 250 rules = np.array([0,1,1,0,1,1,1,0]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules, initial) . 2d Cellular Automata . While the 1d cellular automata is interesting and easy to investigate and study we may want to extend these ideas. The first natural extension is to consider what happens to automata in 2 spatial dimensions. Unlike before we cannot (easily) plot a 2d automata evolving in time on a single figure moving down the page. We can still visualise however through either the use of animations or simply plotting static figures at certain time steps. . By expanding to 2 spatial dimensions it also opens up new geometries. In 1 dimension we were limited to the &quot;left, above and right&quot; neighbours. In 2 dimensions there are options; we could imagine a square lattice with 4 neighbours (up, down, left, right) or 8 neighbours (plus the diagonals) or we could imagine a triangular lattice with 6 neighbours, and so on. This opens up a lot of flexibility. . Many/all of the behaviours we observe in 1d also carry over to 2d. For a simple illustration of a 2d automata we will plot a simple &quot;class 2&quot; rule on a triangular lattice. Plotting a triangular lattice requires a little attention since computer arrays appear to &quot;naturally&quot; be square lattices. However it only requires a renaming of the indices to give 6 neighbours: . . To plot this we will use a scatter plot to more accurately capture the triangular lattice geometry. . We will implement a simple rule, if a cell is &quot;off&quot; and only any 1 neighbour is &quot;on&quot; then it shall flip its state to &quot;on&quot;, otherwise it remains unchanged. . The code for this can be seen below: . # 2d Cellular Automata on a Triangular Lattice import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix lattice size and iterations N = 24 every = 3 num_plots = 3 n_iter = num_plots*every # Set up a container - initialize with single element image = np.zeros((N,N)) image[int(N/2), int(N/2)] = 1 # Define neighbours x_neighbour = np.array([1, 0, 1, -1, 0, -1]) y_neighbour = np.array([-1, -1, 0, 1, 1, 0]) # Set up plot fig, axs = plt.subplots(1, num_plots+1, figsize=(20, 5)) fig.suptitle(&#39;Triangular lattice evolution&#39;) axs[0].set_title(&#39;Initial State&#39;) for x in range(num_plots+1): axs[x].scatter(N/2, N/2, color=&#39;black&#39;) axs[x].set_xticks([]) axs[x].set_yticks([]) axs[x].set_xlim(0,N) axs[x].set_ylim(0,N) axs[x].set_aspect(&#39;equal&#39;) if x &gt; 0: axs[x].set_title(&quot;Time Step: %i&quot; %(x*every)) # Set indicator for stopping plots plot_id = 1 # Loop update for i in range(1, n_iter+1): updated = np.zeros((N,N)) # Loop over lattice for x in range(1, N-1): for y in range(1, N-1): # Reset cumulative sum cumul=0 # Loop over neighbours and store &quot;on&quot; states for k in range(x_neighbour.shape[0]): cumul += image[x+x_neighbour[k], y+y_neighbour[k]] # If cell is empty and only 1 neighbour then update and plot if image[x,y] == 0 and cumul == 1: updated[x,y] = 1 # Loop over remaining plots for p in range(plot_id, num_plots+1): axs[p].scatter(y+(x-N/2)/2, N/2 + 0.866*(x-N/2), color=&#39;black&#39;) # Update image image += updated # Update plot identifier if i % every == 0: plot_id += 1 # End loop plt.show() . A we can see this rule generates a &quot;snowflake&quot;. . In a similar way to before we can try other rules if we so wish. However we shall not do so here for brevity. We could also try moving from a 2 state automata (cells taking states $0$ or $1$) to a multi-state system, however the number of rules to specify such an automata grows quickly (e.g. for a 4 state system in 1d we would require $4^3 = 64$ sub-rules) and the number of patterns increases rapidly also (e.g. for the same 4 state system in 1d there are over $8.5e37$ rule sets!). However via symmetry we can reduce this space slightly. Moreover we could, as above, define simple rules such as &quot;if over 50% of neighbours are on then on&quot; and so forth. . We now turn our attention to specific applications of cellular automata. . Game of Life . The Game of Life was developed by Conway in 1970 - before work on other cellular automata had taken place. It is called &quot;the game of life&quot; as it creates patterns that appear as if they are &quot;living&quot; when animated. In the language we have used so far it is a 2-state 2d cellular automata where the update rules are: . Any active cell with two or three active neighbours remains active. | Any inactive cell with three active neighbours becomes active. | All other active cells cells become inactive. | All other inactive cells remain inactive. | Again despite it&#39;s apparent simplicity the game of life is capable of showing some complex behaviour. . Let&#39;s code it up and take a look: . # Conway&#39;s Game of Life import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import HTML %matplotlib inline # Fix random seed (for initialization only) np.random.seed(123) # Set Grid size and initialize randomly N = 100 image = np.random.randint(low=0, high=2, size=(N,N)) ###### Main Code ###### # Update function to simulate a single evolution time step def update(): # Take copy of previous time step old = image.copy() # Loop over all cells for i in range(N): for j in range(N): # Count number of active neighbours with loop n_sum = 0 for x in range(-1,2): for y in range(-1,2): idx = (i+x) % N idy = (j+y) % N n_sum += old[idx,idy] # Remove current cell n_sum -= old[i,j] # Apply Conway&#39;s rules and update if old[i,j] == 1: if n_sum == 2 or n_sum == 3: image[i,j] = 1 else: image[i,j] = 0 if old[i,j] == 0: if n_sum == 3: image[i,j] = 1 else: image[i,j] = 0 ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, N-1), ylim=(0, N-1)) axes.set_xticks([], []) axes.set_yticks([], []) viz = plt.imshow(image, cmap=&quot;binary&quot;) # Define animation step def animate(frame): update() viz.set_array(image) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=50, interval=150) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect As we can see there appears to be living organisms within our simulation! Somewhat reminiscent of bacteria. Of course there is no such thing as an &quot;organism&quot;, they are just &quot;lightbulbs&quot; turning on and off in sequence. This raises a philosophical question of whether this constitutes a &quot;life&quot; of whether it is just a homuncular functionism (in laymans terms: are we seeing life where it does not exist, like how we often &quot;see&quot; animals in clouds or faces within muffins in a coffee shop). . Since the game of life has been studied so thoroughly there are a number of stable behaviours people have discovered including &quot;static objects&quot;, &quot;blinkers&quot; that appear to oscilate fixed in space, &quot;gliders&quot; that travel along the screen and &quot;ray guns&quot; that appear to &quot;shoot&quot; bullets across the screen. Some of these have been classified on the wikipedia entry: https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life#Examples_of_patterns We could reproduce some of these behaviours by choosing suitable starting conditions in the code above. . It has also been shown that the game of life is capable of performing computation. So you could add two integers together (for example) using the game of life. Moreover it has been shown that the game of life is Turing complete, this essentially means that (with a large enough automata) it can recreate any code. So, in theory, we could recreate the entire Windows operating system entirely out of the game of life! (Of course this is not an efficient way to do this). This is very surprising that such a (seemingly) simple program is capable of creating such complexity. This clearly have implications for evolution, where emergent behaviour and characteristics could be built out of much simpler fundamental iterative rules. If we could codify these rules we could (in theory) put them in a computer and recover the entire evolutionary history of life on earth. . We could modify Conway&#39;s procedure above to test different rule sets to investigate how the &quot;life&quot; changes. We could imagine this akin to different environments in an evolutionary setting. We could also add &quot;noise&quot; to the system, so the rules above are only implemented correctly a certain percentage of the time (e.g. &quot;Any active cell with two or three active neighbours remains active with 80% probability and becomes inactive with 20% probability&quot;). This could be an analogue to mutations in the evolutionary system. We could also investigate &quot;adaptive&quot; rules, whereby the rules change over time (perhaps determinstically or perhaps dependent on the state of the system). Each of these are simple modifications to the base code above. . Termite and Wood Chips . We now look at a simple model infuenced by cellular automata: the termite and woodchip model. One could argue that this is in fact an agent based model rather than a cellular automata. However the line between cellular automata and agent based model is somewhat fuzzy. . The model environment is a 2d grid (square lattice) taking states $0$ (empty) or $1$ (containing a &quot;woodchip&quot;). In the environment there are &quot;termites&quot; that wander around according to a random walk. If the termite encounters a woodchip it picks it up (the grid cells state changes from $1$ to $0$). The termite will then carry the woodchip, if it encounters another woodchip while holding one then the termite will drop the woodchip at the next empty cell that it encounters (that cell changing from state $0$ to $1$). This is repeated multiple times. . As this is a dynamic model it helps to visualize with an animation. We shall denote empty cells as &quot;white&quot; and woodchips by &quot;black&quot;. Termites will be represented by &quot;red circles&quot; traversing the space. . This is a very simple model. We can see an example implementation below: . # Termite and Wood Chip model import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import HTML %matplotlib inline # Fix random seed np.random.seed(123) ###### Model Parameters ###### N = 50 # Grid size N_m = 100 # Number of termites Chip_p = 0.3 # Proportion of grids containing wood chips num_frames = 250 # Number of animated frames # Initialize # Grid of woodchips Grid = np.zeros((N, N)) for i in range(N): for j in range(N): if np.random.random() &lt; Chip_p: Grid[i, j] = 1 Initial_Grid = Grid.copy() # Initial Location of termites Mites = np.random.randint(low=0, high=N+1, size=(N_m, 2)) # Holding woodchip? Hold = np.zeros(N_m) # Need to drop woodchip? Drop = np.zeros(N_m) # Define update step def update(): # Loop over all termites for m in range(N_m): # Select direction to move if np.random.random() &gt; 0.5: # Move vertical if np.random.random() &gt; 0.5: Mites[m,1] = (Mites[m,1] + 1) % N else: Mites[m,1] = (Mites[m,1] - 1) % N else: # Move horizontal if np.random.random() &gt; 0.5: Mites[m,0] = (Mites[m,0] + 1) % N else: Mites[m,0] = (Mites[m,0] - 1) % N x = Mites[m,0] y = Mites[m,1] # If mite encounters chip and isn&#39;t holding # The pick up if (Grid[x,y] == 1) and (Hold[m] == 0): Grid[x,y] = 0 Hold[m] = 1 # If termite is holding and enconters chip # then note it is ready to drop chip if (Grid[x,y] == 1) and (Hold[m] == 1) and (Drop[m] == 0): Drop[m] = 1 # If termite encounters empty grid cell and is ready to drop # ready to drop then drop chip if (Grid[x,y] == 0) and (Hold[m] == 1) and (Drop[m] == 1): Grid[x,y] = 1 Hold[m] = 0 Drop[m] = 0 ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, N-1), ylim=(0, N-1)) axes.set_xticks([], []) axes.set_yticks([], []) viz = plt.imshow(Grid, cmap=&quot;binary&quot;) m_viz = plt.scatter(Mites[:,0], Mites[:,1], color=&#39;red&#39;) # Define animation step def animate(frame): update() viz.set_array(Grid) m_viz.set_offsets(Mites) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=num_frames, interval=25) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect We can see the termites busy working away, we cannot yet see much of a pattern. We do notice however that the chips are slowly becoming more concentrated (there are defined regions of black and white). If we continue the process for many more iterations we can see that the termites do indeed create wood chip piles. . We will not animate the entire process since it will be a long animation and a large amount of data, instead we will just plot the initial state of the woodchips and the state following 10 million time steps: . If we left this run even longer we would be left with a single large pile (or a small number of large piles) of woodchips. . This is really quite a remarkable emergent behaviour. Each individual termite does not show anything we would term &quot;intelligence&quot; and there is no communication between the termites. Yet if we only observe chip piles our first instinct would be that they are a result of some sort of intelligent behaviour and/or cooperation or perhaps even some global oversight telling how each individual termite should behave. It seems almost unfathomable that this could be the result of only a few simple rules defining individual behaviour. . Schelling Model . We now move onto another application of Cellular Automata through the Schelling model. As with the previous model there&#39;s an argument as to whether this is an agent-based model or a cellular automata. And as with the previous models the enviroment shall be a 2d square lattice (although this is not a requirement and it can be modified to essentially any geometry). . Schelling developed this model in the 1970s, reportedly on graph paper using coins to represent the agents - illustrating how simple automata based models can be. Thankfully now computation makes it easier to investigate the model. . Schelling&#39;s model aims to study how spatial segregation occurs. In our example each &quot;cell&quot; in the square lattice can be in one of three states: empty (denoted by $0$ or white), populated by agent of type A (denoted by $1$ or red) or populated by agent of type B (denoted by $-1$ or blue). There is a global &quot;threshold&quot; that represents how tolerant (or intolerant) the agents are, if the percentage of neighbours of the same type is above this threhold the agent is &quot;satisfied&quot; and the cell remains in the same state. If the percentage of neighbours is below this threshold the agent is &quot;unsatisfied&quot; - the cell will then become empty and some other (randomly selected) empty cell will become occupied by an agent of the same type (so the number of A, B and empty states remains constant over time). This is a simple model of agents &quot;moving location&quot; to satisfy some preference. . Since agents &quot;look&quot; for an empty cell it is difficult to implement a synchronous update scheme (e.g. if 2 agents are moving in the same step and select the same empty cell who gets it?) To avoid this we will randomly select an agent to move (cell to update) in each time step, this is fine for our purposes. We will also assume periodic (toroidal) boundary conditions so that each agent has the same number of neighbours and to avoid &quot;edge effects&quot;. . Again this is remarkably simple as a model. We likely wouldn&#39;t believe this is capable of producing any &quot;interesting&quot; behaviour. . Let&#39;s code up an example: . # Schelling&#39;s Segregation Model import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import HTML %matplotlib inline # Fix random seed np.random.seed(123) ###### Model Parameters ###### N = 100 # Lattice size pct_empty = 0.25 # Percentage of cells empty A_B_ratio = 0.5 # Ratio of agents A:B threshold = 0.4 # Threshold for satisfaction N_step = 200 # Number of agent moves per frame to reduce animation size num_frames = 150 # Number animation frames ###### Set up ###### # Set up lattice cells = np.zeros((N,N)) # Set up lookup array for convenience lookup = np.arange(N*N).reshape(N,N) # Set initial conditions according to population proportions for i in range(N): for j in range(N): if np.random.random() &gt; pct_empty: if np.random.random() &gt; A_B_ratio: cells[i,j] = 1 else: cells[i,j] = -1 initial_cells = cells.copy() # Define a single update step def update(): for step in range(N_step): # Non-zero cells nz nz = lookup[cells != 0] # zero cells z z = lookup[cells == 0] # Select random non-zero cell active = np.random.choice(nz) # Retrieve x and y coordinates active_x = active % N active_y = int((active - active_x) / N) # Current state state = cells[lookup == active][0] # Caclulate neighbour proportions n_prop n_same = 0 for x in range(-1, 2): for y in range(-1, 2): xid = (active_x + x) % N yid = (active_y + y) % N if cells[xid, yid] == state: n_same += 1 / 8 # If n_prop is less than threshold select new empty state if n_same &lt; threshold: cells[lookup == active] = 0 new_cell = np.random.choice(z) cells[lookup == new_cell] = state ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, N-1), ylim=(0, N-1)) axes.set_xticks([], []) axes.set_yticks([], []) viz = plt.imshow(cells, cmap=&quot;bwr&quot;) # Define animation step def animate(frame): update() viz.set_array(cells) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=num_frames, interval=75) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect To highlight the segregation we look at the initial (random) configuration and the configuration at the end of the animation: . We can see that even if the agents are fairly &quot;tolerant&quot; (they require only 40% of the surrounding cells to be the same type as them) the system eventually ends up with clusters of agents of the same type. This is quite a remarkable result that goes completely against common sense where one would expect low levels of tolerance (high threshold for satisfaction) to lead to segregation. In fact in the Schelling model very low levels of tolerance leads to less segregation - since the agents are rarely happy and continue to search never settling down. We can find other relations by varying input parameters of the model. . Now one could argue the real world application of this model and the assumptions it makes. However it does raise an interesting point regarding emergence: localised behaviour (of individuals) does not always translate to global behaviour (of the entire system). This is a big problem for the social sciences where they have traditionally opted for the reductionist method as used in the traditional (natural) sciences. Schelling&#39;s simple model shows that this is not always appropriate and the assumption that local and global behaviours are related in all cases is a fallacy. This is fairly troubling when the results from traditional social sciences influences governmental behaviour! Once you are aware of this disconnect you begin to notice this fallacy everywhere. . Despite its simplicity the Schelling model has been shown to produce segregation patterns that closely matches observed residential patterns in cities across the world. . Again if we wanted to it is relatively easy to think of extensions to this model. The obvious example being: what if we have multiple agent types? Another could be: what happens if we allow heterogeneous thresholds (i.e. each agent has their own threshold rather than all agents having the same)? As before adding noise is also an option. Or we could believe that with more exposure agents become more tolerant and so their tolerances change over time depending on exposure. Perhaps we want to allow &quot;foresight&quot; - so an agent evaluates the open cells and chooses the one according to their preference. In addition to that we could allow a &quot;moving delay&quot; so that by the time the agent moves the situation could be different to what they originally intended. It is interesting to consider if these extensions change the behaviour in meaningful ways. . Conclusion . In this blog we started by looking at simple 1d 2-state cellular automata, we moved through to 2 spatial dimensions. We then took these ideas and applied them to create 3 models: Conway&#39;s game of life, Termite-Woodchip and Schelling&#39;s segregation model. Through each of these we noticed that from simple deterministic rules there can be sophisticated emergent behaviours. This has a profound impact on our understanding of nature. . Some of these observations include: . Simple deterministic rules can produce (seemingly) random behaviour - what does this say about the nature of &quot;randomness&quot; in our universe? | Simple deterministic rules can produce chaotic results | Some sets of rules exhibit computational irreducibility while others do not | Simple rules (such as Conway&#39;s Game of Life) can be Turing complete and capable of universal computation | Simple individual behaviour can give the illusion of sophistication and/or large scale cooperation/governance | Observed large scale behaviour does not necessarily correlate with individual local behaviour and vice versa (in the words of PW Anderson: &quot;More is different&quot;) - Schelling&#39;s model shows that are intuitions can be wrong | Following on from the above scientific reductionism is not always appropriate and emergent behaviours may become the next &quot;scientific enlightenment&quot; | .",
            "url": "https://www.lewiscoleblog.com/cellular-automata",
            "relUrl": "/cellular-automata",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Gillespie Algorithm",
            "content": "The Gillespie algorithm is one of the most historically important stochastic simulation algorithms ever created. At its heart the intuition behind it is very simple and it is re-assuring that it &quot;works&quot; - this is not always the case with stochastic simulation where the &quot;obvious&quot; idea can sometimes have unintended debilitating consequences. . The algorithm was first presented by Doob (and is sometimes refered to as the Doob-Gillespie algorithm) in the mid 1940s. It was implemented by Kendall in the 1950s. However it wasn&#39;t until the mid 1970s that Gillespie re-derived the method by studying physical systems that it became widely used. In publishing the method he essentially created the entire fields of systems biology and computational chemistry by opening the door to what is possible through stochastic simulation. . Background . In this blog we will consider applying the Gillespie method to the area of chemical reaction kinetics, this is the application Gillespie originally had in mind. The concepts described will carry over to other applications. . Imagine we wish to model a particular chemical reaction. We could use a determistic approach to model the reaction, this will require setting up a family of coupled differential equations. In doing so we will essentially &quot;ignore&quot; any microscopic behaviour and look at the reaction system at a &quot;high level&quot;. This can mean we miss out on a lot of the &quot;detail&quot; of the reaction which may be of interest to us. Further in some cases this approach may not even be applicable, for example to set up a differential equation we assume that we have large quantities of reactants that are perfectly mixed, this allows us to &quot;average over&quot; all reactants to create nice smooth dynamics. This may not reflect reality if there are only relatively few reactants in a system. An alternate approach is to use a stochastic &quot;discrete event&quot; model - this is where we model individual reactions seperately as discrete events occuring in time. This matches our physical intuition of how reactions occur: we wait until the reactants &quot;bump&quot; into each other in the right way before a reaction occurs. One way to summarise this mathematically is through the use of a &quot;master equation&quot;. . In the sciences a master equation represents the time evolution properties of a multi-state jumping system, by which we mean a system that &quot;jumps&quot; between distinct states through time (in contrast a &quot;diffusion system&quot; varies gradually). The system in question being stochastic in nature we are concerned with observing how the state distribution varies over time, for example: with some initial condition what is the probability of finding the system in a particular state within the next X seconds/minutes/years? Of course the time units depend on the nature of the system (e.g. if we construct a master equation for predator/prey dynamics we are unlikely to be interested in microsecond timescales, however if looking at a chemical reaction we are unlikely to find a timescale in days useful.) If we want to display the master equation mathematically we use a transition rate matrix $A(t)$ - this can evolve in time or it can be static. . We can then express the master equation in the form: $$ frac{d mathbf{P}_t}{dt} = A(t) mathbf{P}_t $$ Where vector $ mathbf{P}_t$ represents the probability distribution of states at time t - obscured by notation is an initial condition. Those from a mathematical or probabilistic background will recognise this as a Kolmogorov backwards equation for jump processes. If we expand the notation a little such that $P_{ij}(s,t)$ represents the probability of the system being in state $i$ at time $s$ and state $j$ at time $t$ then we can note that the transition rate matrix satisfies: begin{align} A_{ij}(t) &amp;= left[ frac{ partial P_{ij}(t,u)}{du} right]_{u=t} A_{ij}(t) &amp; geq 0 quad quad quad quad forall i neq j sum_j A_{ij}(t) &amp;= 0 quad quad quad quad forall i end{align} Further we can note that if there is a distribution $ pi$ such that: $$ pi_j A_{ij}(t) = pi_i A_{ij}(t) $$ For all pairs of states $(i,j)$ then the process satisfies detailed balance and the process is a reversible Markov process. . Gillespie Algorithm . The Gillespie algorithm is allows us to model the exact dynamics described by the master equation. In some (simple) cases we can solve the master equation analytically, but for complicated examples (e.g. say we have 50 different types of reaction occuring) this may not be feasible and so the Gillespie algorithm (or some sort of simulation method) is necessary. In pseudo code we can write down the Gillespie algorithm as: . Initialization - initialize the system, in the context of reaction kinetics this amounts to the setting up initial chemical concentrations | Monte-Carlo - Randomly simulate the time to the next event | Given an event has occurred randomly select which event has occured | | Update - based on 2. move the model time forward to the event time and update the state of the system | Repeat - Iterate through steps 2. and 3. until some stopping criteria is met | This essentially follows our intuition and there is no &quot;technical trickery&quot; such as fancy sampling methods, acceptance/rejection, etc. It is just a clean simple method - which is nice! Since we model by event as opposed to discretizing time steps this is an &quot;exact&quot; simulation method - meaning any trajectory simulated will follow the master equation dynamics exactly. However due to the random nature of any trajectory we will have to loop over these steps multiple times to find &quot;typical&quot; reaction paths (or whatever property we are trying to study). . An Example . To illustrate the algorithm in action we will take a simple reaction. We will have following forward reaction $$A + B to AB$$ Where two monomers $A$ and $B$ react to form a dimer $AB$. The corresponding reverse reaction being: $$AB to A + B$$ We will denote the rate of the forward reaction to be $r_f$ and the rate of the backward reaction to be $r_f$. If we let the number of molecules present be denoted by: $N_A, N_B$ and $N_{AB}$ then the rate of any reaction occurring is: $$R = r_f N_A N_B + r_b N_{AB}$$ Also given a reaction has occured the probability of the forward reaction having taken place is: $$ mathbb{P}(A + B to AB) = frac{r_f N_A N_B}{R}$$ For a model such as this we typically want to remove any &quot;path dependence&quot; - the arrival of the next reaction event is independent of reactions that have occurred previously (given the concentration of reactants). To satisfy this constraint typically reactions events are taken to follow a Poisson process. Under this assumption the number of reactions occuring within a time period $ Delta T$ follows a $Poisson(R Delta T)$ distribution. Moreover the time between reactions is then follows an exponential distribution. Thus if we sample $u sim U[0,1]$ then we take the time until next reaction to be $ tau = frac{1}{R}ln left( frac{1}{u} right)$. (Note: here I have used that $U$ and $(1-U)$ have the same distribution). . A basic implementation of this can be seen below: . # An implenetation of the Gillespie algorithm # applied to a pair of reactions: # A + B -&gt; AB # AB -&gt; A + B import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed for repeatability np.random.seed(123) ###### Fix model parameters ###### N_A0 = 25 # Initial number of A molecules N_B0 = 35 # Initial number of B molecules N_AB0 = 5 # Initial number of AB molecules rf = 2 # Forward reaction rate rb = 1 # Backwards reaction rate steps = 25 # Number of reactions per trajectory cycles = 100 # Number of trajectories iterated over # Set up holder arrays T = np.zeros((cycles, steps+1)) N_A = np.zeros((cycles, steps+1)) N_B = np.zeros((cycles, steps+1)) N_AB = np.zeros((cycles, steps+1)) # Store initial conditions N_A[:,0] = N_A0 N_B[:,0] = N_B0 N_AB[:,0] = N_AB0 ###### Main Code Loop ###### for i in range(cycles): for j in range(steps): # Calculate updated overall reaction rate R = rf * N_A[i,j] * N_B[i,j] + rb * N_AB[i,j] # Calculate time to next reaction u1 = np.random.random() tau = 1/R * np.log(1/u1) # Store reaction time T[i, j+1] = T[i,j] + tau # Select which reaction to occur Rf = rf * N_A[i,j] * N_B[i,j] / R u2 = np.random.random() # Update populations if u2 &lt; Rf: N_A[i,j+1] = N_A[i,j] - 1 N_B[i,j+1] = N_B[i,j] - 1 N_AB[i,j+1] = N_AB[i,j] + 1 else: N_A[i,j+1] = N_A[i,j] + 1 N_B[i,j+1] = N_B[i,j] + 1 N_AB[i,j+1] = N_AB[i,j] - 1 # Calculate an average trajectory plot ave_steps = 100 T_max = T.max() # Set up average arrays T_ave = np.linspace(0,T_max,ave_steps+1) N_A_ave = np.zeros(ave_steps+1) N_B_ave = np.zeros(ave_steps+1) N_AB_ave = np.zeros(ave_steps+1) N_A_ave[0] = N_A0 N_B_ave[0] = N_B0 N_AB_ave[0] = N_AB0 # Pass over average array entries for i in range(1, ave_steps+1): tmax = T_ave[i] A_sum = 0 B_sum = 0 AB_sum = 0 t_count = 0 # Pass over each trajectory and step therein for j in range(cycles): for k in range(steps): if T[j,k] &lt;= tmax and T[j,k+1] &gt; tmax: t_count += 1 A_sum += N_A[j,k] B_sum += N_B[j,k] AB_sum += N_AB[j,k] # Caclulate average - taking care if no samples observed if t_count == 0: N_A_ave[i] = N_A_ave[i-1] N_B_ave[i] = N_B_ave[i-1] N_AB_ave[i] = N_AB_ave[i-1] else: N_A_ave[i] = A_sum / t_count N_B_ave[i] = B_sum / t_count N_AB_ave[i] = AB_sum / t_count ###### Plot Trajectories ###### fig, axs = plt.subplots(3, 1, figsize=(10,20)) # Plot average trajectories axs[0].plot(T_ave, N_A_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[0].set_title(&#39;Number A Molecules&#39;) axs[0].set_ylim((0,35)) axs[0].set_xlim((0,0.125)) axs[1].plot(T_ave, N_B_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[1].set_title(&#39;Number B Molecules&#39;) axs[1].set_ylim((0,35)) axs[1].set_xlim((0,0.125)) axs[2].plot(T_ave, N_AB_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[2].set_title(&#39;Number AB Molecules&#39;) axs[2].set_xlabel(&quot;Time&quot;) axs[2].set_ylim((0,35)) axs[2].set_xlim((0,0.125)) # Plot each simulated trajectory for i in range(cycles): axs[0].plot(T[i,:], N_A[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[1].plot(T[i,:], N_B[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[2].plot(T[i,:], N_AB[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) plt.show() . In these plots we can see the various trajectories along with their average. If we increase the number of molecules and the number of trajectories we can get a &quot;smoother&quot; plot. Since we have the full evolution of the system we can also look at some other statistics, for example let&#39;s suppose we are interested in the distribution in the number of molecules of each type at time 0.5. We can also plot this using our samples: . time = 0.025 N_A_time = np.zeros(cycles) N_B_time = np.zeros(cycles) N_AB_time = np.zeros(cycles) for i in range(cycles): for j in range(1, steps): if T[i,j] =&gt; time and T[i,j-1] &lt; time: N_A_time[i] = N_A[i,j] N_B_time[i] = N_B[i,j] N_AB_time[i] = N_AB[i,j] # If trajectory doesn&#39;t span far enough take latest observation if T[i, steps] &lt; time: N_A_time[i] = N_A[i, steps] N_B_time[i] = N_B[i, steps] N_AB_time[i] = N_AB[i, steps] plt.hist(N_A_time, density=True, bins=np.arange(35), label=&quot;A&quot;, color=&#39;lightgrey&#39;) plt.hist(N_B_time, density=True, bins=np.arange(35), label=&quot;B&quot;, color=&#39;dimgrey&#39;) plt.hist(N_AB_time, density=True, bins=np.arange(35), label=&quot;AB&quot;, color=&#39;red&#39;) plt.legend() plt.show() . If instead of a system of 2 reactions we instead wanted to look a system of a large number of reactions we could modify the method above quite simply. Instead of the calculation of $R$ (overall reaction rate) consisting of 2 terms it will consist of a larger number of terms depending on the nature of the individual reactions. The probability of selecting a particular reaction type would then equally be in proportion to their contribution to $R$. . We can also notice that there is nothing &quot;special&quot; about the method that means it only applies to reaction kinetics. For example: the example code above could equally be a &quot;marriage and divorce model&quot; for heterosexual couples: A representing women and B representing men, AB representing a marriage. Through defining the &quot;reactions&quot; slightly differently it doesn&#39;t take much modification to turn this into a infection model: for example there could be 3 states: susceptible to infection, infected and recovered (potentially with immunity) with transition rates between each of these states. . We can see then that the Gillespie algorithm is very flexible and allows us to model stochastic systems that may otherwise be mathematically intractable. Through the nature of the modelling procedure we can sample from the system exactly (upto the precision of floating point numbers within our computers!) . There is a downside to exact simulation however: it can be very slow! In the example above the speed isn&#39;t really an issue since the system is so simple. However if we were modelling many different reaction types (say the order of 100s) then to allow for adequate samples we will need to run many trajectories, this can quickly spiral into a very slow running code! Thankfully however the method has been adapted in many ways to combat this issue. . Hybrid-Gillespie . We can note that calculating deterministic results from an ODE is (much) quicker than implementing the Gillespie simulation algorithm since there is no random element. However we notice that we do not have to model every reaction type using the same Gillespie approach. For example suppose we have one reaction type that is much slower than the others, say the order of 10 times slower. We could model this reaction via a determinstic ODE approach and simply rely on Gillespie for the more rapidly changing dynamics. Of course this is not applicable in every situation - as with any modelling or approximation used we should be sure that it is applicable to the situation at hand. For brevity we will not code an example of this here but it should be easy enough to modify the code above (for example by adding that molecule $A$ can &quot;disappear&quot; from the system with a rate 1/10 times the rate of the backward reaction). . Tau Leaping . Tau leaping modifies the Gillespie methodology above, it sacrifices exact simulation in favour of an approximate simulation that is quicker to compute. The main idea behind tau-leaping is also intuitive: instead of modelling time to the next event we &quot;jump&quot; forward in time and then compute how many reactions we would expect to see within that time frame and updating the population amounts in one step. By updating the population amounts in one go we should be able to compute much faster. It should be clear that this is an approximation to the Gillespie algorithm. The size of the &quot;leaps&quot; determines how efficient the method is and how accurate the approximation is. If we make very large steps we can model many reactions per step which speeds up the implementation, however the simulation will also be less accurate since the populations will be updated less frequently. Conversely a very small leap size will mean many leaps will not see a reaction and so the algorithm will run more slowly, however this should result in dynamics very close to the Gillespie method. Often choosing the leap size requuires some trial and error. . we can write pseudo-code for the tau-leaping process as: . Initialize - Set initial conditions for the system and set leaping size | Calculate event rates - for each event types depending on state of the system | Monte-Carlo - for each event type sample number of events occuring within the leap | Update - Update system state based on number of events | Repeat - Repeat steps 2-4 until some stopping criteria is met | Recall: in the example above we used an exponential waiting time between reactions. This means the reactions occur as a poisson process - as a result the number of reactions occuring within a given timeframe will follow a poisson distribution. We also have to be careful to not allow a negative population (at least in the example presented - in other systems this may be reasonable). . We can modify our example above to use Tau-leaping as: . # An implenetation of the Gillespie algorithm # with tau leaping # Applied to a pair of reactions: # A + B -&gt; AB # AB -&gt; A + B import numpy as np import matplotlib.pyplot as plt from scipy.stats import poisson %matplotlib inline # Fix random seed for repeatability np.random.seed(123) ###### Fix model parameters ###### N_A0 = 25 # Initial number of A molecules N_B0 = 35 # Initial number of B molecules N_AB0 = 5 # Initial number of AB molecules rf = 2 # Forward reaction rate rb = 1 # Backwards reaction rate leap = 0.005 # Size of leaping steps steps = 25 # Number of leaps per trajectory cycles = 100 # Number of trajectories iterated over # Set up holder arrays T = np.arange(steps+1)*leap N_A = np.zeros((cycles, steps+1)) N_B = np.zeros((cycles, steps+1)) N_AB = np.zeros((cycles, steps+1)) # Store initial conditions N_A[:,0] = N_A0 N_B[:,0] = N_B0 N_AB[:,0] = N_AB0 ###### Main Code Loop ###### for i in range(cycles): for j in range(steps): # Calculate updated reaction rates Rf = rf * N_A[i,j] * N_B[i,j] Rb = rb * N_AB[i,j] # Calculate number of reactions by type uf = np.random.random() ub = np.random.random() Nf = poisson.ppf(uf, Rf*leap) Nb = poisson.ppf(ub, Rb*leap) # Apply limits to prevent negative population Limitf = min(N_A[i,j], N_B[i,j]) Limitb = N_AB[i,j] Nf = min(Nf, Limitf) Nb = min(Nb, Limitb) # Update populations N_A[i,j+1] = N_A[i,j] + Nb - Nf N_B[i,j+1] = N_B[i,j] + Nb - Nf N_AB[i,j+1] = N_AB[i,j] + Nf - Nb # Calculate average arrays N_A_ave = N_A.mean(axis=0) N_B_ave = N_B.mean(axis=0) N_AB_ave = N_AB.mean(axis=0) ###### Plot Trajectories ###### fig, axs = plt.subplots(3, 1, figsize=(10,20)) # Plot average trajectories axs[0].plot(T, N_A_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[0].set_title(&#39;Number A Molecules&#39;) axs[0].set_ylim((0,35)) axs[0].set_xlim((0,0.125)) axs[1].plot(T, N_B_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[1].set_title(&#39;Number B Molecules&#39;) axs[1].set_ylim((0,35)) axs[1].set_xlim((0,0.125)) axs[2].plot(T, N_AB_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[2].set_title(&#39;Number AB Molecules&#39;) axs[2].set_xlabel(&quot;Time&quot;) axs[2].set_ylim((0,35)) axs[2].set_xlim((0,0.125)) # Plot each simulated trajectory for i in range(cycles): axs[0].plot(T[:], N_A[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[1].plot(T[:], N_B[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[2].plot(T[:], N_AB[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) plt.show() . We can see here that even though the trajectories from tau-leaping are less exact the procedure has produced smoother average results for the same number of simulation steps (approximately the same running time). . And again we can look at the distribution at time=0.025: . time = 0.025 N_A_time = np.zeros(cycles) N_B_time = np.zeros(cycles) N_AB_time = np.zeros(cycles) for i in range(cycles): for j in range(1, steps): if T[j] &gt;= time and T[j-1] &lt; time: N_A_time[i] = N_A[i,j] N_B_time[i] = N_B[i,j] N_AB_time[i] = N_AB[i,j] # If trajectory doesn&#39;t span far enough take latest observation if T[i, steps] &lt; time: N_A_time[i] = N_A[i, steps] N_B_time[i] = N_B[i, steps] N_AB_time[i] = N_AB[i, steps] plt.hist(N_A_time, density=True, bins=np.arange(35), label=&quot;A&quot;, color=&#39;lightgrey&#39;) plt.hist(N_B_time, density=True, bins=np.arange(35), label=&quot;B&quot;, color=&#39;dimgrey&#39;) plt.hist(N_AB_time, density=True, bins=np.arange(35), label=&quot;AB&quot;, color=&#39;red&#39;) plt.legend() plt.show() . Here we can see improved distributions with (what appears to be) less noise. To justify this we would want to run more tests however. . Note: this is the most basic implementation of the tau-leaping procedure. In certain situations this needs to be manipulated to improve behaviour, for example if the poisson draw is often large enough to cause the population to go negative then a truncation procedure (or acceptance/rejection scheme) needs to be employed in such a way as to retain the average reaction rates. In this simple example we ignore this complication, there are some occasions where the number of $A$ molecules hits zero so there will be some bias in the estimates presented above. . Adaptive Tau-Leaping . The &quot;problem&quot; with the tau-leaping method above is that it is very sensitive to the leap size. It is also possible that as the system evolves what started out as a &quot;good&quot; leap size becomes &quot;bad&quot; as the dynamics change. One possible solution to this is to use an &quot;adaptive&quot; method whereby the leap size varies depending on the dynamics. The main idea is to limit the leap sizes from being so large that the populations can reach an unfavourable state (e.g. negative population sizes) or jump to a state &quot;too far away&quot;. . There are many ways to do this, one of the more popular was developed by Y. Cao and D. Gillespie in 2006. In order to describe the method we will need to introduce some notation. We let $ mathbf{X}_t = left( X_t^i right)_{i=1}^N$ to be a vector of population sizes at time t. We intorduce variables $v_{ij}$ to represent the change in component $i$ of the population when an event $j$ occurs - we will use $i$ indices to refer to components of the population vector and $j$ indices to refer to event types. $R_j( mathbf{X}_t)$ is the rate of event $j$ with population $ mathbf{X}_t$. In this method we look to bound the relative shift in rates at each step by a parameter $ epsilon$. . In pseudo-code we can describe the process via: . Initialize - Set initial conditions for the population | Calculate event rates - $R_j$ for each event types depending on state of the system | Calculate auxiliary variables - for each state component $i$ begin{align} mu_i &amp;= sum_j v_{ij} R_j sigma_j^2 &amp;= sum_j v_{ij}^2 R_j end{align} | Select highest order event - for each state component $i$, denote the rate of this event as $g_i$ | Calculate time step $$ tau = min_i left( min left( frac{max left( frac{ epsilon X_i}{g_i}, 1 right)}{| mu_i|} , frac{max left( frac{ epsilon X_i}{g_i}, 1 right)^2}{ sigma_j^2} right) right) $$ | Monte-Carlo - for each event type sample number of events occuring within the leap step $ tau$ | Update - Update system state based on number of events | Repeat - Repeat steps 2-7 until some stopping criteria is met | Step 4. involves selecting the highest order event - this essentially is the &quot;most important&quot; event that each $i$ is involved in. For very complex systems this may not be an obvious thing to do and will require more finesse. We can see that aside from steps 3-5 this is the exact same scheme as the previous example. . There are other adaptive leaping schemes that one could use each with different pros and cons. . We can modify the code above to use this scheme via: . # An implenetation of the Gillespie algorithm # With adaptive tau-leaping # Applied to a pair of reactions: # A + B -&gt; AB # AB -&gt; A + B import numpy as np import matplotlib.pyplot as plt from scipy.stats import poisson %matplotlib inline # Fix random seed for repeatability np.random.seed(123) ###### Fix model parameters ###### N_A0 = 25 # Initial number of A molecules N_B0 = 35 # Initial number of B molecules N_AB0 = 5 # Initial number of AB molecules rf = 2 # Forward reaction rate rb = 1 # Backwards reaction rate eps = 0.03 # Epsilon adaptive rate steps = 25 # Number of reactions per trajectory cycles = 100 # Number of trajectories iterated over # Set up holder arrays T = np.zeros((cycles, steps+1)) N_A = np.zeros((cycles, steps+1)) N_B = np.zeros((cycles, steps+1)) N_AB = np.zeros((cycles, steps+1)) # Store initial conditions N_A[:,0] = N_A0 N_B[:,0] = N_B0 N_AB[:,0] = N_AB0 ###### Main Code Loop ###### for i in range(cycles): for j in range(steps): # Calculate updated reaction rates Rf = rf * N_A[i,j] * N_B[i,j] Rb = rb * N_AB[i,j] # Calculate auxiliary variables mu_A = Rf - Rb mu_B = Rf - Rb mu_AB = Rb - Rf sig2_A = Rf + Rb sig2_B = Rf + Rb sig2_AB = Rf + Rb # Select highest order reactions g_A = Rf g_B = Rf g_AB = Rb # Caclulate internal maxima - taking care of divide by zero if g_A == 0: max_A = 1 else: max_A = max(eps*N_A[i,j]/g_A,1) if g_B == 0: max_B = 1 else: max_B = max(eps*N_B[i,j]/g_B, 1) if g_AB == 0: max_AB = 1 else: max_AB = max(eps*N_AB[i,j]/g_AB, 1) # Calculate minima for each component min_A = min(max_A / abs(mu_A), max_A**2 / sig2_A) min_B = min(max_B / abs(mu_B), max_B**2 / sig2_B) min_AB = min(max_AB / abs(mu_AB), max_AB**2 / sig2_AB) # Select tau leap size leap = min(min_A, min_B, min_AB) # Calculate number of reactions by type uf = np.random.random() ub = np.random.random() Nf = poisson.ppf(uf, Rf*leap) Nb = poisson.ppf(ub, Rb*leap) # Apply limits to prevent negative population Limitf = min(N_A[i,j], N_B[i,j]) Limitb = N_AB[i,j] Nf = min(Nf, Limitf) Nb = min(Nb, Limitb) # Update populations and times N_A[i,j+1] = N_A[i,j] + Nb - Nf N_B[i,j+1] = N_B[i,j] + Nb - Nf N_AB[i,j+1] = N_AB[i,j] + Nf - Nb T[i,j+1] = T[i,j] + leap # Calculate an average trajectory plot ave_steps = 100 T_max = T.max() # Set up average array holders T_ave = np.linspace(0,T_max,ave_steps+1) N_A_ave = np.zeros(ave_steps+1) N_B_ave = np.zeros(ave_steps+1) N_AB_ave = np.zeros(ave_steps+1) N_A_ave[0] = N_A0 N_B_ave[0] = N_B0 N_AB_ave[0] = N_AB0 # Pass over average array entries for i in range(1, ave_steps+1): tmax = T_ave[i] A_sum = 0 B_sum = 0 AB_sum = 0 t_count = 0 # Pass over each trajectory and step therein for j in range(cycles): for k in range(steps): if T[j,k] &lt;= tmax and T[j,k+1] &gt; tmax: t_count += 1 A_sum += N_A[j,k] B_sum += N_B[j,k] AB_sum += N_AB[j,k] # Caclulate average - taking care if no samples observed if t_count == 0: N_A_ave[i] = N_A_ave[i-1] N_B_ave[i] = N_B_ave[i-1] N_AB_ave[i] = N_AB_ave[i-1] else: N_A_ave[i] = A_sum / t_count N_B_ave[i] = B_sum / t_count N_AB_ave[i] = AB_sum / t_count ###### Plot Trajectories ###### fig, axs = plt.subplots(3, 1, figsize=(10,20)) axs[0].plot(T_ave, N_A_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[0].set_title(&#39;Number A Molecules&#39;) axs[0].set_ylim((0,35)) axs[0].set_xlim((0,0.125)) axs[1].plot(T_ave, N_B_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[1].set_title(&#39;Number B Molecules&#39;) axs[1].set_ylim((0,35)) axs[1].set_xlim((0,0.125)) axs[2].plot(T_ave, N_AB_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[2].set_title(&#39;Number AB Molecules&#39;) axs[2].set_xlabel(&quot;Time&quot;) axs[2].set_ylim((0,35)) axs[2].set_xlim((0,0.125)) for i in range(cycles): axs[0].plot(T[i,:], N_A[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[1].plot(T[i,:], N_B[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[2].plot(T[i,:], N_AB[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) plt.show() . As with the previous tau-leaping algorithm there the trajectories are noticably less exact than the original Gillespie formulation. However owing to the variable time step the trajectories do appear slightly less granular than in the previous tau-leaping formulation. Again the average trajectory is smoother than in the original method for (approximately) the same amount of run-time. . Looking at the time=0.025 distributions once again: . time = 0.025 N_A_time = np.zeros(cycles) N_B_time = np.zeros(cycles) N_AB_time = np.zeros(cycles) for i in range(cycles): for j in range(1, steps+1): if T[i,j] &gt;= time and T[i,j-1] &lt; time: N_A_time[i] = N_A[i,j] N_B_time[i] = N_B[i,j] N_AB_time[i] = N_AB[i,j] if T[i, steps] &lt; time: N_A_time[i] = N_A[i, steps] N_B_time[i] = N_B[i, steps] N_AB_time[i] = N_AB[i, steps] plt.hist(N_A_time, density=True, bins=np.arange(35), label=&quot;A&quot;, color=&#39;lightgrey&#39;) plt.hist(N_B_time, density=True, bins=np.arange(35), label=&quot;B&quot;, color=&#39;dimgrey&#39;) plt.hist(N_AB_time, density=True, bins=np.arange(35), label=&quot;AB&quot;, color=&#39;red&#39;) plt.legend() plt.show() . Again the distributions for a fixed time appear to have become less noisy. . In a small scale simple example such as this we would expect any &quot;improvements&quot; from a scheme like this to be minor, as we run more complicated examples we would expect a bigger performance differential. . Conclusion . In this blog post we have seen 3 variations of the Gillespie algorithm: the original, tau-leaping and an adaptive tau leaping scheme. We have seen that the original variation produces exact simulations of a specified system, via tau leaping we have seen that we can approximate this and still get reasonable results in a quicker time. Which is important when dealing with more complicated and larger systems. . At this point we should also see the flexibility inherent in the Gillespie framework and why it has been applied in many different areas. We can also see that the algorithm is a &quot;gateway&quot; into agent-based schemes - instead of using a purely stochastic mechanism for selecting reaction types/times we could (for example) model individual molecules moving around in space and if they come within a certain radius of each other at a certain speed then a reaction occurs. This would turn the Gillespie algorithm into a full agent-based model for reaction kinetics (the benefit of doing this in most situations is likely slim to none however). .",
            "url": "https://www.lewiscoleblog.com/gillespie-algorithm",
            "relUrl": "/gillespie-algorithm",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Flock vs Predator",
            "content": "If you have ever looked up at the sky in early evening you might have been greeted by the spectacle of many birds flying in unison creating elaborate and complex patterns in the sky. Or perhaps on the discovery channel you have seen a documentary on the sea and have seen schools of fish creating similar elaborate patterns. . . There are many evolutionary reasons for why these patterns may form: firstly is the &quot;safety in numbers&quot; concept whereby having many individuals grouped together protects against predators. Predators can only attack the edge of the pattern, the number of individuals on the edges is significantly fewer than the individuals in the middle of the pattern - thus increasing safety for the largest number of individuals. The patterns may also trick predators into thinking the pattern is some larger threatening entity and so less likely to attack in the first place. There is also the possiblity that by creating a group pattern that is large enough to be seen from far away it attracts more individuals to join the group. . This phenomena is known as &quot;flocking&quot; - in this blog post we will investigate how flocks form. . An Agent Based Approach . One way in which we could reverse engineer these patterns is to have a &quot;central controller&quot; that tells where each flock member should be located at each moment in time. While this may produce results that are seemingly indistinguishable from real-world flocking behaviour we would be very hard pressed to argue that this is what happens in nature. Instead we would like each member to have some level of autonomy and a simple limited set of rules that when combined produces the desired result - if such a model can be found it will allow us to analyse and study flock formation in more detail. Any model containing many members each following simple rules within an environment can be classified as an &quot;agent based model&quot; (members herein being referred to as agents). Agent based models span further than flocking models, they can be used anywhere where it&#39;s possible to define simple rules for local behaviour and where large scale/population models (e.g. differential equations) do not provide details on the scales that are of interest to us. Agent based models have been used to model a whole manner of areas from biologically inspired predator/prey interactions, spreading of infection through to the social sciences and idea spreading and the economy. . Model Design . Flocking models have been studied fairly extensively. In this blog I will present my implementation of a flocking model, there will be &quot;prey&quot; agents who are large in number and form flocks to try and protect themselves. There will be a small number of &quot;predator&quot; agents who try and eat/kill the prey. Each class of agent has their own dynamic rules for how they move in space. . Rules for Prey . For the prey agents we will assume that there are 5 forces operating to direct the agents. These forces are equivalent to acceleration vectors, we will integrate over time to change the velocity and integrate velocity to get the change in position. We can describe the forces at bay as follows: . Flocking Forces - This represents the attraction of an agent to move towards a density of other agents (join the flock). To do this we consider the velocity vector of agents which we denote $ mathbf{v_i}$. We assume that we will only flock towards others if they are near us, we thus have a &quot;flocking radius&quot; which we denote $r_f$ which is how far the agent can &quot;see&quot;. We define a vector quantity: $ mathbf{V_i} = sum_{j in N_{r_f}(i)} mathbf{v_j} $. Where we denote the set of agents within a neighbourhood of radius $r$ of agent $i$ as: $N_{r}(i)$. The strength of this force is defined by a parameter $ alpha$. We then define the flocking force applied to agent $i$ as: $$ mathbf{F}_{flock}^i = alpha frac{ mathbf{V_i}}{| mathbf{V_i}|}$$ | Repulsive Forces - If we are in a crowd we like our own space, if somebody gets to close we move away. At the most extreme level there is a limit on how close one can get to another owing to the size of their bodies. The repulsive force acts to stop agents getting too close together. This force operates only on a radius $r_0$ - if somebody is very far away they won&#39;t impact how we move in space. We denote $| mathbf{d_{ij}}|$ to be the scalar distance (magnitude) between agents $i$ and $j$. We use boldface $ mathbf{d_{ij}}$ to represent the vector distance. The strength of this force is defined by the parameter $ epsilon$. We can then express this force applied to agent $i$ as: $$ mathbf{F}_{rep}^i = epsilon sum_{j in N_{r_0}(i)} left(1 - frac{| mathbf{d_{ij}}|}{r_0} right)^{ frac{3}{2}} frac{ mathbf{d_{ij}}}{| mathbf{d_{ij}}|} $$ | Self-Propulsion Forces - This force represents a desire to &quot;keep moving&quot;. In some situations there is a tendency for sustained movement - for example in a march there is a desire to keep moving at a constant velocity. We represent this by the self-propulsion force. The strength of this force is controlled via a parameter $ mu$. For agent $i$ we can express this as: $$ mathbf{F}_{prop}^i = mu left(v_0 - | mathbf{v_i}| right) frac{ mathbf{v_i}}{| mathbf{v_i}|} $$ | Random Forces - In the model we may wish to allow for random forces, that is each agent has it&#39;s own autonomy. To do this we will just apply a uniform random draw. We will use a parameter $r_{amp}$ (random amplitude) to denote the strength of this force. | Predator-Repulsive Forces - One of the driving forces for any prey is to escape predators. In this model we will assume that this force trumps all others, that is if a predator is close an agent will run away and ignore all other desires of flocking/self-propulsion/etc. This operates in the exact same way as the repulsive force above but instead of repulsion from other agents it considers repulsion from predators. We define the radius over which this occurs as $r_p$ and the strength of this force $ delta$ - these replace $r_0$ and $ epsilon$ in the formula above. We also apply a &quot;speed limit&quot; to the agents to prevent them travelling arbitrarily quickly. | . Rules for Predators . For predators the dynamics are much simpler. We assume that a predator has infinite vision, it will look for the nearest prey and run towards it at a fixed speed. However we will add a (possibly small) random component to the movement - this could represent indecisiveness on the part of the predator or the evasive tactics employed by the prey. The only model assumptions for predators is the predator speed $pv_{max}$ and the proportion of movement that is random $pr_{amp}$. . Environment . We will assume a simple environment consisting of a unit-square. The boundaries will be periodic (i.e. the left edge will &quot;connect&quot; to the right edge and top to bottom). Like the game asteroids this assumes the dynamics take place on the surface of a torus. This assumption simplifies the dynamics since there is no need to consider any boundary conditions - if we assume a fixed &quot;wall&quot; around the environment we would need to define rules as to what happens when an agent tries to cross (e.g. do they &quot;reflect&quot; off walls? Do they &quot;stick&quot;? Do they slow down and change direction to avoid hitting the wall all together?). For convenience in calculating distances in the periodic boundary world we have a helper function called &quot;buffer&quot; which replicates any agents near the walls and places them outside the unit-square - this ensures that distances are calculated correctly. . Other Model Assumptions . There are a few other modelling assumptions made, one of the major decisions of any agent based model is deciding whether to use synchronous or asynchronous updating. We will assume synchronous updating here, each agent will make their movements at the same time. This avoids the problem of agents who are updated &quot;later&quot; within a tick of an asynchronous scheme gaining an &quot;advantage&quot; over those updated first (this would be a particular issue for predator-prey relations where if a predator waits until the prey has moved it is more likely to catch the prey!) . To avoid the problem of a predator following a single prey-agent aimlessly we will use an &quot;eat&quot; dynamic. Here if a predator gets within a small distance of a prey it will &quot;eat&quot; it, the prey will disappear and instantly re-spawn to a random location in the environment with a random starting velocity. There will be no limit to how many prey a predator can eat in a single step and it is assumed the predator can eat without stopping/slowing down. . The Code . An implementation of this model can be seen below. I use matplotlib&#39;s animaton feature to make an animation of the system through time. In the animation prey are represented by small blue crosses and predators by larger red diamonds. . import numpy as np from matplotlib import animation import matplotlib.pyplot as plt from IPython.display import HTML %matplotlib inline ###### Model Parameters ###### dt = 0.01 # Time step num_frames = 250 # Number of animated frames burn = 1000 # Number of burn in iterations # Flock Parameters N = 250 # Number of flocking agents r0 = 0.1 # Repulsion force range eps = 2.0 # Repulsion force strength rf = 0.25 # Flocking force range alpha = 10.0 # Flocking force strength v0 = 0.25 # Target speed vmax = 0.75 # Maximum speed mu = 0.1 # Self-Propulsion force ramp = 1.0 # Random force rp = 0.5 # Predator-repulsion force range delta = 5.0 # Predator-repulsion force strength # Predator Parameters M = 3 # Number of predator agents pv_max = 1.0 # Predator speed pramp = 0.25 # Predator random force eat = 0.025 # Eating radius ###### Fix Random Seed ###### seed = 123 np.random.seed(seed) ###### Helper Functions ###### def buffer(rb, x, y, vx, vy): &quot;&quot;&quot; buffer(rb, x, y, vx, vy) Takes copies of agents near the edges of the unit square and extends outwards by a buffer margin rb. This makes it easier to calculate distances between agents for flocking and repulsive forces Inputs: rb - buffer range x - array of x coordinate positions [0,1] y - array of y coordinate positions [0,1] vx - array of x-component of velocity vy - array of y-component of velocity Outputs: nb, xb, yb, vxb, vyb - Buffered copies of inputs &quot;&quot;&quot; _N = x.shape[0] # Initialize buffer arrays xb[0:_N] = x yb[0:_N] = y vxb[0:_N] = vx vyb[0:_N] = vy # nb is a counter already have _N agents nb = _N-1 for i in range(nb+1): if x[i] &lt;= rb: # left edge nb+=1 xb[nb] = x[i]+1 yb[nb] = y[i] vxb[nb] = vx[i] vyb[nb] = vy[i] if x[i] &gt;= 1 - rb: # right edge nb+=1 xb[nb] = x[i]-1 yb[nb] = y[i] vxb[nb] = vx[i] vyb[nb] = vy[i] if y[i] &lt;= rb: # bottom edge nb+=1 xb[nb] = x[i] yb[nb] = y[i]+1 vxb[nb] = vx[i] vyb[nb] = vy[i] if y[i] &gt;= 1 - rb: # top edge nb+=1 xb[nb] = x[i] yb[nb] = y[i]-1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &lt;= rb) and (y[i] &lt;= rb): # bottom left corner nb+=1 xb[nb] = x[i]+1 yb[nb] = y[i]+1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &lt;= rb) and (y[i] &gt;= 1 - rb): # top left corner nb+=1 xb[nb] = x[i]+1 yb[nb] = y[i]-1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &gt;= 1 - rb) and (y[i] &lt;= rb): # bottom right corner nb+=1 xb[nb] = x[i]-1 yb[nb] = y[i]+1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &gt;= 1 - rb) and (y[i] &gt;= 1 - rb): # top right corner nb+=1 xb[nb] = x[i]-1 yb[nb] = y[i]-1 vxb[nb] = vx[i] vyb[nb] = vy[i] return nb, xb, yb, vxb, vyb def force(nb, xb, yb, vxb, vyb, x, y, vx, vy, mb, pxb, pyb): &quot;&quot;&quot; force(nb, xb, yb, vxb, vyb, x, y, vx, vy, mb, pxb, pyb) Calculate force applied to agents (x,y) divided into: - flocking force (flockx, flocky) - repulsive force (repx, repy) - Self propulsion (fpropx, fpropy) - Random force (frandx, frandy) - Predator repulsive force (fpredx, fpredy) Inputs: nb - number of buffered agents xb - buffered agent x position array yb - buffered agent y position array vxb - buffered agent velocity x coordinate array vyb - buffered agent velocity y coordinate array x - agent x position array y - agent y position array vx - agent x coordinate velocity array vy - agent y coordinate velocity array mb - number buffered predator agents pxb - predator x-coordinate position array pyb - predator y-coordinate position array Global Variables Called: rf - Flocking force range r0 - Repulsion force range alpha - Flocking force strength mu - Self propulsion force v0 - Target velocity ramp - Random force strength rp - Predator repulsion force range Outputs: fx, fy - x and y coordinate forces &quot;&quot;&quot; _N = x.shape[0] for i in range(_N): repx = 0 repy = 0 flockx = 0 flocky = 0 nflock = 0 fpredx = 0 fpredy = 0 for j in range(nb): d2 = (xb[j] - x[i])**2 + (yb[j] - y[i])**2 if (d2 &lt;= rf**2) and (i != j): flockx += vxb[j] flocky += vyb[j] nflock += 1 if d2 &lt;= r0**2: d = np.sqrt(d2) repx += eps*(1- d / r0)**1.5 * (x[i] - xb[j]) / d repy += eps*(1- d / r0)**1.5 * (y[i] - yb[j]) / d normflock = np.sqrt(flockx**2 + flocky**2) if nflock == 0: normflock = 1 flockx = alpha * flockx / normflock flocky = alpha * flocky / normflock vnorm = np.sqrt(vx[i]**2 + vy[i]**2) fpropx = mu * (v0 - vnorm) * (vx[i] / vnorm) fpropy = mu * (v0 - vnorm) * (vy[i] / vnorm) frandx = ramp * (2*np.random.random() - 1) frandy = ramp * (2*np.random.random() - 1) fx[i] = (flockx + frandx + fpropx + repx) fy[i] = (flocky + frandy + fpropy + repy) for k in range(mb): d2 = (pxb[k] - x[i])**2 + (pyb[k] - y[i])**2 if (d2 &lt;= rp**2): d = np.sqrt(d2) fpredx += delta*(1- d / rp)**1.5 * (x[i] - pxb[k]) / d fpredy += delta*(1- d / rp)**1.5 * (y[i] - pyb[k]) / d if fpredx**2 + fpredy**2 &gt; 0: fx[i] = fpredx fy[i] = fpredy return fx, fy def pred_dist(px1, py1, x1, y1): &quot;&quot;&quot; pred_dist(px1, py1, x1, y1) Returns an array of distances from predator to an array of prey Inputs: px1 - predator x coordinate py1 - predator y coordinate x1 - array of prey x coordinates y1 - array of prey y coordinates Outputs: distance array &quot;&quot;&quot; dx1 = np.abs(px1-x1) dx2 = np.abs(1+px1-x1) dx3 = np.abs(px1-(1+x1)) dy1 = np.abs(py1-y1) dy2 = np.abs(1+py1-y1) dy3 = np.abs(py1-(1+y1)) dx = np.minimum(np.minimum(dx1, dx2), dx3) dy = np.minimum(np.minimum(dy1, dy2), dy3) return np.sqrt(dx**2+dy**2) def direction(start_x, start_y, target_x, target_y): &quot;&quot;&quot; direction(start_x, start_y, target_x, target_y) Find a unit direction vector from a starting point to a target Inputs: start_x - starting point x coordinate start_y - starting point y coordinate target_x - target point x coordinate target_y - target point y coordinate Returns dir_x - direction in x dir_y - direction in y &quot;&quot;&quot; dx1 = target_x - start_x dx2 = target_x - (start_x + 1) dx3 = target_x - (start_x - 1) dy1 = target_y - start_y dy2 = target_y - (start_y + 1) dy3 = target_y - (start_y - 1) if abs(dx1) &lt;= (abs(dx2) and abs(dx3)): dir_x = dx1 elif abs(dx2) &lt;= (abs(dx1) and abs(dx3)): dir_x = dx2 elif abs(dx3) &lt;= (abs(dx1) and abs(dx2)): dir_x = dx3 if abs(dy1) &lt;= (abs(dy2) and abs(dy3)): dir_y = dy1 elif abs(dy2) &lt;= (abs(dy1) and abs(dy3)): dir_y = dy2 elif abs(dy3) &lt;= (abs(dy1) and abs(dy2)): dir_y = dy3 return dir_x, dir_y def unitize(vec_x, vec_y): &quot;&quot;&quot; unitize(vec_x, vec_y) Returns a vector of same direction as input but with unit size &quot;&quot;&quot; norm = np.sqrt(vec_x**2 + vec_y**2) return vec_x/norm, vec_y/norm def update(): &quot;&quot;&quot; update() Update function that updates position and velocities of prey and predator agents by one time step Inputs: None Outputs: None (updates values of global variables) &quot;&quot;&quot; global x global y global vx global vy global px global py global vpx global vpy global eat_count x_old = x.copy() y_old = y.copy() nb, xb, yb, vxb, vyb = buffer(max(r0,rf), x, y, vx, vy) mb, pxb, pyb, vpxb, vpyb = buffer(rp, px, py, vpx, vpy) fx, fy = force(nb, xb, xb, vxb, vyb, x, y, vx, vy, mb, pxb, pyb) # Use force to calculate speeds vx += fx * dt vy += fy * dt # Apply speed limit of maximum velocity vmod = np.sqrt(vx**2 + vy**2) vmult = np.where(vmod &lt; vmax, vmod, vmax) / vmod vx *= vmult vy *= vmult # Calculate new positions and re-position in unit square x += vx * dt y += vy * dt x = (1 + x) % 1 y = (1 + y) % 1 # Predator calculation for i in range(M): pred_x = px[i] pred_y = py[i] # Calculate distances to prey dist = pred_dist(pred_x, pred_y, x_old, y_old) # If prey within eat range then eat and regenerate prey for j in range(N): if dist[j] &lt; eat: x[j] = np.random.random() y[j] = np.random.random() vx[j] = 2*np.random.random() - 1 vy[j] = 2*np.random.random() - 1 eat_count += 1 # Find closest prey min_dist = np.min(dist) mask = dist == min_dist prey_x = x_old[mask][0] prey_y = y_old[mask][0] # Calculate direction to move in vpx_t, vpy_t = direction(pred_x, pred_y, prey_x, prey_y) vpx_t, vpy_t = unitize(vpx_t, vpy_t) # Simulate random component and normalize vpx_rand = 2*np.random.random() - 1 vpy_rand = 2*np.random.random() - 1 vpx_rand, vpy_rand = unitize(vpx_rand, vpy_rand) # Combine prey-direction and random components vpx[i] = (1-pramp)*vpx_t + pramp*vpx_rand vpy[i] = (1-pramp)*vpy_t + pramp*vpy_rand vpx[i], vpy[i] = unitize(vpx[i], vpy[i]) # Scale to maximum velocity vpx[i] *= pv_max vpy[i] *= pv_max # Move predator forward px[i] += vpx[i]*dt py[i] += vpy[i]*dt # Re-position predators to unit square px = (1 + px) % 1 py = (1 + py) % 1 ###### Main Code ###### ###### Set Up Arrays ###### # Flock arrays x = np.zeros(N) y = np.zeros(N) vx = np.zeros(N) vy = np.zeros(N) fx = np.zeros(N) fy = np.zeros(N) # Flock buffer arrays xb = np.zeros([4*N]) yb = np.zeros([4*N]) vxb = np.zeros([4*N]) vyb = np.zeros([4*N]) # Predator arrays px = np.zeros(M) py = np.zeros(M) vpx = np.zeros(M) vpy = np.zeros(M) # Predator buffer arrays pxb = np.zeros([4*M]) pyb = np.zeros([4*M]) vpxb = np.zeros([4*M]) vpyb = np.zeros([4*M]) # Eat counter initialization eat_count = 0 # Initialize positions and velocities for i in range(N): x[i] = np.random.random() y[i] = np.random.random() vx[i] = 2*np.random.random() - 1 vy[i] = 2*np.random.random() - 1 for i in range(M): px[i] = np.random.random() py[i] = np.random.random() vpx[i] = 2*np.random.random() - 1 vpy[i] = 2*np.random.random() - 1 # Burn simulations to improve initial conditions for i in range(burn): update() # Reset Eat count eat_count = 0 ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, 1), ylim=(0, 1)) axes.set_xticks([], []) axes.set_yticks([], []) scatter_prey = axes.scatter(x, y, marker=&#39;x&#39;, s=25, c=&#39;Blue&#39;) scatter_pred = axes.scatter(px, py, marker=&#39;D&#39;, s=100, c=&#39;Red&#39;) # Define animation step def animate(frame): update() prey_data = np.array((x, y)).T scatter_prey.set_offsets(prey_data) pred_data = np.array((px, py)).T scatter_pred.set_offsets(pred_data) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=num_frames, interval=50) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect Since it&#39;s a bit hard to follow all that is going on with regards to eating prey, in this animation we can track the number of prey eaten: . print(&quot;Number of eaten prey per predator per frame:&quot;, eat_count / M / num_frames) . Number of eaten prey per predator per frame: 0.05466666666666666 . As the dynamics evolve we can see flocks form, these are occasionally interrupted as a predator &quot;breaks&quot; them apart. When this happens the predator can get &quot;confused&quot; and doesn&#39;t know which flock is the one it should follow - showing how flocking is beneficial to the prey agents! Through changing parameters we can observe a wide variety of behaviours owing to the relatively large parameter space. . These emergent behaviours are particularly interesting. It is, seemingly, impossible to predict how the dynamics will behave without running a simulation (or if it is predictable it is not apparently obvious). The model rules seem to exhibit a &quot;computational irreducibility&quot; - there is no &quot;short-cut&quot; to computing them. This concept is prevelant in the social sciences, we can study a lot about individuals yet have no way of extending these ideas to populations. As such many problems in complex systems (e.g. social sciences, some areas of biology, etc) are in some sense incompatible with the prevelant scientific principle of reductionism - the study of irreducible computation (and agent based modelling) will no doubt have a large impact on how we understand these systems in the future. . Suggested Extensions or Improvements . Part of the beauty of agent based models such as the one presented here is we can modify the code to investigate different behaviours. Some things that you may want to consider for yourselves include: . Study parameter space - The obvious first thing to do is study the parameters of this model. There are a large number of parameters and so a large variation in potential behaviour. In particular we would like to create a &quot;zoo&quot; of behaviours - a taxonomy of what is possible with the model and whether this captures what is observed in nature. We can also look for &quot;reasonable ranges&quot; of parameters that produce desired outputs. This could also include the creation of summary statistics describe the behaviours. | Improved predator dynamics - At the moment the predators are not very smart, they are incapable as working as a team. We may want to consider the effect of adding a &quot;repulsion force&quot; betwen predators so that they can cover the space more effectively. We could also allow for the predators to take the speed of prey into account, so a close fast moving prey may be less attractive than a slower prey slightly further away. The predators could &quot;learn&quot; too - perhaps after being well fed they can move more quickly or have a larger eating radius? Perhaps we want to have limited vision for predators? We could really go to town with this and implement a reinforcement learning algorithm to look for &quot;successful&quot; strategies for eating prey. | Non-static population size - The model so far assumes the populations stay the same. We may wish to allow for the prey to decrease as they are eaten. Perhaps the predators, when well fed, can reproduce and if they don&#39;t eat for a long time they die off. | Remove homogeneity - Each agent current has the same dynamics properties. We may wish to allow for some heterogeneity in the populations - some prey may be slower/have less good vision and so are easier targets. What if we create classes of prey/predator agents? Perhaps some are highly flock seeking while others prefer to be alone. | Environment - We have only looked at this model with periodic boundary conditions. What happens if there are &quot;walls&quot; at the edges so prey can get trapped? What happens if we put barriers/obstacles in the environment? | Different forces - We could also consider adding different forces or replacing the existing ones. For example we may not believe that agents are able to effectively calculate forces based on all their neighbours within a radius, instead perhaps we want to refactor the model so that the agents consider their N-nearest neighbours only instead. Perhaps we could imagine that there is a section of the environment where &quot;food&quot; for the prey exists so there is a force that draws them towards that area. Perhaps we want to implement a force that &quot;slows down&quot; the prey if they have been moving quickly (effectively the agents get &quot;tired&quot;). It&#39;s not hard to think of other possibilities. | New agent types - Suppose that instead of a simple predator-prey relation we have multiple species, some happily coexisting while others eat the other. The &quot;prey&quot; in the existing model could have an attractive force to a species higher up the food chain that eats their predator! We could also investigate &quot;rock paper scissors&quot; type behaviour where no one species is &quot;dominant&quot; by design. | Performance enhancements - The code as presented is reasonably efficient but could be made to run quicker. Replacing the current code with Cython/Numba would be the obvious choice but there are also opportunities for more structural changes that could improve efficiency. For example we could approximate the forces using a quadtree (as in the Barnes-Hut N-body algorithm). | . Conclusion . In this blog post we have seen how to implement a basic flocking algorithm with predators added to the environment. Although each agent in the model is &quot;dumb&quot;, complex behaviours of the population can emerge out of relatively simple rules. From this we can notice some interesting applications for these methods - for example we could model crowd behaviour in rock concerts/sports events to increase safety, or we could model evacuation procedures for large buildings/venues that could influence their design. We could also use these techniques for video games and CGI in movies where we may wish to generate large crowds that look &quot;natural&quot; and not repeated/regular. . Hopefully this post acts as a good motivation for the use of agent-based models in general and in particular how they can be used to study emergent behaviours in complex systems. .",
            "url": "https://www.lewiscoleblog.com/flocking-model",
            "relUrl": "/flocking-model",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Spin Glass Models 5: Applications and Extensions",
            "content": ". This is the fifth blog post in a series - you can find the previous blog post here . . If you have read my previous few blog posts you will have some appreciation for spin-glasses and some models used to study and simulate them. In this blog post we will look a few applications and extensions of spin-glasses. . Potts Spin-Glass Model . The Potts spin-glass model is a generalization of the Ising model we looked at previously. Whereas the Ising model allows only &quot;up&quot; or &quot;down&quot; spins the Potts model is a &quot;q-spin&quot; model where spins can take any of $q$ possible values. The Hamiltonian is typically specified as: $$H = - sum_{x,y} J_{xy} delta( sigma_x sigma_y) - sum_x h_x sigma_x $$ Where $ delta(.)$ represents a Kronecker delta function. For $q=2$ this is equivalent to the Ising model. . Another representation of a Potts model is the vector Potts model, here spins take values as defined as angles: $$ theta_i = frac{2 pi i}{q} $$ For $i$ between 1 and $q$. The Hamiltonian can then be defined as: $$H = - sum_{x,y} J_{xy} cos( sigma_x - sigma_y) - sum_x h_x sigma_x $$ The case for $q to infty$ is called the XY-model. . On a 2d square lattice we find that a phase transition exists for $q&gt;4$ for $q leq 4$ there is a 2nd order phase transition (as with the Ising model $q=2$). The techniques and simulation procedures discussed previously can be adapted to the Potts model with minimal change. . Other generalizations of this model have been used in various physical and biological applications. The Potts model has also been used in image processing. . Markov-Random-Field . The Ising model can also be thought of as the simplest example of a Markov-Random-Field (MRF). The theory and application of these could easily take a blog post (or more) by itself. Instead we will just look at some basic definitions and the major theorems. . A Markov-Random-Field is an example of a probabilistic graphical model (PGM). We define a MRF via a set of random variables and an undirected graph: $(X, G)$. Unlike other some other types of PGM an MRF graph is undirected and there is no requirement for the graph to be acyclic. For a given graph $G = (V,E)$ with specified vertices and edges, a set of random variables $(X_v)_{v in V}$ forms a Markov-Random-Field over G if they satisfy the folowing properties: . Pairwise Markov Property: any two non-adjacent variables are conditionally independent given all other variables: $$X_u perp X_v | X_{V / {u,v }}$$ | Local Markov Property: A variable is conditionally independent of all other variables given its neighbors: $$X_u perp X_{V /N(u)} | X_{N(u)} $$ Where $N(u)$ represents the set of neighbours of vertex $u$ according to $G=(V,E)$ | Global Markov Property: Any two subsets of variables are conditionally independent given a separating subset: $$X_A perp X_B | X_S $$ Where every path from a node in $A$ to a node in $B$ passes through $S$. | One useful way of specifying the MRF is through the conecpt of clique potentials. A subgraph $C subset G$ is a clique if it is a fully connected subgraph, the set of all cliques of $G$ is denoted: $Cl(G)$. We can then specify a joint distribution of random-variables $ mathbf{X} = (X_v)_{v in V}$ via clique potential functions: $ phi_C(X_C)$ such that: $$P( mathbf{X} = mathbf{x}) = prod_{C in Cl(G)} phi_C (x_C) $$ We note that every joint distribution specified in this was is a MRF. However the reverse is not necessarily true, we can however factorise a joint distribution this way if: . The distribution is strictly positive (known as the Hammersley-Clifford theorem) | The graph is chordal | . A convenient form of clique potential is the exponential form which we can denote as: $$ P( mathbf{X} = mathbf{x}) = frac{1}{Z} exp left( sum_{C in Cl(G)} w_C f_C(x_C) right)$$ With: $$ Z = sum_{ mathbf{x}} exp left( sum_{C in Cl(G)} w_C f_C(x_C) right) $$ Which is nothing more than a general form of the Gibbs distribution we looked at in previous blog posts. To see this for the Ising model the cliques are adjacent pairs of spins $C_{xy} = ( sigma_x, sigma_y)$ with $f_{C_{xy}} = sigma_x sigma_y$ and $w_{C_{xy}} = - J_{xy}$ - this is the simplest form of a MRF. We can then extend some of the ideas and results we have looked at previously to a much larger class of probabilistic models. . Markov random fields have been used in a variety of applications relating to computer vision and image/video processing including texture synthesis, compression, enhancement, etc. They have also been used in machine learning and computational biology. The ideas are also used in combinatorial optimization problems owing to their similarity to spin glasses. . Hopfield Network . We now look at a(n artificial) neural-network model developed by John Hopfield in 1982. This model is no longer particularly in favour as it doesn&#39;t have particularly strong performance but is interesting nonetheless. We will look at Hopfield networks as a mechanism for pattern storage and recall, they can be used in different applications. The model took spin glass models as a direct inspiration. . A Hopfield network is a single layer recurrent network with a fully connected geometry, each neuron receives input from every other neuron and also gives output to every other neuron. We can specify the network through a weight matrix $(W_{ij})_{ij}$ which has the following properties: . Symmetry: $W_{ij} = W_{ji} ; ; ; forall i,j$ | No self-connectivity: $W_{ii} = 0 ; ; ; forall i$ | . The neurons themselves take discrete values of $ sigma_i pm 1$ denoting &quot;on&quot; or &quot;off&quot; states. At each update step we define a quantity: $$ theta_i = sum_j W_{ij} sigma_j + b_i$$ Where $b_i$ is a bias term (we can take $b_i = 0$). If $ theta_i &gt; 0$ then we set $ sigma_i = 1$ and vice-versa. We can also note that this induces an energy function: $$ E = - frac{1}{2} sum_{ij} W_{ij} sigma_i sigma_j $$ Which is exactly the Hamiltonian we investigated in our spin glass blog posts. We can see that the Hopfield Network is in fact nothing more than a Sherrington-Kirkpatrick spin glass viewed in a different way. From fairly basic arguments we can see that for each step of the dynamics described above the energy decreases. . Now suppose we want the network to &quot;remember&quot; patterns, that is we want it to remember a certain &quot;on-off&quot; configuration of neurons. To do this we have to select the correct weight matrix. By noting that the dynamics suggest each step leads to an energy decrease we want our stored patterns to be local energy minima of the network. Suppose we want to store an $N$ bit pattern: $ hat{x} = left( hat{x}_1, hat{x}_2, ... , hat{x}_N right)$ if we take: $W_{ij} = c hat{x}_i hat{x}_j$ for some constant $c&gt;0$ (representing the learning rate), then we have: $$ theta_i = sum_j W_{ij} sigma_j = sum_j c hat{x}_i hat{x}_j sigma_j = c sum_j hat{x}_i = c(N-1) hat{x}_i$$ And so we have $ hat{x}$ forms a(n attractive) fixed point of the dynamics. Starting from a noisy input configuration eventually we should end up with the stored pattern. . We can naturally extend this further if we want to store $P$ different patterns $ left( hat{x}^p right)_{p=1}^P$: $$W_{ij} = c sum_p hat{x}_i^p hat{x}_j^p$$ Typically we would take $c$ to scale with the number of patterns, such as: $c= frac{1}{P}$. . From basic arguments and intuition we can see that recall will deteriorate as we add more and more patterns. Can we construct an argument to find a &quot;capacity&quot; of the network? Yes! And it is quite simple. We start by re-writing the sum for $ theta_i( hat{x}^p)$ (that is $ theta_i$ evaluated at one of the stored patterns $p$): $$ theta_i( hat{x}^p) = sum_j W_{ij} hat{x}^p_j = frac{1}{P} sum_j sum_q hat{x}_i^q hat{x}_j^q hat{x}^p_j = hat{x}_i^p + frac{1}{P} sum_j sum_{q neq p} hat{x}_i^q hat{x}_j^q hat{x}^p_j $$ Where the second term (the double summation) is referred to as the crosstalk term, if this is less than 1 then $ hat{x}^p$ is a fixed point and the network has &quot;remembered&quot; it. We can then define: $$ C_i^p = - hat{x}_i^p frac{1}{P} sum_j sum_{q neq p} hat{x}_i^q hat{x}_j^q hat{x}^p_j $$ If $C_i^p&lt;0$ then $ hat{x}^p$ will be a fixed point. For $C_i^p&gt;1$ then there is instability. We want to find $P(C_i^p &gt; 1)$. By considering the limit of a large number of neurons and patterns we can consider the case of the terms $ hat{x}_i^q hat{x}_j^q$ being uniformly random. By applying the central limit theorem we get: $C_i^p sim N(0, sigma^2)$ with $ sigma^2 = frac{P}{N}$ - that is the ratio of the number of stored patterns over the number of neurons. For example if we have $ frac{P}{N} = 0.37$ then the probability of observing an error is around $5%$. From further analysis (not shown here) we can find that these errors can &quot;add up&quot;, for stability we require: $ frac{P}{N} &lt; 0.138$. So for a $10x10$ neuron array we will only want to store 13 patterns or fewer. . So far we have assumed that each pattern is treated equally by the network, we can remember some patterns &quot;more strongly&quot; by assigning them a multiplicity (essentially the number of times the pattern is &quot;remembered&quot;). With higher multiplicity the pattern will have a larger basin of attraction. However we have to consider the sum of multiplicities now as opposed to the number of unique patterns. For example for the $10x10$ neuron case, if we have a pattern with multiplicity $10$ then we can only store a maximum of 3 extra patterns (of multiplicity 1). . So far the picture looks rosy with our Hopfield-Network, unfortunately there are a few issues that prevents them being widely used. Firstly we have the occurence of &quot;spurious states&quot; - these are non-remembered states that are attractors of the network dynamics. If we start near one of these spurious states the dynamics will converge towards them, which is obviously not ideal. These are due to the complex energy landscape of the Sherrington-Kirkpatrick spin-glass. One simple example of a spurious state would be any inverse image of a stored pattern (i.e. flipping +1 neuron activations to -1 and vice versa). There can be other spurious states however. . We now present a basic implementation of a Hopfield network as an illustration. We will consider a $7x7$ neuron grid and aim to store $4$ patterns. We will &quot;flatten&quot; the neuron grid to a $49$ element single dimension array for convenience. . # A code implementing a Hopfield network # Storing a number of 2d images for recall import matplotlib.pyplot as plt import numpy as np # Set random seed for reproducibility np.random.seed(123) num_patterns = 4 grid_width = 7 grid_height = 7 grid_size = grid_width*grid_height # Fix stored patterns stored = np.zeros((num_patterns, grid_size)) stored[0] = [-1, -1, +1, +1, -1, -1, -1, -1, +1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, +1, +1, +1, +1, +1, -1 ] stored[1] = [-1, +1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, +1, +1, -1, +1, +1, +1, +1, +1, -1, +1, +1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, +1, +1, +1, +1, +1, +1 ] stored[2] = [-1, +1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, +1, +1, +1, +1, -1, -1, -1, -1, -1, -1, -1, +1, +1, -1, -1, -1, -1, -1, +1, -1, +1, +1, +1, +1, +1, +1 ] stored[3] = [-1, -1, -1, +1, +1, -1, -1, -1, -1, +1, -1, +1, -1, -1, -1, +1, -1, -1, +1, -1, -1, +1, -1, -1, -1, +1, -1, -1, +1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1 ] # Display the patterns fig, ax = plt.subplots(1, num_patterns, figsize=(10, 5)) for i in range(num_patterns): ax[i].imshow(stored[i].reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[i].set_xticks([]) ax[i].set_yticks([]) ax[i].set_title(&quot;Pattern %i&quot; %(i+1)) fig.suptitle(&quot;Remembered Patterns&quot;, x=0.5, y=0.8, size=&#39;xx-large&#39;) plt.show() # Create network weights matrix W = np.zeros((grid_size, grid_size)) for i in range(grid_size): for j in range(grid_size): if i == j or W[i, j] != 0.0: continue w = 0.0 for n in range(num_patterns): w += stored[n, i] * stored[n, j] W[i, j] = w / num_patterns W[j, i] = W[i, j] # Test noisy inputs noise_rate = 0.25 num_cycles = 5 fig, ax = plt.subplots(num_patterns, 3, figsize=(10, 15)) for x in range(num_patterns): original_image = stored[x] noisy_image = original_image.copy() for i in range(grid_size): if np.random.random() &lt; noise_rate: noisy_image[i] *= -1 test_image = noisy_image.copy() for _ in range(num_cycles): for i in range(grid_size): test_image[i] = 1.0 if np.dot(W[i], test_image) &gt; 0 else -1.0 # Display results ax[x,0].imshow(noisy_image.reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[x,0].set_xticks([]) ax[x,0].set_yticks([]) ax[x,0].set_title(&quot;Noisy Input Pattern %i&quot; %(x+1)) ax[x,1].imshow(test_image.reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[x,1].set_xticks([]) ax[x,1].set_yticks([]) ax[x,1].set_title(&quot;Hopfield Network Prediction&quot;) ax[x,2].imshow(original_image.reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[x,2].set_xticks([]) ax[x,2].set_yticks([]) ax[x,2].set_title(&quot;Target Pattern %i&quot; %(x+1)) fig.suptitle(&quot;Noisy Image Retrieval (Noise Rate=%d&quot; %(noise_rate*100) +&quot;$ %$)&quot;, x=0.5, y=0.91, size=&#39;xx-large&#39;) plt.show() # Calculate energies of stored patterns energy = np.zeros(num_patterns) for x in range(num_patterns): e = 0 for i in range(grid_size): for j in range(grid_size): e += W[i,j] * stored[x][i]* stored[x][j] e *= -0.5 print(&quot;Energy of pattern &quot;,x+1,&quot;:&quot;,e) . Energy of pattern 1 : -294.0 Energy of pattern 2 : -447.0 Energy of pattern 3 : -454.0 Energy of pattern 4 : -301.0 . In this toy example we have seen results that are not unreasonable, however even in this relatively simple example we see that patterns 2 and 3 are not recalled exactly - most likely as there is a high degree of overlap between them, each time the network dynamics converged to a spurious pattern. . Hopfield networks were the basis for the development of Boltzmann machines however which are stochastic versions of the Hopfield network. We will not touch on these here. . NK Model . We now focus our attention on another model with a similarity to spin-glasses: the NK model propsed by Stuart Kauffmann. Described as a &quot;tunable ruggedness&quot; model by Kauffmann, the NK model originally proposed as a model of evolution (not to be confused with an &quot;evolutionary algoirthm&quot;). The model controls the size of a landscape and the number of local optima via the 2 parameters $N$ and $K$. The $N$ parameter denotes the degrees of freedom or the &quot;size&quot; of the system. the parameter $K$ represnts the level of interactivity between those degrees of freedom. $K$ varies from $K=0$ (leading to a smooth landscape) through to $K=N-1$ the most rugged/peaked. . In the context of spin-glass models we can express the Hamiltonians corresponding to the NK model as: begin{align} K=0 implies H &amp;= - sum_{i} J_{i} sigma_{i} K=1 implies H &amp;= - sum_{i} J_{i} sigma_{i} - sum_{ij} J_{ij} sigma_{i} sigma_{j} K=2 implies H &amp;= - sum_{i} J_{i} sigma_{i} - sum_{ij} J_{ij} sigma_{i} sigma_{j} - sum_{ijk} J_{ijk} sigma_{i} sigma_{j} sigma_{k} end{align} And so on. through this interpretation we can see that the NK model is nothing more than a generalization of the spin glass models we have looked at previously. However the model did not use spin-glasses as a basis, it was developed as a way of studying evolutionary systems and how one can navigate a fitness landscape. . As one would expect given the similarity the NK model shares some of the properties of spin-glasses; most notably that it has a very complicated landscape that often makes it hard to find even local optima. If the model sufficiently captures the properties of evolutionary systems this has some interesting implications. . We&#39;ll now present an NK model outside of the context of spin-glasses - namely as an evolution model as originally intended. We start by considering a sequence of genes $(s_i)_{i=1}^N$. We want to observe what happens to these genes over time. Within this model we assume an organism ($ mathbf{s}$) is completely defined by its gene sequence. The fitness of an organism is defined as an average of the fitness of its genes: $$ F( mathbf{s}) = frac{1}{N} sum_i f(s_i)$$ The fitness function depends on the value of the gene itself but also other genes - in biology this is known as &quot;epistasis&quot;. We use the matrix $ left( A_{ij} right)_{i,j=1}^N$ to denote dependence. If $A_{ij} = 1$ then gene $i$ depends on gene $j$ and is zero otherwise. Note that this matrix denotes a directed network and need not be symmetric. We can then compute a fitness landscape for each organism according to this equation. . In the NK model we assume that initially the entire population consists of one organism (sequence of genes). At random a mutation occurs to one of the genes, this new mutation either dies out quickly (if it has a lower fitness) or it reproduces faster than the original organism (which dies out) and becomes the sole organism in the system. The assumption here of course is that genetic mutations are far enough apart in time that the system can &quot;settle&quot; at each step. . Given a fitness landscape we can specify the dynamics of the NK model as: . From $ mathbf{s} = (s_1, ..., s_N)$ pick a gene $i in {1, ... , N}$ at random and mutate its value to create a new organism $ mathbf{ hat{s}}$ | If $F( mathbf{ hat{s}}) &gt; F( mathbf{s})$ use $ mathbf{ hat{s}}$ as the new state, otherwise keep $ mathbf{s}$ and repeat | Clearly this is simply a greedy hill climber algorithm and the dynamics will end up in a nearby local maxima. The main attraction of the NK model is through the development of the fitness landscape. . The NK model has been developed further in the NKCS model. The NK model essentially looks at genetic variation of one species, the NKCS model extends this idea to multiple species. To do this we assume each species follows its own NK dynamics, its dynamics also interacts with $S$ other species. Each gene in a species is coupled to $C$ randomly chosen genes from the $S$ species. The dynamics can then be described as: . Sequentially select each species during a time step | With the selected species mutate one of the genes | Calculate the updated fitness and either accept or reject this mutation and repeat | Whereas the NK model always ends up in a local fitness maxima this is not necessarily true of the NKCS model since for a fixed point we would require each species to be in a fitness maxima at the same time. This allows the model to display some more sophsticated dynamics such as criticality, self-organised-criticality and co-evolution (and others). . In addition to evolution these models can also be used as a model of immunity. Along with this NKCS models have been used as a model of technological evolution. The model is general enough that it can be applied to, essentially, any evolutionary system. . These models can get fairly complicated, in the future I may write a full length post on models of evolution. . Conclusion . We have now seen that we can extend our spin-glass models to more sophisticated spin-glass models through the Potts model which are more physically plausible. We also saw that spin-glass models were a major influence on some of the first artificial neural networks. Lastly by looking at spin-glasses from a different point of view we have seen how they can inform our understanding of evolutionary systems. . We did not cover it here but spin-glasses have also influenced combinatorial optimization problems. We touched upon this in a previous post where we introduced the concept of simulated annealing. These techniques have been used in computer chip design (where one needs to optimize size/geometry of chip design versus performance). They have also been used in studying protein folding where there is a large universe of possible orientations that need to be searched according to a number of constraints. .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-5",
            "relUrl": "/spin-glass-models-5",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Spin Glass Models 4: Ising Model - Simulation",
            "content": ". This is the fourth blog post in a series - you can find the previous blog post here . . Setup . In this blog we will limit ourselves to trying to find minimum energy states of the Ising model - we could use these schemes in order to estimate other thermodynamic properties also. The models will be in 2-dimensions on a square lattice since the results of these will be easier to display, it should be fairly obvious how we could modify this code for other dimensions. We will also use Gaussian distributed interaction strengths since we do not require mathematical tractability when we are simulating. . Interactions . Unlike the Sherrington-Kirkpatrick model we will have to be a bit &quot;clever&quot; in defining the interaction strengths. I have decided to do this using 4 (NxN) arrays representing the up, down, left and right interaction strengths. This is a bit wasteful in terms of memory use but I think it is worth the trade-off to improve readability of the code. We then have the following symmetries: . up[i,j] == down[i-1,j] left[i,j] == right[i,j-1] . Where these indices exist, for the boundary coniditons we will take the following: . up[0,j] == 0 down[N-1,j] == 0 left[i,0] == 0 right[i,N-1] == 0 . For all possible $i, j$. This is a fixed boundary condition, the edge cases will have only 3 neighbours and corner cases only 2. This can introduce some instability for small spin-glasses but this is fine for our purposes. Modifying this to allow for periodic boundary conditions (i.e. a toroidal geometry) would not be particularly difficult. . Mixing, Convergence, etc. . The methods presented below are Markov-Chain Monte-Carlo (MCMC) methods. These methods are notoriously finicky and require parameter tuning, running multiple chains, etc. in order to ensure good performance. As to not get distracted in this blog post I will not mention these concerns but please keep them in mind when reading on. For now we will run a single chain and look at the results it produces, in practice one would not do this. . Local Update Methods . We begin by looking at local update methods. In the Sherrington-Kirkpatrick blog post we looked at one such (very bad) method: the greedy gradient descent. Local update methods mean at each simulation step we update 1 site only. . Metropolis-Hastings . First we look at the &quot;classic&quot; approach to simulating the Ising model. In many texts and online references this will be the only method that is presented, it leads some to believe (incorrectly) that the simulation method is somehow part of the Ising model itself. This is the method I presented (without elaboration) in my Cython/Numba blog post. . We present Metropolis-Hastings in it&#39;s most general form first (our application to the Ising model will appear below): . Initialize the system in state $x_t$ | Sample a new proposed state from some distribution $Q(x&#39;_{t+1} | x_t)$ | Accept this new state with probability $min left[ alpha, 1 right]$, else $x_{t+1} = x_t$, where: $$ alpha = frac{P(x&#39;_{t+1})Q(x_t |x&#39;_{t+1})} {P(x_t)Q(x&#39;_{t+1} |x_{t})} $$ | Repeat steps 2. and 3. | In this context $P(.)$ represents the probability distribution we wish to estimate. We notice that this does not need to be standardized (i.e. we can ignore any difficult partition functions) so this is a (relatively) easy algorithm to implement. We find that the sequence $x_t$ forms a Markov-Chain - hence the term Markov-Chain Monte Carlo (MCMC). . We can see that that Metropolis-Hastings consists of 2 steps: a proposal step then an acceptance/rejection step. We see that there are essentially no constraints on $Q(.)$ so we can use something easy to sample from. We only need to be careful that it has a support that contains the support of $P(.)$ - otherwise we will not be able to sample all possible values as required. Selecting a suitable $Q(.)$ is at the heart of the algorithm however, the best performance will be where $Q$ and $P$ are very similar. Since $P$ is generally unknown in advance this often requires a bit of trial and error. There are many ways to test whether the algorithm is doing a &quot;good job&quot; but we will not cover them here (in a later set of blog posts I should probably go deeper into MCMC methods - here we&#39;re just concerned with the Ising model in particular). . Moving back to the Ising model picture, we can apply the Metropolis-Hastings methodology via: . First pick a random site | Calculate the change in energy associated with &quot;flipping&quot; this sites spin (going from +1 to -1 or vice versa) | If the energy decreases accept the flipped state and start again with a new site | If energy increases accept the flipped state with some acceptance probability or keep the same state | Begin again | We can see that in contrast to the &quot;greedy hill climber&quot; approach in the Sherrington-Kirkpatrick blog post we are not simply declining into a local minima, there is some probability that we can jump to a state of higher energy and escape a local minima to find a &quot;better&quot; one. In theory if we wait long enough we should find ourselves in a global minimum state. We now derive the acceptance probability as given by Metropolis-Hastings: . Recall that the Gibbs distribution for the Ising model with a given configuration is: $$ P( sigma) = frac{exp(- beta H_{ sigma})}{Z_{ beta}} $$ . Since we are uniformly selecting new states the $Q(.)$ terms in $ alpha$ cancel. If we denote the propsed state as: $ hat{ sigma}$ then the relative likelihood is: $$ alpha = frac{P( hat{ sigma})}{P( sigma)} = frac{exp(- beta H_{ hat{ sigma}})}{exp(- beta H_{ sigma})} = exp left(- beta left(H_{ hat{ sigma}} - H_{ sigma} right) right) = exp(- beta Delta H) $$ . To summarise by Metropolis-Hastings we then accept the new configuration with probability: $$ p_{flip} = min left[1, exp(- beta Delta H) right] $$ . In this particular case we can calculate $ Delta H$ quite efficiently since we are changing the spin of only 1 site, the contributions of all other sites &quot;cancels out&quot;. We can write this down as: $$ Delta H = 2 sigma_{i,j} sum_{(x,y) in langle i, j rangle} J_{x,y} sigma_{x,y} $$ . This introduces some efficiency in a code implementation since we do not have to calculate the energy of the whole configuration each time we want to update a site&#39;s spin. . An example implementation of this can be seen below: . # An implementation of an Ising spin-glass of size NxN # With fixed boundary conditions using Metropolis-Hastings # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Updates occur at randomly selected sites import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 32 spins = np.random.choice([-1, 1], (N,N)) # Fix number of timesteps and some containers timesteps = 10000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction arrays # Have 4 arrays: up, down, left right # These represent the interaction strengths to the # up/down/left/right neighbours of a site # There is a symmetry between these matrices # This is not the most memory efficient solution s_h = 1 s_v = 1 up = np.zeros((N,N)) down = np.zeros((N,N)) left = np.zeros((N,N)) right = np.zeros((N,N)) up[1:N,:] = np.random.rand(N-1,N) * s_v down[0:N-1,:] = up[1:N,:] left[:,1:N] = np.random.rand(N,N-1) * s_h right[:,0:N-1] = left[:,1:N] mag[0] = spins.sum() for i in range(N): for j in range(N): if i == 0: up_neighbour = 0 down_neighbour = spins[i+1,j] elif i == N-1: up_neighbour = spins[i-1,j] down_neighbour = 0 else: up_neighbour = spins[i-1,j] down_neighbour = spins[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = spins[i,j+1] elif j == N-1: left_neighbour = spins[i,j-1] right_neighbour = 0 else: left_neighbour = spins[i,j-1] right_neighbour = spins[i,j+1] energy[0] += spins[i,j]*(up[i,j]*up_neighbour + down[i,j]*down_neighbour + left[i,j]*left_neighbour + right[i,j]*right_neighbour) # Avoid double count - each neighbour pair # counted twice in above since loop over each site energy[0] /= 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1 # Define proposal step def proposal(s_array): _N = s_array.shape[0] return np.random.choice(_N, 2) def energy_change(spin_site, bt, s_array, up_array, down_array, left_array, right_array): i = spin_site[0] j = spin_site[1] if i == 0: up_neighbour = 0 down_neighbour = s_array[i+1,j] elif i == N-1: up_neighbour = s_array[i-1,j] down_neighbour = 0 else: up_neighbour = s_array[i-1,j] down_neighbour = s_array[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = s_array[i,j+1] elif j == N-1: left_neighbour = s_array[i,j-1] right_neighbour = 0 else: left_neighbour = s_array[i,j-1] right_neighbour = s_array[i,j+1] dE_tmp = 2*s_array[i,j]*(up_array[i,j]*up_neighbour + down_array[i,j]*down_neighbour + left_array[i,j]*left_neighbour + right_array[i,j]*right_neighbour) return dE_tmp def acceptance(bt, energy): if energy &lt;= 0: return -1 else: prob = np.exp(-bt*energy) if prob &gt; np.random.random(): return -1 else: return 1 # Define update step dE = 0 dM = 0 def update(bt, s_array, up_array, down_array, left_array, right_array): global dE global dM # Proposal Step site = proposal(s_array) # Calculate energy change dE = energy_change(site, bt, s_array, up_array, down_array, left_array, right_array) dM = -2*s_array[site[0],site[1]] # Acceptance step accept = acceptance(bt, dE) if accept == -1: s_array[site[0], site[1]] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts, s_array, up_array, down_array, left_array, right_array): s_temp = s_array.copy() for i in range(ts): update_step = update(beta, s_temp, up_array, down_array, left_array, right_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, up, down, left, right) mag = mag / (N**2) energy = energy / (N**2) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . Gibbs Sampling . Many presentations of the Ising model only show the Metropolis-Hastings scheme, as such there is a misconception that the Metropolis-Hastings sampling is somehow part of the Ising model itself. This is not true, an alternative to Metropolis-Hastings is Gibbs-Sampling. We can describe Gibbs-Sampling in general terms as: . Pick an initial state $ pmb{x_t} = left(x^1_t, x^2_t, ... , x^N_t right)$ | For each $N$ dimension sample: $x^i_{t+1} sim P left(x^i_{t+1} | x^i_{t+1}, x^2_{t+1}, ... , x^{i-1}_{t+1}, x^{i+1}_t, ... x^N_i right)$ and define next state as: $ pmb{x_{i+1}} = left(x_1^{i+1}, x_2^{i+1}, ... , x_N^{i+1} right)$ | Repeat sampling for each successive state | We can see this is just a special case of Metropolis-Hastings, if we denote $ pmb{x_t^{-j}} = left(x_t^1, x_t^2, ... , x_t^{j-1}, x_t^{j+1}, ..., x_t^N right)$ (all components apart from the jth). Then we can set: $$Q(x_{t+1}^j pmb{x_t^{-j}} | pmb{x_t} ) = P(x_{t+1}^j | pmb{x_t^{-j}})$$ In the Metropolis-Hastings scheme, the acceptance probablilty then becomes: begin{align} min left[1 , frac{Q(x_{t+1}^j pmb{x_t^{-j}} | pmb{x_t} ) P( pmb{x_t})}{Q( pmb{x_t} | x_{t+1}^j pmb{x_t^{-j}} ) P(x_{t+1}^j pmb{x_t^{-j}})} right] &amp; = min left[1 , frac{P( pmb{x_t})P(x_{t+1}^j | pmb{x_t^{-j}})}{P(x_{t+1}^j pmb{x_t^{-j}})P(x_{t}^j | pmb{x_t^{-j}})} right] &amp;= min left[1 , frac{P(x_{t}^j | pmb{x_t^{-j}})P( pmb{x_t^{-j}})P(x_{t+1}^j | pmb{x_t^{-j}})}{P(x_{t+1}^j | pmb{x_t^{-j}})P( pmb{x_t^{-j}})P(x_{t}^j | pmb{x_t^{-j}})} right] &amp;= min left[1, 1 right] = 1 end{align} Which results in the Gibbs procedure as described above. . We can implement Gibbs sampling in the context of an Ising model as: . Pick a random site $(i,j)$ | Set spin to +1 with probability $p_{ij}$, or -1 with probability $(1-p_{ij})$ | Begin again | The probability is defined as: $$ p_{ij} = frac{1}{1+ exp(- beta Delta H / sigma_{i,j})} $$ . At lower temperatures this should behave similarly to the Metropolis-Hastings. However the Metropolis-Hastings method has approximately twice the probability of accepting energetically unfavourable states, as such the Gibbs might be less efficient. . Note: this is true for the Ising model with methods as descibed - it is note a general point on Gibbs/Metropolis-Hastings. When sampling from higher dimensional distributions Gibbs samplers sample each dimension independently whereas Metropolis-Hastings samples points from the high-dimensional space. In some situations the Gibbs sampler can perform significantly better. . Since this is only a minor update to the Metropolis-Hastings code we will not present it here. . Glauber Dynamics (and Heat-Bath) . Another confusion I have seen is that Metropolis-Hastings is the only acceptance-rejection scheme. This is not true, we can also define alternative acceptance probabilites. One such example is Glauber dynamics where we can express the acceptance probability as: $$ p_{flip} = frac{1}{2} left( 1 - tanh left( beta Delta H / 2 right) right) frac{exp(- beta Delta H/2)}{exp( beta Delta H/2) + exp(- beta Delta H/2)} $$ We will not derive this here nor simulate using Glauber dynamics (but we could modify 1 or 2 lines of the code above to ahcieve this), it is just to illustrate another option. For the Ising model Glauber dynamics has a lower acceptance probability for all possible states, as such its performance is likely to be slightly worse than Metropolis-Hastings. . For the Ising model Glauber-Dynamics is identical to the Heat-Bath method. . Simulated Annealing and Simulated Tempering . We now move onto our first &quot;improvement&quot; to the Metropolis-Hastings scheme: simulated annealing. . The concept is very simple and takes inspiration from physical systems. Essentially we just start the system in a high temperature and gradually cool it down. This makes intuitive sense since at higher temperatures we have an increased chance of jumping out of local-minima and get closer to a better overall minima. However we will struggle to actually locate the minima at high temperature for the same reason. In contrast with a low temperature we will be able to locate a nearby local-minima very accurately but will not be able to jump out of it to find a better one. By starting off &quot;hot&quot; the samples will jump between many local minima, as we slowly cool down there should be fewer jumps between local minima and it should eventually get stuck in the domain of a &quot;good&quot; local minima (not far from the global minima ideally) as we cool the temperature further we should get closer and closer to that minima. This is similar to the process of annealing metals by heating them then cooling them to reorganize the crystalline structure. . Simulated annealing has proved very useful in the field of combinatorial optimization in situations where we want to quickly generate &quot;good&quot; solutions (not necessarily &quot;best&quot;). Variations on the idea can be seen in many areas (e.g. variable learning rate algorithms in deep learning can be thought of as a form of simulated annealing). We can also note there is nothing in the method that is particular to Metropolis-Hastings (or the other variations presented) - it can be used with pretty much any simulation method. . We have not descibed &quot;how&quot; we would want to decrease the temperature over time. This is part of the &quot;issue&quot; with simulated annealing (and many MCMC algorithms in general) - there is just as much art as their is science to implementing them. There are no real hard fast rules for getting good results, one in essense has to just try various options until something works (or on well studied problems borrow schemes from others). . We will take a simple approach where we decrease temperature by 10% every 500 steps starting from a temperature of 4 (this was chosen arbitrarily - it is not a suggestion of what might work well in this situation!) We can make use of the code example above for Metropolis-Hastings to give a compact implementation of: . # An implementation of a Metropolis-Hastings algorithm # with simulated annealing applied to a 2d Ising spin glass # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 32 spins = np.random.choice([-1, 1], (N,N)) # Set up initial beta beta = 1/4 def _main_loop_SA(ts, bt_initial, s_array, up_array, down_array, left_array, right_array): s_temp = s_array.copy() bt_live = bt_initial for i in range(ts): if ts % 500 == 0: bt_live *= 1/0.9 update_step = update(bt_live, s_temp, up_array, down_array, left_array, right_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop_SA(timesteps, beta, spins, up, down, left, right) mag = mag / (N**2) energy = energy / (N**2) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . We can see that the system settled down to a lower energy state more quickly and smoothly than with the vanilla Metropolis-Hastings scheme. . Although simulated annealing can improve on vanilla Metropolis-Hastings it can still struggle to find a global minima of the system. There are various &quot;hacks&quot; that can further improve this however - one such example being the concept of restarting. Again this is a very &quot;obvious&quot; thing to try - we store the &quot;best&quot; state we&#39;ve visited so far in a simulation, if we &quot;get stuck&quot; somewhere above this energy level we &quot;jump back&quot; to this best state and try again. . We can extend the simulated annealing idea further to the concept of &quot;simulated tempering&quot;. Here we treat the temperature of the system as a variable in itself, the teperature can go up as well as down during the simulation. This can further improve convergence properties since it allows the system to escape energy boundaries more easily by increasing temperature. This can remove the need to use restarting since a higher temperature is always available as an option at all times. . One such simulated tempering scheme is &quot;parallel tempering&quot; - as the name suggests this involves running many Markov-Chains in parallel and &quot;jumping&quot; between chains as the algorithm progresses. In some instances the cost of running multiple chains is less than the improvement in performance. Again however there is an art to selecting the correct number of chains and temperatures to run in parallel, most times there is no substitute for just trying things and running tests on the results. For the interests of brevity we will not present a full code here - but we note that for 2 chains at temperatures $T_1$ and $T_2$ our proposed update is to switch the states between the two chains (or swap temperatures of the 2 chains) the acceptance probability is then: $$p_{flip} = min left[1, exp((H_1-H_2)( beta_1 - beta_2)) right] $$ Where $H_1$ is the energy as defined by the Hamiltonian for chain with temperature $T_1 = 1/ beta_1$ and similar for $H_2$. This can be easily adapted to more chains and temperatures. . Cluster Update Methods . We now have a few options for simulating the Ising model, however they are by no means perfect. The issue still remains of falling into local optima instead of a global optima. From our previous mathematical study we know that energy minima for the Ising model are &quot;far away&quot; from each other, that is they have very little overlapping spins. By flipping individual spins 1 by 1 it is very hard to make the chains explore the energy landscape fully. The natural way to solve this is to flip multiple spins simultaneously at each step. From the general definition of the Metropolis-Hastings method there is nothing stopping us in following this line of reasoning. . Unfortunately this makes things much harder, the complications arise in finding a valid scheme for flipping multiple spins at once. We have glossed over the mathematical foundations of MCMC here but the proposal/acceptance probabilities need to be selected in &quot;smart way&quot; in order for the resulting Markov chain to have certain properties. When looking at more than 1 spin at a time in the Ising model this proved fairly difficult. This is evidenced by the original Metropolis-Hastings scheme being proposed in 1953 yet the first multi-spin method not being proposed and justified until the late 1980s. . Wolff Algorithm . The main idea of the algorithm is to look for &quot;clusters&quot; of spin sites with the same spin. We then decide to flip the spin of all the sites within this cluster at once. We then pick a new cluster and repeat this process as necessary. The pseudocode for this algorithm as it applies to the Ising model is: . A site $x$ with spin $ sigma_x$ is selected at random and added to an empty cluster | For each neighbour $y$ of $x$ such that $ sigma_y = sigma_x$ we add $y$ to the cluster stack with probability $p_{xy} = 1 - exp(-2 beta J_{xy})$ else move onto next neighbour | After all neighbours are exhausted select next site in the cluster stack and repeat the previous step until the cluster stack is exhausted | Once the cluster is fully specified flip the spins of all sites in the cluster and begin again | We can see that like the Gibbs sampling algorithm, here the Wolff algorithm is &quot;rejection free&quot; - that is all proposed sites are flipped. We also note that there is nothing in this method that is incompatible with simulated annealing/tempering - these techniques are often used together. . Some of the more computer-science focussed readers may be thinking: &quot;creating clusters is computationally expensive, will a brute force local update method not be better?&quot; Which is a valid concern; there a 2 things to consider here - firstly there are many efficient cluster generating algorithms from percolation theory which can help speed up this process (we presented a very simple method above for clarity.) And secondly the local update methods will take a very long time to make &quot;big jumps&quot; away from the current configuration - even though there is some probability to make unfavourable movements most of the time these will not be accepted, to jump to a different local energy minima we would need many such unfavourable moves which means we could be waiting for a very long time! This is why spin glass models form a good test bed for optimization algorithms as they are one of the simplest to define models with &quot;difficult&quot; energy landscapes. . We should find this algorithm performs better in general, especially near the 2nd order phase transition whereby successive samples become increasingly correlated (whereas we would want more independent samples). We will try and produce plots of the 2nd order themodynamic variables: heat capacity and susceptibility. We should expect to see an approximate discontinuity at the critical temperature. To do this we will re-run the Wolff algorithm multiple times for each target temperature. For each target temperature we will run 1500 &quot;burn&quot; steps and then evaluate our variables over the next 2500 steps. This should be long enough to get some reasonable results. . Note that the Wolff algorithm does not &quot;converge&quot; to a low energy state like the preceeding algorithms, instead it samples from the entire space in a &quot;smart way&quot; - even if it finds itself in the global energy minima there is still a relatively high probability of escaping. As such the graphs we produced before will look more &quot;wiggly&quot; - I&#39;ve heard the term &quot;fat caterpillar&quot; used to describe the graph of a well mixed MCMC algorithm. If we were interested in finding a ground state we could keep track of the configuration corresponding to the lowest observed energy state so far (we will not do this in our code however). . In the example below we&#39;ll run a constant $J=1$ to try and reproduce the heat capacity we found analytically in the previous blog post as a proof of concept. I will leave the functionality for general interaction strengths should you wish to experiment. . An implementation of this method can be seen below (note: this is a very slow code since we&#39;re using Python lists! It would be a prime candidate for being sped up using Cython/Numba/etc.): . # An implementation of the Wolff algorithm # With simulated annealing applied to a # 2d Ising spin glass import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model NxN and initial spins N = 32 spins_initial = np.random.choice([-1, 1], (N,N)) # Fix time-steps burn = 1500 evaluation = 2500 time_steps = burn + evaluation # Initialize interaction arrays # Have 4 arrays: up, down, left right # These represent the interaction strengths to the # up/down/left/right neighbours of a site # There is a symmetry between these matrices # This is not the most memory efficient solution s_h = 1 s_v = 1 up = np.zeros((N,N)) down = np.zeros((N,N)) left = np.zeros((N,N)) right = np.zeros((N,N)) # Using J=1 constant so graphs are easier to generate # Replace with comments to give an EA spin glass up[1:N,:] = 1 #np.random.rand(N-1,N) * s_v down[0:N-1,:] = up[1:N,:] left[:,1:N] = 1 #np.random.rand(N,N-1) * s_h right[:,0:N-1] = left[:,1:N] # Create function to find neigbour sites def nbr_udlr(s_site, s_array): _N = s_array.shape[0] i = s_site[0] j = s_site[1] if i == 0: up_site = 0 down_site = [i+1, j] elif i == _N-1: up_site = [i-1,j] down_site = 0 else: up_site = [i-1,j] down_site = [i+1, j] if j == 0: left_site = 0 right_site = [i,j+1] elif j == N-1: left_site = [i,j-1] right_site = 0 else: left_site = [i,j-1] right_site = [i,j+1] return [up_site, down_site, left_site, right_site] # Create function to return interactions strength def int_strength(s_site, udlr, up_array, down_array, left_array, right_array): if udlr == 0: return up_array[s_site[0], s_site[1]] if udlr == 1: return down_array[s_site[0], s_site[1]] if udlr == 2: return left_array[s_site[0], s_site[1]] if udlr == 3: return right_array[s_site[0], s_site[1]] def energy_calc(s_array, up_array, down_array, left_array, right_array): _N = s_array.shape[0] energy = 0 for i in range(_N): for j in range(_N): if i == 0: up_neighbour = 0 down_neighbour = s_array[i+1,j] elif i == N-1: up_neighbour = s_array[i-1,j] down_neighbour = 0 else: up_neighbour = s_array[i-1,j] down_neighbour = s_array[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = s_array[i,j+1] elif j == N-1: left_neighbour = s_array[i,j-1] right_neighbour = 0 else: left_neighbour = s_array[i,j-1] right_neighbour = s_array[i,j+1] energy += s_array[i,j]*(up_array[i,j]*up_neighbour + down_array[i,j]*down_neighbour + left_array[i,j]*left_neighbour + right_array[i,j]*right_neighbour) return -energy/2 def wolff_step(bt, s_array, up_array, down_array, left_array, right_array): _N = s_array.shape[0] initial_site = np.random.choice(_N, 2) initial_site = [initial_site[0], initial_site[1]] old_spin = s_array[initial_site[0], initial_site[1]] cluster = [initial_site] stack = [initial_site] while stack != []: site = stack[np.random.choice(len(stack))] # Cycle neigbours nbr = nbr_udlr(site, s_array) for i in range(4): nbr_live = nbr[i] if nbr_live == 0: continue nbr_spin = s_array[nbr_live[0], nbr_live[1]] if nbr_spin == old_spin: if nbr_live not in cluster: p = 1 - np.exp(-2*bt*int_strength(site, i, up_array, down_array, left_array, right_array)) if np.random.random() &lt; p: cluster.append(nbr_live) stack.append(nbr_live) stack.remove(site) for site in cluster: s_array[site[0], site[1]] *= -1 return s_array ###### Main Code # Create useful constants N1 = evaluation*N*N N2 = evaluation*evaluation*N*N # Define temp ranges temp_steps = 20 temp_min = 1.75 temp_max = 2.75 temp_array = np.linspace(temp_min, temp_max, num=temp_steps) M = np.zeros(temp_steps) E = np.zeros(temp_steps) C = np.zeros(temp_steps) X = np.zeros(temp_steps) for t in range(temp_steps): spins = spins_initial.copy() M1 = 0 M2 = 0 E1 = 0 E2 = 0 beta = 1/temp_array[t] for i in range(time_steps): spins = wolff_step(beta, spins, up, down, left, right) if i &gt; burn: mag_tmp = abs(spins.sum()) M1 += mag_tmp M2 += mag_tmp**2 energy_tmp = energy_calc(spins, up, down, left, right) E1 += energy_tmp E2 += energy_tmp**2 M[t] = M1 / N1 E[t] = E1 / N1 C[t] = (E2/N1 - E1**2/N2)*beta**2 X[t] = (M2/N1 - M1**2/N2)*beta # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].scatter(temp_array, M, color=&#39;Red&#39;) axs[0, 0].set_title(&quot;Magnetism&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;Magnetism&quot;) axs[0, 1].scatter(temp_array, E, color=&#39;Blue&#39;) axs[0, 1].set_title(&quot;Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;Energy&quot;) axs[1, 0].scatter(temp_array, X, color=&#39;Red&#39;) axs[1, 0].set_title(&quot;Susceptibility&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;Susceptibility&quot;) axs[1, 1].scatter(temp_array, C, color=&#39;Blue&#39;) axs[1, 1].set_title(&quot;Heat Capacity&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;Heat Capacity&quot;) plt.show() . The plots here are a bit noisy but they loosely match our previous theoretical findings. . Swendsen-Wang Algorithm . Another cluster algorithm is the Swendsen-Wang. Unlike the Wolff algorithm Swendsen-Wang looks at multiple clusters concurrently and applies a spin-flip to all clusters. It was propsed 2 years prior to the Wolff method. . In pseudo code it can be presented as: . From an initialized spin configuration for each neighbour pair of sites $ langle x, y rangle$ we specify a bond: $b_{x,y} in {0, 1 }$ Where we sample according to: begin{align} &amp; mathbb{P}(b_{x,y} = 0 | sigma_x neq sigma_y) = 1 &amp; mathbb{P}(b_{x,y} = 1 | sigma_x neq sigma_y) = 0 &amp; mathbb{P}(b_{x,y} = 0 | sigma_x = sigma_y) = exp(-2 beta J_{xy}) &amp; mathbb{P}(b_{x,y} = 1 | sigma_x = sigma_y) = 1 - exp(-2 beta J_{xy}) end{align} | Generate clusters using bonds. If there exists a bond between sites $b_{x,y} = 1$ then the sites belong to same cluster | For each cluster with probability 1/2 flip all spins within the cluster to get a new configuration | Repeat process of generating bonds and clusters | We can see this is slightly different to the Wolff algorithm since it looks at multiple clusters within a given step. The performance of the Swendsen-Wang is slightly worse than that of the Wolff since it has a lower probability of flipping large clusters (in the case of Ising models). Both algorithms have been adpated and used to alternative spin glass models (as well as models outside of spin glasses). . We won&#39;t present an implementation of this method here since we already looked at the Wolff algorithm for an example of a cluster algorithm. . Conclusion . In this blog post we have looked at a variety of MCMC methods for simulating Ising models. We started by looking at various local update methods, which we now know do not behave optimally. We extended these ideas to cluster update methods which can show better performance for Ising models. We also looked at the very intuitive simulated annealing and simulated tempering methods, which have been used in optimization problems far outside the realms of spin glass models or even statsitical physics in general. . . This is the fourth blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-4",
            "relUrl": "/spin-glass-models-4",
            "date": " • Mar 24, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Spin Glass Models 3: Ising Model - Theory",
            "content": ". This is the third blog post in a series - you can find the previous blog post here . . In this blog post we are going to look at another spin-glass model. In the previous post we looked at the Sherrington-Kirkpatrick model which allowed us to study the spin-glass analytically. However there is a certain lack of &quot;realism&quot; in this model - the infinite interaction range means that the Sherrington-Kirkpatrick (in a sense) does not occupy any space, there is no concept of dimension nor geometry. While providing interesting mathematical results, some of which appear to be universal to spin-glass systems, we would like to look at a different model that captures more of the real world system and perhaps might uncover new behaviours. . Introducing the Ising Model . One model that we can look at is the Edwards-Anderson Ising model. We have actually looked at these models achronologically: Edwards-Anderson proposed the Ising model before Sherrington-Kirkpatrick proposed theirs. In fact Sherrington-Kirkpatrick developed their model partly due to the difficulty in deal with the Ising model. I have presented the models in this order in this series of blog posts because it feels more natural to look at models in increasing complexity order rather than presenting a historical account. . The main difference between Ising and Sherrington-Kirkpatrick is the extent over which the interactions can occur, instead of an infinite range the Ising model only allows &quot;nearest-neighbour&quot; interactions. This relaxation means that there is a concept of dimension and geometry to an Ising spin-glass. For example we could think of spins oriented on a line segment (a finite 1d Ising model), a square lattice (a 2d Ising model) or something more exotic like spins oriented on the nodes of a 32 dimensional hexagonal lattice. Through careful use of limits we could also consider infinite-dimensional Ising models too. The Hamiltonian follows the form one would expect: $$ H = - sum_{ lt x,y gt} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ Where we use the notation $ lt x,y gt$ to denote the sum occuring over neighbour pairs. . As before the selection of $J_{xy}$ is somewhat arbitrary, however unlike the Sherrington-Kirkpatrick we do not have to scale this with lattice size to retain meaningful &quot;average energy&quot; or &quot;average magnetism&quot; measurements when we increase the size of the system. If we take $J_{xy} = J$ for some fixed $J$ we get the Ising model of Ferromagnetism, if we allow $J_{xy}$ to vary according to some distribution we get the Edwards-Anderson Ising Model. In this blog we will use the term &quot;Ising model&quot; to refer to either situation, the mathematics of moving from the fixed $J$ case usually involves integrating against the density function, this can lead to more complicated formulae so in this blog we will mainly focus on the ferromagnetic case unless otherwise stated. . As before spins can only take values $ pm 1$ - in some literature this is referred to as &quot;Ising spins&quot; (e.g. you can read references to &quot;Sherrington-Kirkpatrick with Ising spins&quot; - this means an infinite interaction range with binary spins). . A pictorial representation of a 3d square lattice Ising spin-glass can be seen below: . For the rest of this article we will limit ourselves to talking about &quot;square&quot; lattices since this is where the majority of research is focussed. Using different lattice types won&#39;t change the story too much. . In studying these models it is also worth noting what happens on the &quot;boundary&quot; of the lattice: for finite size systems this can become an issue. In this blog post we will skirt over the issue, where necessary we will assume periodic boundary conditions (i.e. there is a toroidal type geometry to the system) so each &quot;edge&quot; of the lattice is connected to an opposing one, so in a sense there are no boundary conditions to specify. . 1D Ising Model . We will now consider a 1 dimensional Ising model where spins are located on a line segment. This simplication means that finding the ground state (minimal energy spin configuration) is trivial: we pick a site in the middle of the line segment (the origin) we will pick a spin ($ pm 1$) at random. We then propagate out from the origin in the left and right direction, we pick the spin for the next site according to the interaction strength for example: if $ sigma_0 = +1$ and $J_{0,1} &gt; 0$ then $ sigma_1 = +1$ and so on. We can see that (as with all spin glass systems) there is a symmetry: if we flip all the spins direction we end up with an equivalent energy - we call these &quot;pairs&quot; of configurations. It doesn&#39;t take much convincing to realise that the pair of configurations we constructed is a unique minima for the system. To see this imagine we have only one pair of spins that opposes the direction indicated by the interaction strength. For a large enough system we can eventually find another interaction strength that is greater in magnitude (by definition having a spin pair as indicated by the sign of the interaction strength). By simply flipping the relative orientation of the pairs we will end up with a configuration of lower energy - which is a contradiction. . We will now look to solve the 1d Ising model analytically. For simplicity we will assume no external magnetic field, this just makes the formulae look &quot;prettier&quot;. The crux of understanding any spin glass system is finding the Gibbs distribution, as before: $$P( sigma) = frac{exp(- beta H_{ sigma})}{Z_{ beta}}$$ Where: $$H = - sum_{ lt x,y gt} J_{xy} sigma_x sigma_y$$ And $ beta$ is the inverse temperature. We can write an expression for the partition function as: $$ Z_{ beta} = sum_{ sigma} exp(- beta H_{ sigma}) = sum_{ sigma} exp(- beta sum_{ lt x,y gt} J_{xy} sigma_x sigma_y) $$ Suppose we look at a line segment with $N$ spins, we can then write this as: $$ Z_{ beta} = sum_{ sigma_1, ..., sigma_N} exp(- beta (J_{1,2} sigma_1 sigma_2 + J_{2,3} sigma_2 sigma_3 + ... + J_{N-1,N} sigma_{N-1} sigma_N))$$ We notice that we can factorise this summation as: $$ Z_{ beta} = sum_{ sigma_1, ..., sigma_{N-1}} exp(- beta (J_{1,2} sigma_1 sigma_2 + ... + J_{N-2,N-1} sigma_{N-2} sigma_{N-1})) sum_{ sigma_N}exp(- beta J_{N-1,N} sigma_{N-1} sigma_N) $$ The internal sum can then be evaluated: $$ sum_{ sigma_N}exp(- beta J_{N-1,N} sigma_{N-1} sigma_N) = exp( beta J_{N-1,N} sigma_{N-1} ) + exp(- beta J_{N-1,N} sigma_{N-1} ) = 2cosh( beta J_{N-1,N} sigma_{N-1}) = 2cosh( beta J_{N-1,N}) $$ The last equality making use of the fact $ sigma_{N-1} = pm 1$ and that $cosh(x) = cosh(-x)$. By repeating this process we can express the partition function as: $$ Z_{ beta} = 2 prod_{i=1}^{N-1} 2 cosh( beta J_{i,i+1}) $$ We can evaluate this exactly. If we are sampling the interaction strengths according to some distribution we can find expected values (or other statistical properties) by integrating against the density function as usual, since this adds to notational complexity we will assume that the interaction strengths are fixed for now. We can then calculate thermal properties using the equations: . begin{align} F &amp;= - frac{1}{ beta} ln Z_{ beta} = - T ln 2 - T sum_{i=1}^{N-1} ln (2 cosh( beta J_{i,i+1})) U &amp;= - frac{ partial}{ partial beta} ln Z_{ beta} = - sum_{i=1}^{N-1} J_{i,i+1} tanh( beta J_{i,i+1}) C &amp;= frac{ partial U}{ partial T} = sum_{i=1}^{N-1} ( beta J_{i,i+1})^2 sech^2( beta J_{i,i+1}) S &amp;= frac{U - F}{T} = ln2 + sum_{i=1}^{N-1} left( - beta J_{i,i+1} tanh( beta J_{i,i+1}) + ln(2cosh( beta J_{i,i+1}) right) end{align}Where: F - is the Helmholtz-Free Energy U - is the thermodynamic energy (ensemble average) C - is the heat capacity S - is the entropy . We can find the expected value of a given instantiation of interactions through an integral such as: $$ mathbb{E}(U) = - sum_{i=1}^{N-1} int J_{i,i+1} tanh( beta J_{i,i+1}) Q(J_{i,i+1}) dJ_{i,i+1} $$ With $Q(J)$ being the density function of the distribution and the integral occuring over its support. Similar formulae exist for the other thermodynamic variables and you can calculate other statistics (e.g. variance) in the usual way. . We can plot the values of these variables for a given set of interaction weights: . # This code creates a 4 figure plot showing how # Thermodynamic variables change with temperature # For a 1d Ising Model with Gaussian interactions import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix seed np.random.seed(123) # Fix number of points N = 100 # Instantiate interaction strength J = np.random.normal(0,1,size=N-1) # Set temperature ranges T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min beta = 1 / T # Set up holders for variables F = np.zeros(T_steps) U = np.zeros(T_steps) C = np.zeros(T_steps) S = np.zeros(T_steps) # Loop over T_steps and calculate at each step for i in range(T_steps): F[i] = - T[i] * np.log(2) - T[i] * (np.log(2*np.cosh(beta[i]*J))).sum() U[i] = - (J * np.tanh(J * beta[i])).sum() C[i] = ((beta[i] * J)**2 * (np.cosh(beta[i] * J))**-2).sum() S[i] = (U[i] - F[i]) / T[i] # Divide by number of points to give a scale invariant measure F = F / N U = U / N C = C / N S = S / N # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].plot(T, F) axs[0, 0].set_title(&quot;Free Energy&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;F / N&quot;) axs[0, 1].plot(T, U) axs[0, 1].set_title(&quot;Average Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;U / N&quot;) axs[1, 0].plot(T, C) axs[1, 0].set_title(&quot;Heat Capacity&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;C / N&quot;) axs[1, 1].plot(T, S) axs[1, 1].set_title(&quot;Entropy&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;S / N&quot;) plt.show() . From these plots we can see there is no phase transition taking place - this is true for all 1d Ising models. . We can also calculate the correlation function for the system. By following a similar line of logic as before we find: $$ langle sigma_n sigma_{n+r} rangle = prod_{i=0}^{r} tanh( beta J_{n+i, n+i+1})$$ . We can plot this as a function of $r$ starting at the first position: . # This code creates a plot displaying # The correlation function # For a 1d Ising Model with Gaussian interactions import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix seed np.random.seed(123) # Fix number of points N = 100 # Instantiate interaction strength J = np.random.normal(0,1,size=N-1) # Create holders for variables r = np.arange(N) corr_array = np.zeros(N) # Fix beta beta = 1 # Calculate correlations tanh_array = np.tanh(beta * J) for i in range(N): corr_array[i] = tanh_array[:i].prod() plt.plot(r[0:20], corr_array[0:20]) plt.title(&quot;Correlation Function&quot;) plt.ylabel(r&quot;$ langle sigma_{1} sigma_{r} rangle$&quot;) plt.xlabel(&quot;r&quot;) plt.xticks(np.arange(11)*2) plt.ylim((-1,1)) plt.xlim((0,20)) plt.show() . As we can see there is a specific &quot;structure&quot; to the correlation function given an instantiation - this is not particularly surprising. If we picked an alternate starting state (e.g. the 2nd site) this graph can look totally different, the decay in absolute value will be similar however. We also notice that the correlation decays to zero fairly rapidly suggesting there isn&#39;t a long range structure to the model. . 2d Ising Model . We now turn our attention to the 2d Ising model. The mathematics here will, understandably, get more complicated. We will require a bit more sophistication to solve this system. The solution was originally published by Lars Onsager in 1944, with alternative proofs and derivations published later. The derivation itself is fairly involved and would take a blog post (at least) by itself to cover - I may come back to this at a later date. For now I will simply present the result for the free energy (F) in absence of a magnetic field ($h=0$): $$ - frac{ beta}{N} F = ln2 + frac{1}{8 pi^2} int^{2 pi}_{0} int^{2 pi}_{0} ln left[ cosh(2 beta J_H) cosh(2 beta J_V) - sinh(2 beta J_H)cos( theta_1) - sinh(2 beta J_V)cos( theta_2) right]d theta_1 d theta_2 $$ Where instead of $J_{xy}$ being sampled from a gaussian distribution we assume fixed interaction strengths $J_H$ and $J_V$ in the horizontal and vertical directions. . For simplicity we will take: $J_H = J_V = J$ and we can derive the thermodynamic properties (per site - e.g. $f= frac{F}{N}$) of this system as: begin{align} f &amp;= frac{-ln2}{2 beta} - frac{1}{2 pi beta} I_0( beta, J) u &amp;= -J coth(2 beta J) left[ 1 + frac{2}{ pi} left( 2tanh^2(2 beta J) -1 right) I_1( beta, J) right] c &amp;= 2J beta^2 left[U csch(2 beta J)sech(2 beta J) + frac{8J}{ pi} sech^2(2 beta J) I_1( beta, J) - frac{2 beta J}{ pi}(cosh(4 beta J)-3)^2 sech^6(2 beta J) I_2( beta, J) right] s &amp;= frac{U - F}{T} end{align} . For convenience I created 3 new functions $I_0( beta,J), I_1( beta, J)$ and $I_2( beta, J)$ to make the equations a little shorter. These functions are defined as: begin{align} I_0( beta, J) &amp;= int^ pi_0 ln left[ cosh^2(2 beta J) + sinh^2(2 beta J) sqrt{1 + csch^4(2 beta J) - 2 csch^2(2 beta J) cos(2 theta)} right] d theta I_1( beta, J) &amp;= int^{ frac{ pi}{2}}_0 left[1 - 4csch^2(2 beta J)( 1 + csch^2(2 beta J))^{-2} sin^2( theta) right]^{- frac{1}{2}} d theta I_2( beta, J) &amp;= int^{ frac{ pi}{2}}_0 sin^2( theta) left[1 - 4csch^2(2 beta J)( 1 + csch^2(2 beta J))^{-2} sin^2( theta) right]^{- frac{3}{2}} d theta end{align} . As before we can produce plots of these: . # This code creates a 4 figure plot showing how # Thermodynamic variables change with temperature # For a 2d Ferromagnetic Ising Model with fixed interaction import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad %matplotlib inline # Fix seed np.random.seed(123) # Instantiate interaction strength J = 1 # Set temperature ranges T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min beta = 1 / T # Set up holders for variables f = np.zeros(T_steps) u = np.zeros(T_steps) c = np.zeros(T_steps) s = np.zeros(T_steps) # Set up integrands for I0, I1, I2 def integrand0(x, b, j): return np.log(np.cosh(2*b*j)**2 + np.sinh(2*b*j)**2 * np.sqrt(1 + np.sinh(2*b*j)**(-4) - 2*np.sinh(2*b*j)**(-2) * np.cos(2*x))) def integrand1(x, b, j): return (1 - 4*np.sinh(2*b*j)**(-2)*(1 + np.sinh(2*b*J)**(-2))**(-2)*np.sin(x)**2)**(-0.5) def integrand2(x, b, j): return np.sin(x)**2 * integrand1(x, b, j)**3 # Loop over T_steps and calculate at each step for i in range(T_steps): bt = beta[i] I0 = quad(integrand0, 0, np.pi, args=(bt, J)) I1 = quad(integrand1, 0, np.pi/2, args=(bt, J)) I2 = quad(integrand2, 0, np.pi/2, args=(bt, J)) f[i] = - np.log(2) / (2 * bt) - I0[0] / (2 * np.pi * bt) u[i] = -J*np.tanh(2*bt*J)**(-1) * ( 1 + (2/np.pi)*(2*np.tanh(2*bt*J)**2 -1)*I1[0]) c[i] = 2*bt**2*J*( u[i]*np.sinh(2*bt*J)**(-1)*np.cosh(2*bt*J)**(-1) + (8*J / np.pi)*np.cosh(2*bt*J)**(-2)*I1[0] - (2*bt*J/np.pi)*((np.cosh(4*bt*J)-3)**2)*np.cosh(2*bt*J)**(-6)*I2[0] ) s[i] = (u[i] - f[i])*bt # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].plot(T, f) axs[0, 0].set_title(&quot;Free Energy&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;f&quot;) axs[0, 1].plot(T, u) axs[0, 1].set_title(&quot;Average Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;u&quot;) axs[1, 0].plot(T, c) axs[1, 0].set_title(&quot;Heat Capacity&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;c&quot;) axs[1, 1].plot(T, s) axs[1, 1].set_title(&quot;Entropy&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;s&quot;) plt.show() . We find there is a critical temperature $T_c$ where the specific heat equation diverges. We can compute the value of this as satisfying: $$ sinh left( frac{2J_H}{T_c} right)sinh left( frac{2J_V}{T_c} right) = 1 $$ In the case where $J_H = J_V = J$ we have: $$T_c = frac{2 J}{ln left(1+ sqrt{2} right)} $$ This is an example of a second order phase transition since the discontinuity only arises under a second derivative. . For temperatres under this critical temperature we have that the spontaneous magnetization can be calculated as: $$ m = left[ 1 - csch^2 left( frac{2J_H}{T} right)csch^2 left( frac{2J_V}{T} right) right]^{ frac{1}{8}} $$ . We can plot this (for $J_H = J_V = J = 1$) as: . # This code plots the spotaneous magnetization of # a 2d Ising model on a square-lattice import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set up temperature array T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min # Fix interaction strength constant J = 1 m = np.power(np.maximum(1 - np.sinh(2*J/T)**-4,0), 1/8) plt.plot(T,m) plt.xlabel(&quot;T&quot;) plt.ylabel(&quot;Magnetization&quot;) plt.title(&quot;2d Ising Model Spontaneous Magnetization&quot;) plt.xlim((0,5)) plt.show() print(&quot;Critical Temp (Tc):&quot;, (2*J) / np.log(1 + np.sqrt(2))) . Critical Temp (Tc): 2.269185314213022 . The correlation function is harder to compute, here we just present the correlation function between elements on the diagonal of the lattice: . $$ langle sigma_{0,0} sigma_{N,N} rangle = det left[ A_N right]$$ Where $A_N = (a_{n,m})_{n,m=1}^N$ is an $NxN$ matrix with entries: $$ a_{n,m} = frac{1}{2 pi} int_0^{2 pi} e^{i(n-m) theta} sqrt{ frac{sinh^2(2 beta J) - e^{-i theta}}{sinh^2(2 beta J) - e^{i theta}} } d theta $$ . We can plot this as so: . # This code plots the diagnoal correlation function # For the 2d square-lattice Ising model # Lattice size = NxN # At the critical temperature import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad %matplotlib inline # Set up integrand function imag = complex(0,1) def integrand(x, n, m, beta, J): return np.exp(imag*(n-m)*x)*np.sqrt((np.sinh(2*beta*J)**2-np.exp(-imag*x))/(np.sinh(2*beta*J)**2-np.exp(imag*x))) # Set up constants J = 1 beta = np.log(1 + np.sqrt(2)) / (2*J) # Set up arrays N_max = 20 N_array = np.arange(N_max+1) correl_array = np.zeros(N_max+1) for x in range(N_max + 1): N = N_array[x] A_N = np.zeros((N, N)) for n in range(N): for m in range(N): I = quad(integrand, 0, 2*np.pi, args=(n, m, beta, J)) A_N[n,m] = I[0]/(2*np.pi) correl_array[x] = np.linalg.det(A_N) # Plot correlations plt.plot(N_array, correl_array) plt.xlabel(&quot;N&quot;) plt.ylabel(r&quot;$ langle sigma_{0,0} sigma_{N,N} rangle$&quot;) plt.title(&quot;Correlation Function&quot;) plt.xticks(np.arange(11)*2) plt.ylim((0,1)) plt.xlim((0,20)) plt.show() . We can see that even though we have short range interactions (nearest neighbour) this gives rise to longer-range structure within the model. In fact we find for temperatures below the critical temperature there is an infinite correlation length, whereas for temperatures above the critical temperature the correlation length is finite. . Infinte Dimension Ising Model . Next we consider the situation where the Ising model exists in infinite dimensions. In this case each site has infinitely many neighbours, as such a mean-field approximation is valid and we have a model that is somewhat similar to the Sherrington-Kirkpatrick fully connected geometry. (Please excuse the lack of rigour here; one has to be careful in how limits are defined and it is a non-trivial exercise. For this blog post I don&#39;t want to go down into that sort of detail.) . If each site has infinitely many neighbours we only need to concern ourselves with the ratio of positive and negative spin neighbours. By mean field if we take the probability of a positive spin as $p$ then via the Gibbs distribution we have: $$ frac{p}{1-p} = exp(2 beta H)$$ The average magnetization can then be calculated: $$ mathbb{E} left[M right] = (1)p + (-1)(1-p) = 2p - 1 = frac{1 - exp(2 beta H)}{1 + exp(2 beta H)} = tanh(2 beta H)$$ . By investigating this function we can gain insight into spontaenous magnetization. Other similar arguments can be invoked for the other themodynamic properties. It is possible to show, as with the Sherrington-Kirkpatrick, that a phase transition occurs. . n-d Ising Model ($n geq3$) . Finally we look at the case where the dimension of the model is finite but strictly greater than 2. In this situation things get much trickier, in fact there are not many defined mathematical results in these systems and this is the subject of current research. As such I will just briefly outline some of the approaches that have been suggested to study these systems and their properties (presented in chronological order from when they were proposed): . Replica-Symmetry Breaking - From our previous post on Sherrington-Kirkpatrick models we briefly looked at this sort of method. They were an obvious first choice in trying to deal with short range interactions. It has been shown that the &quot;standard&quot; techniques are insufficient but there have been extensions proposed that have shown some promise. Like in the infinite range model it suggests states have an ultrametric structure and uncountably many pure states. | Droplet Scaling - The first such argument being presented by Rudolf Peierls. The main idea is to consider the arisal of &quot;loops&quot; or &quot;islands&quot; of spins (clusters of atoms all with the same spin being enclosed by some boundary). We then aim to count the number of such loops, often in high or low temperature ranges via approximation. This leads to only 2 pure states. | Chaotic Pairs - Has properties somewhat similar to a combination of the preceeding 2 methods, like replica symmetry breaking there are infinitely many thermodynamic states however the relationship between them is much simpler and has simple scaling behaviour - much like droplet scaling. | TNT - This interpretation does not itself specify the number of pure states, however it has been argued that it most naturally exists with 2 pure states - much like droplet scaling. However it has scaling properties more similar to that of replica symmetry breaking. | . For completeness: a pure state of a system is a a state that cannot be expressed as a convex combination of other states. . As far as I am aware there is currently no conesensus on which (if any) of the options presented is the correct interpretation. . One of the main questions that we would like to answer is whether there is a phase transition (first order). This is unresolved currently. Another question we might ask is whether there exists multiple ground state pairs (i.e. we can find 2 different configurations that have minimal energy that are not merely negative images of each other) - again this is unresolved. In infinite dimensions we can see this would be true, in 1d we know this cannot be true - for other dimensions it is unclear (although it is believed it is probably not true for d=2). . In addition to this there are many other unanswered questions that are the subject of current research into Ising type models. . Conclusion . In this blog post we have investigated some of the properties of square-lattice Ising models in various dimensions. In particular we have seen that there is no phase transition in 1d, a second order phase tranisition in 2d, a phase transition in infinite dimension and currently other dimensions are unresolved. We can see that the short-range interactions cause a lot of headaches when trying to analyse these systems. In the next blog post in this series we will begin to look at ways of simulating Ising models (and other spin glass models generally). . . This is the third blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-3",
            "relUrl": "/spin-glass-models-3",
            "date": " • Mar 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Spin Glass Models 2: Sherrington-Kirkpatrick",
            "content": ". This is the second blog post in a series - you can find the previous blog post here . . From a previous blog post we now have a reasonable understanding of what a spin glass is, some of the (frankly) bizarre behaviours they exhibit and some motivation behind why we might want to wish to study them. In this blog post we will embark on studying our first spin glass model due to Sherrington-Kirkpatrick. (Note: this model is sometimes also called the fully-connected Ising model.) . For our purposes we will just consider finding the properties and behaviours of interest rather than trying to capture exact behaviour of specific materials. As such we will assume measurement units are such that any constants &quot;disappear&quot; this should help with clarity. It is worth keeping this in mind if you look at any research on the topic (particularly in Physics journals) where extra terms may appear, these are typically to correct for units (e.g. to get energy measurements in Joules, distances in metres, etc.) . It is also worth noting that this model was not the first spin glass model and some other models appeared before this one. I have chosen to start with this as it is in some ways the &quot;simplest&quot; model. . Simplification . As with any good model we want to simplify the real world to a set of minimal requirements to capture the behaviour of interest. Recall the Hamiltonian for a general spin glass as: $$ H = - sum_{x,y} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ In a real world spin glass $ sigma$ represent spins in the form of vectors denoting the direction the magnetic moment faces. It turns out that allowing for all range of orientation of spins is unnecessary to observe interesting behaviour. For modelling purposes $ sigma_x = pm 1$ is usually sufficient. . Turning our attention to interacting pairs now, we can simplify significantly here by assuming a fully connected model. That is each atom interacts with every other. This is particularly useful since it allows us to use a range of mathematical &quot;tricks&quot; through mean field type approaches. This is the key to the Sherrington-Kirkpatrick model and what differentiates it from other models (which we will review in a later blog post). . This leaves us with one final assumption to make (excluding the external magnetic field): what values should $J_{xy}$ take. There are two options that we could use here to make our lives easier: the first is $J_{xy} = pm frac{J}{ sqrt{N}}$ - that is we select each interaction to be positive (ferromagnetic) or negative (antiferromagnetic) at random. Another option is to take values via a continuous probability distribution, typically a normal distribution owing to its &quot;nice&quot; mathematical properties. Typically we will want a mean of zero since we do not want the material to exhibit any magnetism in a ground state. We want to scale the standard deviation by $ frac{1}{ sqrt{N}}$ as in the bernoulli case. If we are using simulation we could try more &quot;exotic&quot; distributions (such as double fat-tail, assymetric distributions, etc) The scalings applied to the interaction weights is merely a convenience to allow us to scale the size of the spin glass in such a way as to allow population averages to remain comparable between glass size. . We will thus define the Sherrington-Kirkpatrick Hamiltonian as: $$ H_N = - sum_{x,y} J_{xy} sigma_x sigma_y $$ With $J_{xy} sim N(0, frac{s^2}{N})$ iid (value of $s$ is somewhat irrelevant as long as it&#39;s not too large/small). We have removed the external magnetic field as it isn&#39;t crucial for the story we are telling here. . An Alternate Interpretation . For those that are finding spin glasses and magnet terminology a little confusing or alien for this particular model we can introduce another interpretation in the form of the &quot;Dean Problem&quot;. Imagine you are the dean of a college hall, you have a number of students ($x, y$) who can like or dislike each other ($J_{xy}$). Your job is to place students in one of two groups ($ sigma = pm 1$) so as to maximise the overall happiness of the students ($ sum J_{xy} sigma_x sigma_y$). We can note that this maximization problem is equivalent to the energy minimization problem presented by the spin glass (i.e. the minimization of a negative quantity is equivalent to the maximization of its modulus) . Mathematical Analysis . The nice thing about the assumptions made by Sherrington-Kirkpatrick is that it allows for (comparatively) easy mathematical analysis. This is largely due its regularity (every atom looks like every other), by assuming arbitrarily large spin glasses we can also take limits and look at asymptotic behaviour. In other models this is not always possible and if it is it becomes increasingly more difficult. . First question we will ask is what is the minimum value attained by the Hamiltonian? This is equivalent to: $$M_N = max_{ sigma} sum J_{xy} sigma_x sigma_y = max_{ sigma} left[ - H_N right] $$ Where we are considering $N$ atoms in the system. We are using a slighlty sloppy notation for $max_{ sigma}$ to represent the maximum over all possible configurations. . Studying maximum quantities mathematically is often difficult. To overcome this difficulty we instead look at the Helmholtz free energy function ($F_N$) instead: $$F_N( beta) = frac{1}{N beta} mathbb{E} left[ log sum_{ sigma} exp(- beta H_N) right] $$ . We have done this since we can write the following inequality: $$ frac{1}{N} mathbb{E} left[ M_N right] leq F_N( beta) leq frac{1}{N} mathbb{E} left[ M_N right] + frac{log(2)}{ beta} $$ . This is a deceptively simple inequality, to see why it holds for the lower bound we replace the summation in $F_N$ by the maximum value in the sum (1 term). For the upper bound we replace every term in the sum with this attained maximum ($2^N$ values). The logs/exponents/beta/etc. all cancel leaving the result. This is useful to us because it means in the limit $ beta to infty$ we get the relation: $F_N( beta) to frac{1}{N} mathbb{E} left[ M_N right] $ which is what we are interested in studying. . This is closely related to the Gibbs distribution of the system. This gives us a probability distribution of states of the spin glass: $$G_N( sigma) = frac{exp(- beta H_N( sigma))}{Z_N( beta)} $$ Where $Z_N( beta)$ is the partition function, it is a normalizing constant (i.e. it is a sum over all possible terms of the numerator of $G_N$). We can think of the Gibbs distribution as weighting the configurations according to their free energy. We can see that finding the partition function is the crux of understanding the Gibbs distribution (and through comparison to the free energy the ground states). . One of the first ways this was investigated mathematically was to use a &quot;replica trick&quot;, this is just a result of using the identity: $$ln(Z) = lim_{n to 0} frac{Z^n - 1}{n} $$ . On the partition function. Essentially one takes $n$ independent copies (replicas) of the system and computes an average over all of them. Approximations are then used to take the limit $n$ to zero. For certain system behaviours this method works well but for others it can be inaccurate. In particular looking at very low temperatures (small beta near ground state) the approximations do not perform well. This method assumes certain symmetries between atoms which are not true in practice (due to taking independent replicas and averaging), these methods have been extended to &quot;replica symmetry breaking&quot; (RSB) methods. These methods were further superceded by the work of Parisi using variational principles. The mathematical details would take too long to put in a blog like this. Please see the references for links to papers on the topic. . What we are really interested in with spin glass systems is when a phase transition occurs. To do this physicists look at an order parameter which captures all behaviours of the system. Edwards and Anderson suggested the following order parameter: $$ q = frac{1}{N} sum_x hat{ sigma}_x^2 $$ Where $ hat{ sigma}_x$ represents the average over time of spin $x$. This order parameter is such that for $q=0$ the configuration is non-magnetic. For $q&gt;0$ then it is in a spin glass phase. Using the replica method (and some work!) we can show that under full symmetry we have that $q$ satisfying the self-consistency formula: $$ q = 1 - frac{1}{ sqrt{2 pi}} int_{- infty}^{ infty} exp(-z^2/2) sech^2( beta s sqrt{q} z) dz $$ Using this we can find a phase transition occurs at $T=s$ - temperatures below this the system exhibits glassy behaviour and above this the system is not magnetic (in equilibrium). . Unfortunately assuming this sort of symmetry this $q$ does not behave well at lower temperatures, it does not display all the characteristics of the system. . If we introduce symmetry breaking we re-write the order parameter of the form: $$ q_{ alpha beta} = frac{1}{N} sum_x hat{ sigma}^{ alpha}_x hat{ sigma}^{ beta}_x$$ For two states $ alpha$ and $ beta$ - This is also sometimes called the &quot;spin overlap function&quot;. If we consider the likelihood of observing a system state $ alpha$ as $W_{ alpha}$ (so that $ sum_{ alpha} W_{ alpha} = 1$) we can define the overlap density: $$P_ tilde{J}(q) = sum_{ alpha beta} W_{ alpha}W_ beta delta(q - q_{ alpha beta})$$ Where $ delta(.)$ is the Dirac delta function and the density is defined for some fixed realisation of interaction strengths $ tilde{J}$. Finally we can use this to define the Parisi order parameter function as: $$P(q) = int prod_{xy} Q(J_{xy}) P_ tilde{J}(q) dJ_{xy}$$ With $Q(J_{xy})$ representing the density by which the interaction strength is chosen (e.g. Gaussian). This order parameter does not suffer from the issues of symmetry like the order parameter function above. . In addition to uncovering a phase transition, the new Parisi order parameter uncovers some other interesting properties: . The broken symmetry of the spin glass requires an infinite number of order parameters to characterize | In the limit of large-N there is no self-averaging in the spin glass state. That is there exists distinct samples even as N approaches infinity. This is unlike most other systems where there is no concept of a &quot;sample&quot; when N increases. | Given 2 states of the spin glass, there is no &quot;inbetween&quot; spin glass state - this is called ultrametric structure. This gives the space of spin glass states a very interesting structure. In some sense the states are clustered. More than that it is clustered at any scale you look at - if you look at states within a distance of $d$ of each other you get a number of clusters, if you look at a larger scale $d&#39; &gt; d$ then these small clusters will merge into larger ones. (Distance here is defined using overlap of states). | . Simulation . After all the theory we will now look at a simple simulation of this model. . We start by doing a very naive Monte-Carlo method - we will generate random configurations, calculate the energy and a Gibbs measure. With the results we will estimate the average energy, the average magnetism, ground state energy and the partition function. If we want to find the ground state this would be a very bad method, for an $N$ size spin glass there will be $2^N$ possible configurations so finding any one will be difficult! We can implement this: . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Very naive Monte-Carlo approach import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N N = 1000 # Fix number of timesteps and some containers timesteps = 10000 gibbs = np.zeros(timesteps) energy = np.zeros(timesteps) mag = np.zeros(timesteps) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Fix Temperature for Gibbs distribution beta = 1/(s*0.5) for i in range(timesteps): configuration = np.random.choice([-1, 1], N) energy[i] = -1 * np.dot(configuration, np.dot(configuration, interaction)) / 2 gibbs[i] = np.exp(-beta*energy[i]) mag[i] = configuration.sum() print(&quot;Estimated Ground State Energy: &quot;, energy.min()) print(&quot;Estimated Average Energy:&quot;, energy.mean()) print(&quot;Estimated Partition Function:&quot;, gibbs.mean()) print(&quot;Estimated Average Magnetism:&quot;, mag.mean()) . Estimated Ground State Energy: -78.55263447711921 Estimated Average Energy: 0.1314391563670086 Estimated Partition Function: 1.7717002620362574e+64 Estimated Average Magnetism: 0.5468 . Next we take another bad method: a greedy hill climber (or greedy gradient descent). The idea behind this algorithm is that when we are in one configuration we pick a site at random and look at the change in energy associated with flipping the spin. If the energy is lower we accept the change, if higher we ignore and pick another site at random. This algorithm will converge to some local minima but it will not necessarily be a good global minima. We can code this up as: . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Greedy gradient descent import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 1000 spins = np.random.choice([-1, 1], N) # Fix number of timesteps and some containers timesteps = 100000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Calculate initial values mag[0] = spins.sum() energy[0] = -1 * np.dot(spins, np.dot(spins, interaction)) / 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1/(0.5*s) # Define update step dE = 0 dM = 0 def update(s_array, i_array): &quot;&quot;&quot; update function performs 1 update step to the model inputs: s_array - an array of N spins (+-1) i_array - an array of interaction strengths NxN &quot;&quot;&quot; global dE global dM _N = s_array.shape[0] old_s = s_array.copy() # Select a spin to update site = np.random.choice(_N, 1)[0] # Get interaction vector i_vector = i_array[site,:] # Calculate energy change associated with flipping site spin dE = 2*np.dot(i_vector, s_array)*s_array[site] dM = -2*s_array[site] # Sample random number and update site if dE &lt;= 0: s_array[site] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts , s_array, i_array): s_temp = s_array.copy() for i in range(ts): update_step = update(s_temp, i_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, interaction) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . We can see after about 25,000 steps the system is &quot;stuck&quot; in a local energy minima. If we re-ran the code starting from a different spot we would likely end up in a vastly different configuration. . The last method we will look at is the Metropolis-Hastings algorithm. Given a configuration of spins we will perform an update step by picking a site at random, we will compute the probability of flipping the spin and then accept/reject this change based on a random draw. This process will be repeated for a set number of steps. We will keep track of the energy of the system (the Hamiltonian) and the overall magnetism ($ sum_x sigma_x$). . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Updates occur at randomly selected sites import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 1000 spins = np.random.choice([-1, 1], N) # Fix number of timesteps and some containers timesteps = 100000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Calculate initial values mag[0] = spins.sum() energy[0] = -1 * np.dot(spins, np.dot(spins, interaction)) / 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1/(0.75*s) # Define update step dE = 0 dM = 0 def update(s_array, i_array): &quot;&quot;&quot; update function performs 1 update step to the model inputs: s_array - an array of N spins (+-1) i_array - an array of interaction strengths NxN &quot;&quot;&quot; global dE global dM _N = s_array.shape[0] old_s = s_array.copy() # Select a spin to update site = np.random.choice(_N, 1)[0] # Get interaction vector i_vector = i_array[site,:] # Calculate energy change associated with flipping site spin dE = 2*np.dot(i_vector, s_array)*s_array[site] dM = -2*s_array[site] # Calculate gibbs probability of flip prob = np.exp(-beta*dE) # Sample random number and update site if dE &lt;= 0 or prob &gt; np.random.random(): s_array[site] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts , s_array, i_array): s_temp = s_array.copy() for i in range(ts): update_step = update(s_temp, i_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, interaction) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . In this code we can see in the beginning the magnetism of the system fluctating and the energy decreasing. This stablises to some sort of &quot;quasi-equilibrium&quot;. . Since the energy always decreases it suggests the system is slowly finding its way to a local energy minimum rather than exploring to find a better energy minima. This is to be expected with such a basic implementation. To observe this better we will re-run the code for a second time from a different starting spot, we will compare the resulting spin array - we should notice that there is quite a large discrepency between the 2 runs - this shows that the system is settling down to a different local energy minima. . old_spins = spins old_energy = energy[timesteps] old_mag = mag[timesteps] spins = np.random.choice([-1, 1], N) #### Run Main Loop _main_loop(timesteps, spins, interaction) # Calculate a distance metric dist = ((old_spins * spins).sum() / N + 1) / 2 print(&quot;Proportion of sites with the same spin is:&quot;, dist) print(&quot;Resting energies of the 2 systems are:&quot;, old_energy, &quot;and:&quot;, energy[timesteps]) print(&quot;Resting magnetism of the 2 systems are:&quot;, old_mag, &quot;and:&quot;, mag[timesteps]) . Proportion of sites with the same spin is: 0.508 Resting energies of the 2 systems are: -608.7546315240614 and: -589.7510068842638 Resting magnetism of the 2 systems are: -84.0 and: 56.0 . The proportion of sites having the same spin is high (around 50%!) and the energy attained is different, suggesting the system has converged to 2 different local minima that are &quot;far apart&quot; from each other. If we want to find the global minima (or at least a &quot;better&quot; minima) we will have to adopt a better strategy. We will introduce some options when we look at the next spin glass model. The code above should act as a warning about blindly simulating and relying on computational power/time to solve complex problems: it doesn&#39;t always work! . We&#39;ll finish this blog post by looking at the Edwards-Anderson order parameter. We will use a basic numerical technique to solve the self consistency integral equation. . from scipy.integrate import quad def integrand(x, c): return np.exp(-x**2/2)*np.cosh(c*x)**(-2) n_approx = 100 beta_min = 0 beta_max = 2 beta_array = np.arange(n_approx + 1)*(beta_max - beta_min)/n_approx + beta_min q_array = np.zeros(n_approx+1) thresh = 0.001 n_max = 100 for i in range(n_approx+1): beta_tmp = beta_array[i] q_old = 0 q_tmp = 1 j = 0 while np.abs(q_old - q_tmp) &gt; thresh and j &lt; n_max: q_old = q_tmp c = beta_tmp*s*np.sqrt(q_old) I = quad(integrand, -np.inf, np.inf, args=(c)) q_tmp = 1 - I[0] / (np.sqrt(2*np.pi)) j =+ 1 q_array[i] = q_tmp plt.plot(beta_array, q_array) plt.xlabel(r&quot;$ beta$&quot;) plt.ylabel(&quot;q&quot;) plt.title(&quot;Edwards-Anderson Order Parameter (s=1)&quot;) plt.show() . This displays the behaviour we expect (approximately) for low beta below $1/s$ the temperature is above $s$ and so $q=0$ (non-magnetic) above this point the system is in the glassy phase. Since we have only approximated here there is some noise around the transition point. As we add more approximation points the transition should become sharper at $ beta = 1/s$. . Conclusion . In this blog post we have introduced the assumptions of the Sherrington-Kirkpatrick (fully connected Ising) spin glass model. We have seen that although fairly involved we can &quot;solve&quot; this model analytically to uncover its properties. We have also a basic implementation in Python - however as we noted this has bad convergence properties so shouldn&#39;t really be used other than for illustration. . References . This blog post was inspired by chapter 5 of &quot;Spin Glasses and Complexity&quot; by Daniel L Stein and Charles M Newman. . Some relevant papers include: . The original paper: &quot;Solvable Model of a Spin-Glass&quot; - Sherrington, Kirkpatrick 1975 | Summary of Parisi Method: &quot;The Sherrington-Kirkpatrick model: an overview&quot; - Panchenko 2012 | . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-2",
            "relUrl": "/spin-glass-models-2",
            "date": " • Mar 10, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Spin Glass Models",
            "content": "What is a Spin Glass? . A spin glass is not something you find down the pub (well it could be, but thats not what we&#39;re talking about here). Instead spin glasses are models of certain magnetic materials. Very loosely we can think of the atoms of a magnetic material as having a &quot;spin&quot; relating to the magnetic polarity (&quot;north&quot; or &quot;south&quot; ends of a bar magnet) - we typically call these up-spin and down-spin. In this context we use the term &quot;atom&quot; loosely, it may be an atom in a chemical sense but it could also be a molecule - essentially a minimal element of the material. . In a ferromagnetic material (such as iron) spins orient in the same direction. In contrast antiferromagnetic materials the spins orient to oppose each other. In a spin glass we have some ferromagnetic interactions and some antiferromagnetic interactions, we say the system is &quot;disordered&quot;. . . In the image above we can see various spin configurations. The spins are indicated by arrows (yellow for up and green for down) the ferromagnetic interactions are denoted by blue lines while antiferromagnetic interactions by red. However it is somewhat misleading to think of these occuring only within a square lattice in 2d such as this. Spins also do not have to be 180 degree rotations of each other - they can be at arbitrary angles from each other (but this would make for a messy diagram). A spin glass can also occur in arbitrarily many dimensions, and the interactions do not only have to occur between &quot;nearest neighbours&quot; any atom can have any number of interacting partners. . We can represent the energy of a spin glass system using the Hamiltonian: $$ H = - sum_{x,y} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ Where $J_{xy}$ denotes an interaction strength between atoms $x$ and $y$. $ sigma_x$ represents the spin/magnetism of an atom $x$ (can be a vector in which case all products are dot products). A system will tend to a state of lowest energy and so $J_{xy} &gt; 0$ represents a ferromagnetic interaction (minimised for pairs of matching spin) and $J_{xy} &lt; 0$ for antiferromagnetic interactions. The range over which the sum applies has been left purposely ambiguous as to allow for various lattice topologies. The second summation reflects an interaction with an external magnetic field (denoted by $h$) if there is no external field the term can be ignored. (It is worth noting that magnetic spins for a specific atom can only take one of two values (rotated through 180 degrees) since electrons have half integer spin) . This allows us to define an important term relating to spin glasses. We can note that at a minimal energy (&quot;ground state&quot;) the spin glass will not be &quot;ordered&quot; (it will appear &quot;random&quot;). We call this property &quot;quenched disorder&quot; - this is due to the similarity to glass materials that are essentially cooled down liquids that get &quot;frozen&quot; into a state of disorder. . It is important to note this is different to pure &quot;randomness&quot;. If we think of a continuum whereby we have complete order on one side (think of a crystalline type structure as an example) and pure randomness on the other the spin glass lives somewhere in the middle - partially structured and ordered but partially random. This is a particularly interesting place to be: in the pure ordered end of the scale there are many established mathematical tools to deal with this situation. Similarly in a pure random situation probability and statistics can provide us tools for study. In the middle things get complicated since you cannot necessarily assume that any one element will &quot;look the same&quot; as any other - thus mean field methods fall down. This occurs in many &quot;real world&quot; phenomena, this can result in a lack of study since it &quot;falls inbetween&quot; different disciplines. . . There is another important term relating to spin glasses called &quot;frustration&quot;. This is where an atom has interactions with other atoms that are in conflict with each other - one interaction would suggest a lowest energy state for the atom is an up spin and another interaction suggests a down spin. An example of this can be seen below: . . We can see that the spin of the centre atom is not clearly defined by interaction with its neighbours. The vertical neighbours interactions suggest the lowest energy state would be a yellow up spin, while horizontal neighbours suggest a green down spin. In a large spin glass with random (or nearly random) configuartions there may be many such frustrated atoms. This gives rise to complexity and questions such as &quot;what is the lowest energy state for a system?&quot; becomes very difficult to answer. In many cases we are not able to determine this analytically. The energy landscape (the Hamiltonian energy for a given configuration of spins given a fixed topology of interactions) can become very complex with lots of local minima, which means &quot;typical&quot; optimization procedures based around greedy hill climbing (and the like) will struggle to find the global minimum. See the plot of energy landscape below as an example: . . If one ends up in a configuration near one of these local minima it requires relatively large changes to the configuration to escape the valley. This leads to a kind of &quot;metastability&quot; in the system where the configuration will &quot;stick&quot; around these points for a long time. As such spin glasses tend to violate the Ergodic principle, this again adds to the mathematical complication in dealing with these systems. . Although the system would &quot;prefer&quot; to be in a lower energy state through the application of a temperature (or placing the system within an external magnetic field) the atoms can have sufficient energy to escape this lower energy state. For high enough temperatures this means a ferromagnetic material can become antiferromagnetic. In most cases there exists a critical temperature where a phase transition occurs. Phase transitions are interesting examples of emergence - one example of a phase transition that everybody is familiar with is the phenomena of melting a solid to create a liquid. It is interesting that this is a very sharp transition - why is it not the case that a solid gradually becomes &quot;softer&quot; and more liquid like? Instead small temperature fluctuations can cause the state of matter to change. It is not immediately obvious why this is the case, other phase transitions exist in other systems and they are often interesting to analyse. . The eagle eyed amongst you may notice that we have ignored the interference between spins themselves. It is true that this will have an impact but in most mathematical models of spin glasses it can be ignored. As with all mathematical models we look for a &quot;minimal description&quot; that captures the behaviour of interest, it turns out that this complication tends not to add much to the model (although I&#39;m sure there exists research with interesting results capturing this interference). . So what are some physical examples of a spin glass in the real world? Technically any iron magnet subject to rust (which is antiferromagnetic) will be a spin glass, however typically the ferromagnetic atoms will still be so prevelent that we can think of it as a ferromagnet. There are other &quot;exotic&quot; molecules (e.g. europium strontium sulphide) that are spin glasses also. Many of the experiments on spin glasses involve melting down a noble metal (e.g. gold or silver) and adding a small amount of dispersed molten iron (typically around 0.1-5%) and cooling the mixture very quickly. Many counter-intuitive and contradictory properties have been found through these experiments including: . By cooling quickly one can avoid the transition from liquid to solid - creating a viscous liquid spin glass | Relaxation times (how long it takes the system to adjust to changes in temperature) can be very slow, way beyond experimental time frames | Interactions with magnetic fields are odd. Absent of a magnetic field a spin glass is not magnetic. By carefully applying and removing external magnetic fields one can create a magnetic spin glass with varying properties (decays, apparent permanence etc.) | Spin glasses appear to have a &quot;memory&quot; of previous states and undergo something akin to an aging process Creating theoretical explanations of these (and other) phenomena is the subject of much research on the subject. | . Why do we care? . Ok, so at this point we may have a base understanding of what a spin glass is and some of the complications and properties therein, but you may be thinking: &quot;but who cares about magnets anyway?&quot; (Unless of course you are Charlie Kelly) It does seem like a lot of work and as a non-physicist it might seem interesting. However in dealing with the complications of spin glasses we can gain a lot of insight into other systems. In the next few bullet points I will try and convince you that it is worth time playing around with spin glasses: . They&#39;re interesting! - Spin glasses exhibit a number of properties that I personally find very interesting, for example: emergent behaviour, &quot;in between&quot; order and randomness, simple concepts to explain but difficult to write down mathematically, etc. | Non-ergodic systems are everywhere! - Although spin glasses themselves are quite stylised if they can give insight into the behaviour of non-Ergodic systems this is very useful. Loosely speaking an Ergodic system does not exhibit path depedence (e.g. whatever the state at present eventually any other state can be reached). When looking at complex systems this is typically not the case. | Frustration occurs more than we would like - We often end up in the situation with &quot;conflicting&quot; information and dealing with this gives rise to many opportunities. | They&#39;re easy to simulate - while some of the properties above make mathematical analysis difficult in all but a few special cases, spin glasses are fairly easy to code up and simulate. If you wonder &quot;what would happen if....?&quot; you can quickly modify a model and play around to see what happens, you don&#39;t need to spend much time thinking about boundary/initial conditions or other technical aspects. | There are many different applications - Given the ubiquity of some of the complications relating to spin glasses the techniques and theory have been applied to many situations including (but not limited to): optimization techniques, neural networks (biological and artificial), machine learning, protein folding, materials science, evolutionary models. The study of quantum spin glasses is also fairly active with applications in quantum computing. | . Conclusion . In this blog post we have been on a whistle-stop tour of the very basic concepts of spin glasses and models of spin glasses. We have seen some of the difficulties with them and what makes them interesting and useful to study. In future blog posts we will look at specific models and mathematical techniques used to study them. . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models",
            "relUrl": "/spin-glass-models",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Standing Ovation Model",
            "content": "Standing Ovations - as a Phenomena . Although not particularly &quot;exciting&quot; for a subject to model, standing ovations have many interesting properties. A truly fantastic performance may elicit an instantaneous standing ovation whereby all audience members instinctively decide to stand. However this cannot be the only factor, we can observe in other performances a few hardcore fans may initially stand and then pass through the crowd as a &quot;wave&quot; through social pressure (or perhaps the inability to see the stage if others in front of you stand!) Eventually resulting in a standing ovation. At other times maybe only a handful of audience members stand, notice very few people around them are standing and then sheepishly sit down again. There is a clear temporal aspect to standing ovations but there is also some sense of criticality whereby a phase transition occurs. While understanding the mechanisms behind this for standing ovations may not have much &quot;value&quot; in of itself these principles turn up in many complex systems and so understanding here can be transferable to more &quot;interesting&quot; situations. . Modelling Standard Ovations . We now move onto the question: how can we model standing ovations? We first consider the initial (instinctive) decision of standing or not following a performance. Clearly for a &quot;perfect&quot; performance (however that may be defined) everybody would show their appreciation and stand. However when the performance is not &quot;perfect&quot; why do some stand and some not? We could model each audience member as having some &quot;error function&quot; that clouds their judgement of the performance, so for (objectively) a good performance we could have that some percieve the performance as bad owing to their error function, and so they decide not to stand. We can express this mathematically: we denote the performance quality as $P$. Each individual has an error-rate $ epsilon$ - this defines a signal $S$ as: $$ S = P + epsilon $$ We can then state that an individual will stand for a signal that is in excess of their threshold $T$. We can take this threshold to be fixed for all individuals by adjusting their $ epsilon$ accordingly. . But what does the error-rate represent? There are a number of interpretations. For example it could represent &quot;stubborness&quot;, some may be difficult to impress. It could also represent differences in knowledge, somebody unfamiliar with jazz-club etiquette may stand/stay seated at the wrong times whereas somebody more seasoned will be more closely tied to the quality of the performance. . For arguments sake let&#39;s say that $P$ can vary from 0 to 1. The error rates are uniformly distributed. We can then code up this initial standing ovation model as below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set audience size N = 50 M = 100 # Fix performance quality as 50% P = 0.5 # Fix seed np.random.seed(123) # Fix all audience as seated (0) initially audience = np.zeros((N, M)) # Set error functions for audience members epsilon = np.random.random((N, M)) - 0.5 def initial_stand(error, p, t): signal = p + error out = signal &gt; t return out * 1 # We set up an array to vary the threshold T = np.arange(100) / 100 Pct = np.zeros(100) # Loop over all thresholds and plot the percentage of audience standing for i in range(100): Pct[i] = initial_stand(epsilon, P, T[i]).mean() plt.plot(T, Pct) plt.xlabel(&quot;Threshold&quot;) plt.ylabel(&quot;Percent Standing&quot;) plt.show() . As expected we get a straight line decreasing with threshold. From the discussion before we know that standing ovations are not entirely determined by the performance quality (and resulting threshold) behaviour. There is a social aspect to the phenomena. We now move onto looking at a temporal model. One way we could do this is that an individual takes a survey of all their neighbours and if enough of them are standing they also decide to stand (somewhat similar to the movement mechanism in a Schelling Segregation Model). However if you are sat in the audience it is unlikely you&#39;ll turn round to see what the people behind you are doing, you will only notice those in front of you. You may also notice those from many rows in front of you rather than just proximal neighbours. Page/Miller suggest a viewing &quot;funnel&quot; where each agent can see as: . Where we can adjust the number of rows somebody can see as a paramter (with 3 presented here). We will make a small adjustment to this method; let&#39;s place more weight on the audience members closest to us. We will calculate the proportion standing in each row in or field of vision and take a weighted sum of these according to the square distance away (e.g. row 2 will be weighted 1/4 of row 1, row 3 will be 1/9 weighted and so on.) This should capture the behaviour that there is more social pressure from those around us, this might lead to a slow &quot;wave&quot; propagating through the audience as this pressure grows. For this model lets also assume that the lights are on in the theatre and we can see all rows up to the front in detail. . For this &quot;funnel&quot; we will ignore the proximal &quot;next door&quot; neighbours from this calculation. We will assume there is a different mechanism for these individuals, we will look at the proportion of neighbours standing (either 0, 0.5 or 1) and we will apply a &quot;neighbour weight&quot; which will weight between this score and the funnel score to give an overall social pressure score as: $$ Social _Pressure = (1 - neighbour _weight) times Funnel _Pressure + neighbour _weight times Neighbour _Pressure $$ . We will use this mechanism of seperating out next door neighbours since if we go to a performance with others they will typically be sat next to us. Further one could argue that somebody is more likely to stand/sit based on their friend/partner&#39;s behaviour than those of a stranger. The neighbour weight parameter allows us to vary the relative importance. . Using the overall social pressure metric a sitting individual will stand if the social pressure exceeds a fixed threshold, similarly a standing individual will sit if the social pressure is smaller than one-less the fixed threshold. We are assuming that there is a symmetry invoked here which may not be the case. For example the &quot;embarrassment&quot; of standing while others are sitting might mean people are quicker to sit if they&#39;re in a small minority standing. However for a first quick and dirty implementation of a model the symmetry assumption will suffice (it will be easy enough to modify later should we desire). . We will also add 2 random components to this model: the first being a rate that if one is sitting down they stand up regardless of the information presented. The other being the reverse of this: a rate by which individuals sit down if they&#39;re standing. We will denote these rates as $ delta, gamma$ respectively. This might allow for the possiblity of a &quot;spontaneous ovation&quot; occuring. . We now have one choice remaining before we can implement this model: how will the updates be made? There are 2 main classes of updates in an agent based model: asynchronous and synchronous. The former essentially means each agent updates &quot;one by one&quot;, the order of upates can be random or by some defined order (in this example we could update those nearest the stage and iterate backwards for example). In a synchronous updating scheme all agents updated their state together once per time-step, of course by doing this one has to be careful in some situations (e.g. if there is an asset that can get depleted who gets to use it first?) but in this simple model there are no such concerns. For this model I think synchronous modelling is appropriate since I believe this is how ovations work in practice (e.g. we do not wait for &quot;our turn&quot; to make a decision), we could investigate how this affects the outcome later if we wish to. . My implementation of this model can be seen below. We will look at how the proportion of audience members standing evolves over time (functions use njit decorator to simply improve run time - looping over the array multiple times in the funnel calculation is fairly slow in pure python): . import numpy as np from numba import njit import matplotlib.pyplot as plt %matplotlib inline # Set audience size N = 50 M = 100 # Fix performance quality as 50% P = 0.5 # Fix Performance threshold T = 0.6 # Fix seed np.random.seed(123) # Fix all audience as seated (0) initially audience = np.zeros((N, M)) # Set error functions for audience members epsilon = np.random.random((N, M)) - 0.5 # Fix number of timesteps T_steps = 100 Pct_Hold = np.zeros(T_steps+1) # Fix social pressure threshold Pressure_T = 0.4 # Fix neighbour weight N_weight = 0.5 # Fix probaility of spontaneous standing delta = 0.1 # Fix probability of spontaneous sitting gamma = 0.05 # Calculate initial reactions to performance @njit def initial_stand(error, p, t): signal = p + error out = signal &gt; t return out * 1 # Function to calculate social pressure @njit def pressure(i, j, aud): rows = aud.shape[0] cols = aud.shape[1] pct_sum = 0 norm = 0 for x in range(i): active_row = i - x - 1 left = max(j - (x+1), 0) right = min(j + (x+1), cols) pct_sum += aud[active_row, left:right+1].mean() / (x+1)**2 norm += (x+1)**-2 if norm == 0: res = 0 else: res = pct_sum / norm return res # Calculate pressure from (nextdoor) neighbours @njit def neighbour_pressure(i, j, aud): cols = aud.shape[1] count = 0 left = 0 right = 0 if j != 0: left = aud[i, j-1] count += 1 if j != cols -1: right = aud[i, j+1] count += 1 return (left + right) / count # Calculate overall pressure score @njit def pressure_score(i, j, aud, n_weight): return pressure(i, j, aud)*(1 - n_weight) + neighbour_pressure(i, j, aud)*n_weight # Spontaneous sitting/standing function @njit def spontaneous(i, j, aud, dlt, gma): rnd = np.random.random() if aud[i,j] == 0 and rnd &gt; (1-dlt): aud[i,j] = 1 elif aud[i,j] == 1 and rnd &gt; (1-gma): aud[i,j] = 0 return aud # Main Code Loop audience = initial_stand(epsilon, P, T) Pct_Hold[1] = audience.mean() for x in range(2, T_steps+1): audience_old = audience.copy() for i in range(N): for j in range(M): up_score = pressure_score(i, j, audience_old, N_weight) down_score = pressure_score(i, j, 1 -audience_old, N_weight) if up_score &gt; Pressure_T and audience_old[i,j] == 0: audience[i,j] = 1 if down_score &gt; Pressure_T and audience_old[i,j] == 1: audience[i,j] = 0 spontaneous(i, j, audience, delta, gamma) Pct_Hold[x] = audience.mean() plt.plot(Pct_Hold) plt.xlabel(&quot;Time Step&quot;) plt.ylabel(&quot;Percent Standing&quot;) plt.show() . By selecting particular parameters we can see a few different behaviours in the model. These include an inital peak of enthusiasm that dies down, a gradual increase as the applause trickles from the front of the auditorium to the back and a rapturous applause leading to nearly all participants standing up very quickly. These are all behaviours that we do see in real life audiences. If we were particularly motivated we could look for real world phenomena in standing ovations and see if these rules are able to replicate them. . Potential Improvements . With agent based models there are always &quot;extra&quot; things that could be added/modified. A few select suggestions include: . Revisit the &quot;funnel&quot; mechanism and compare various schemes | What happens if there are various levels/balconies or perhaps aisles between seats? | What happens with &quot;stooges&quot;? That is if we add agents who always applaud regardless, what is the minimum number required to guarantee a standing ovation? Where are they best placed (perhaps taking into account ticket cost - 10 in the middle compared to 2 at the front), etc. | We could code in &quot;partners&quot; or &quot;groups&quot; more precisely to investigate the impact that has | How does changing to asynchronous updating change the behaviour? | What if every so often an agent &quot;turns round&quot; to view the seats behind them? | and so on | . Conclusion . We have seen how we can code up a basic agent based model for standing ovations. While the subject itself isn&#39;t particularly &quot;useful&quot; in of itself the principles are common to many other systems. We have also seen some of the considerations that go into an agent based model. . As a final remark it is worth noting that this is not how you would produce an agent based model in practice. Typically you would start with a simple model and add features gradually and test the impact, I did not want to write a series of long posts on this model so I &quot;skipped to the end&quot; coding in what I thought some pertinent features could be all at once (although I did not spend long thinking about them!) . References . The original paper on standing ovation models presented by Scott Page and John Miller (can be seen here! for those wanting to read more about their version of the model. .",
            "url": "https://www.lewiscoleblog.com/standing-ovation",
            "relUrl": "/standing-ovation",
            "date": " • Feb 25, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Barnes-Hut Algorithm",
            "content": "The N-Body Problem . It is well understood that for $N &gt; 2$ that the problem of determining trajectories under gravitation for N-bodies cannot be solved analytically. Instead we must rely on computation to create trajectories. From Newton&#39;s equations we can write the force $ pmb{F_{i,j}}$ between two bodies $i, j$ with masses $m_i, m_j$ and positions $ pmb{p_i}, pmb{p_j}$ as: $$ pmb{F_{i,j}} = frac{G m_i m_j ( pmb{p_i} - pmb{p_j})} { lVert pmb{p_i} - pmb{p_j} rVert^3} $$ . For $N$ bodies we therefore have $ frac{N(N-1)}{2}$ forces to be calculated. For small $N$ this is not a problem, however if we want to model galaxy dynamics with thousands (or more) of bodies this becomes a computational issue. . Barnes Hut Process . With Barnes Hut we can move from $O(N^2)$ complexity to $O(Nln(N))$ - this is a significant improvement for large $N$. . The crux of the Barnes Hut scheme is to approximate the effect of &quot;far away&quot; bodies but calculate exactly the forces attributed to &quot;near&quot; bodies. To do this a quadtree data structure is used (in 3d an octree or higher dimensions a $2^d$tree). To do this the space is partitioned into 4 equal squares at depth 1 (or 8 cubes in 3d or $2^d$ hypercubes in higher dimension) - at depth 2 each of these squares is then partitioned into 4 equal squares and so on. From this a tree is constructed with each node representing the number of bodies within the quadrant at that depth. This is repeated recursively until a depth is reached where each quadrant has 1 or 0 bodies in it. It is easier to understand this with a visual example: . . With this data structure in place we can begin to simulate. For each quadrant in the quadtree we can calculate a centre of mass (in the obvious way). Then given a body we can calculate the force acting on it from a given quadrant by applying the Newtonian force calculation using the centre of mass. We could, for example, apply this to the 4 quadrants at depth 1 to approximate the force acting on the body. This of course might not be particularly accurate if there are many bodies present. To overcome this the Barnes Hut algorithm specifies a critical distance $ theta$ if the distance beween the body and the centre of mass of the quadrant is greater than $ theta$ then it is used as an approximation, if not the algorithm moves to the next depth of the quadtree and tries again, this happens recursively until the distance becomes below $ theta$ or there is only 1 body in the quadrant (as such setting $ theta = 0$ returns to the $O(N^2)$ brute force approach). . While this is presented as an algorithm for the N-body problem it can be used in other situations that involve a spatial dimension and many entities. I am currently considering an application to speed up an agent based model (potentially the subject of another blog post). . Implementation . Initially I will code up an example in Python, I may revisit this and create a more efficient implementation in Cython/C++/etc. I will ignore all &quot;complications&quot; for example I will not give each body a size nor will it consider collisions, etc. Each body is a &quot;point mass&quot; in this example. For now I&#39;ll also ignore the issues relating to plotting trajectories. . We first create a node class to represent nodes within the quadtree. Using this data structure we will then apply a verlet time step to calculate positions: . from copy import deepcopy import numpy as np class node: &quot;&quot;&quot; A class for a node within the quadtree. We use the terminology &quot;child&quot; for nodes in the next depth level - consistent with tree nomenclature If a node is &quot;childless&quot; then it represents a body &quot;&quot;&quot; def __init__(self, x, y, px, py, m): &quot;&quot;&quot; Initializes a childless node m - Mass of node x - x-coordinate centre of mass y - y-coordinate centre of mass px - x- coordinate of momentum py - y-coordinate of momentum pos - centre of mass array mom - momentum array child - child node s - side-length (depth=0 s=1) relpos = relative position &quot;&quot;&quot; self.m = m self.pos = np.array([x,y]) self.mom = np.array([px,py]) self.child = None def next_quad(self): &quot;&quot;&quot; Places node in next quadrant and returns quadrant number &quot;&quot;&quot; self.s = 0.5*self.s return self.divide_quad(1) + 2*self.divide_quad(0) def divide_quad(self, i): &quot;&quot;&quot; Places node in next level quadrant and recomputes relative position &quot;&quot;&quot; self.relpos[i] *= 2.0 if self.relpos[i] &lt; 1.0: quadrant = 0 else: quadrant = 1 self.relpos[i] -= 1.0 return quadrant def reset_quad(self): &quot;&quot;&quot; Repositions to the zeroth depth quadrant (full space) &quot;&quot;&quot; self.s = 1.0 self.relpos = self.pos.copy() def dist(self, other): &quot;&quot;&quot; Calculates distance between node and another node &quot;&quot;&quot; return np.linalg.norm(self.pos - other.pos) def force_ap(self, other): &quot;&quot;&quot; Force applied from current node to other &quot;&quot;&quot; d = self.dist(other) return (self.pos - other.pos) * (self.m * other.m / d**3) def add_body(body, node): &quot;&quot;&quot; Adds body to a node of quadtree. A minimum quadrant size is imposed to limit the recursion depth. &quot;&quot;&quot; new_node = body if node is None else None min_quad_size = 1.e-5 if node is not None and node.s &gt; min_quad_size: if node.child is None: new_node = deepcopy(node) new_node.child = [None for i in range(4)] quad = node.next_quad() new_node.child[quad] = node else: new_node = node new_node.m += body.m new_node.pos += body.pos quad = body.next_quad() new_node.child[quad] = add_body(body, new_node.child[quad]) return new_node def force_on(body, node, theta): if node.child is None: return node.force_ap(body) if node.s &lt; node.dist(body) * theta: return node.force_ap(body) return sum(force_on(body, c, theta) for c in node.child if c is not None) def verlet(bodies, root, theta, G, dt): for body in bodies: force = G * force_on(body, root, theta) body.mom += dt * force body.pos += dt * body.mom / body.m def model_step(bodies, theta, g, step): root = None for body in bodies: body.reset_quad() root = add_body(body, root) verlet(bodies, root, theta, g, step) ########## Main Code ########## # Parameters Theta = 0.7 G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X0 = np.random.random(N_bodies) Y0 = np.random.random(N_bodies) PX0 = np.random.random(N_bodies) - 0.5 PY0 = np.random.random(N_bodies) - 0.5 # Initialize Bodies = [node(x0, y0, pX0, pY0, masses) for (x0, y0, pX0, pY0, masses) in zip(X0, Y0, PX0, PY0, Masses)] # Main Model Loop def Model_Loop_BH(n): for i in range(n): model_step(Bodies, Theta, G, dt) %timeit Model_Loop_BH(N_steps) . 7.97 s ± 446 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . This code is not the most efficient possible for this model (in all likelihood one would only use Barnes-Hut for large galaxy simulations and so it&#39;s unlikely that Python would be the best choice). For 100 bodies and 1000 time steps this code runs on my machine in around 8s. . We can also code up a &quot;brute force&quot; approach (calculating forces between all pairs of bodies) to compare, it is likely that the cost of computing the quadtree could be more costly than just computing all forces directly when the number of bodies is suitably small. We could run these codes multiple times to find where this is the case. . import numpy as np # from numba import njit G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X = np.random.random(N_bodies) Y = np.random.random(N_bodies) PX = np.random.random(N_bodies) - 0.5 PY = np.random.random(N_bodies) - 0.5 pos = np.array((X,Y)) mom = np.array((PX, PY)) #@njit def force_array(pos_arr, m_array): n = pos_arr.shape[1] force_arr = np.zeros((2 ,n, n)) for i in range(n): for j in range(i): force_arr[:, i, j] = G * m_array[i] * m_array[j] * (pos[:,i] - pos[:, j]) / np.abs((pos[:,i] - pos[:, j]))**3 force_arr[:, j, i] = - force_arr[:, i, j] return force_arr #@njit def update_mom(step, mom_arr, force_arr): n = mom_arr.shape[1] del_mom = np.zeros_like(mom_arr) for i in range(n): for j in range(n): del_mom[:, i] += step * force_arr[:, i, j] return mom_arr + del_mom #@njit def update_pos(step, pos_arr, new_mom, m_arr): return pos_arr + step * new_mom / m_arr #@njit def main_loop(n, pos_arr, mom_arr): for i in range(n): force = force_array(pos_arr, Masses) mom_arr = update_mom(dt, mom_arr, force) pos_arr = update_pos(dt, pos_arr, mom_arr, Masses) return pos_arr %timeit main_loop(N_steps, pos, mom) . 1min 18s ± 1.89 s per loop (mean ± std. dev. of 7 runs, 1 loop each) . This brute force code is not implemented in a particularly efficient way, however it is good enough for illustration purposes. Even for relatively small model sizes (100 bodies for 1000 time steps) the code takes considerably longer than the Barnes-Hut algorithm, with timeit giving an approximate timing of around 78s - about 10 times slower than the Barnes-Hut implementation. . If we use Numba with the njit decorator we can improve this to 3s - which while quicker is not considerably better than the Barnes-Hut algorithm. As the number of bodies increases (and perhaps some fine tuning of the Barnes-Hut parameters) we would expect even more drastic improvements. . import numpy as np from numba import njit G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X = np.random.random(N_bodies) Y = np.random.random(N_bodies) PX = np.random.random(N_bodies) - 0.5 PY = np.random.random(N_bodies) - 0.5 pos = np.array((X,Y)) mom = np.array((PX, PY)) @njit def force_array(pos_arr, m_array): n = pos_arr.shape[1] force_arr = np.zeros((2 ,n, n)) for i in range(n): for j in range(i): force_arr[:, i, j] = G * m_array[i] * m_array[j] * (pos[:,i] - pos[:, j]) / np.abs((pos[:,i] - pos[:, j]))**3 force_arr[:, j, i] = - force_arr[:, i, j] return force_arr @njit def update_mom(step, mom_arr, force_arr): n = mom_arr.shape[1] del_mom = np.zeros_like(mom_arr) for i in range(n): for j in range(n): del_mom[:, i] += step * force_arr[:, i, j] return mom_arr + del_mom @njit def update_pos(step, pos_arr, new_mom, m_arr): return pos_arr + step * new_mom / m_arr @njit def main_loop(n, pos_arr, mom_arr): for i in range(n): force = force_array(pos_arr, Masses) mom_arr = update_mom(dt, mom_arr, force) pos_arr = update_pos(dt, pos_arr, mom_arr, Masses) return pos_arr %timeit main_loop(N_steps, pos, mom) . 3 s ± 33.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Conclusion . We have seen that even for relatively modest models the Barnes-Hut algorithm provides fairly efficient computation for the N-body problem. We would expect the improvement to be even more marked for larger models. However Python on its own is not the best place to implement this and a C++ implementation would offer better performance. With some work a Cython approach should be possible and offer performance improvements. The code would require some major re-working if Numba is to be used instead owing to the reliance on classes to generate the quadtree. .",
            "url": "https://www.lewiscoleblog.com/barnes-hut",
            "relUrl": "/barnes-hut",
            "date": " • Feb 18, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Cython Vs Numba: An Example",
            "content": "Following on from the previous blog post here comparing the relative merits of Cython Vs Numba I thought I&#39;d illustrate this with implementations of a relatively simple model: a vanilla 2d Ising Model. This is a prime target for a performance boost since it is a very &quot;loopy&quot; code. I will not cover what the Ising Model is/how it works (you can check that here!). It is a very interesting model and I may return to it at a later date to look at some of its properties and explore it more deeply (and maybe spin glass models more generally). For now it will just be a test subject to explore performant python type code. . In this blog I will present a few different versions, I won&#39;t throw the kitchen sink at them to get the absolute best performance but will adopt an 80/20 principle. I will not try any parallelisation or clever memory management outside of what comes pre-canned. . Python . This is a basic python implementation using a lot of looping. Even for a relatively small models this code will likely be fairl slow due to the looping. . import numpy as np # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours def neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res def dE(i, j, spin_array): return 2*spin_array[i, j]*neighbour_sum(i, j, spin_array) def update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array def _main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = update(spin_tmp) return spin_tmp . Numba . For this code we will just take the above code and use the njit decorator (forcing the use of LLVM - a jit decorator will fall back to Python object mode if it cannot work out how to use LLVM). This is literally a few seconds of coding updates. . import numpy as np from numba import njit # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @njit def nb_neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res @njit def nb_dE(i, j, spin_array): return 2*spin_array[i, j]*nb_neighbour_sum(i, j, spin_array) @njit def nb_update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = nb_dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array @njit def nb_main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = nb_update(spin_tmp) return spin_tmp . Cython . Again we will modify the python code. We will see that while not too onerous it does require a little more work than the Numba example. The code is largely boilerplate but requires a little more thinking than the Numba example (e.g. in implementing this I initially forgot a static type definition of 1 variable which was causing a 300% increase in runtime). . %load_ext Cython . %%cython cimport cython import numpy as np cimport numpy as cnp from libc.math cimport exp from libc.stdlib cimport rand cdef extern from &quot;limits.h&quot;: int RAND_MAX # Grid size: N (int) NxN square latice cdef int N = 1000 # Fix Temp cdef float kT = 2 / np.log(1 + np.sqrt(2)) cdef float kTinv = 1 / kT # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_neighbour_sum(int i, int j, cnp.int32_t[:, :] spin_array): cdef int north = 0 cdef int south = 0 cdef int east = 0 cdef int west = 0 cdef int max_height = spin_array.shape[0] cdef int max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] cdef int res = north + south + east + west return res @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_dE(int i, int j, cnp.int32_t[:, :] spin_array): return 2*spin_array[i, j]*cy_neighbour_sum(i, j, spin_array) @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_update(cnp.int32_t[:, :] spin_array): cdef int height = spin_array.shape[0] cdef int width = spin_array.shape[1] cdef int y_offset, x_offset cdef int i, j cdef int dEtmp for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = cy_dE(i, j, spin_array) if dEtmp &lt;= 0: spin_array[i, j] *= -1 elif exp(-dEtmp * kTinv) * RAND_MAX &gt; rand(): spin_array[i, j] *= -1 return spin_array @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_main_code(int M, cnp.int32_t[:, :] spin_array): cdef int x for x in range(M): spin_array = cy_update(spin_array) return spin_array @cython.boundscheck(False) @cython.wraparound(False) cpdef cnp.int32_t[:, :] cpy_main_code(int M, cnp.int32_t[:, :] spin_array): return cy_main_code(M, spin_array) . Timing Results . # Python %timeit _main_code(10, spins) # Numba %timeit nb_main_code(10, spins) # Cython %timeit cpy_main_code(10, spins) . 52 s ± 2.53 s per loop (mean ± std. dev. of 7 runs, 1 loop each) 244 ms ± 4.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 395 ms ± 6.92 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . For our purposes the %timeout magic will be good enough a proxy for performance. We can see that the intial python implementation is very slow. For a 1000x1000 lattice and looping over 10 times, for my machine the python interation takes 50-55s. This would be fairly problematic if we wanted to sweep over the parameter space, find critical temperatures, perform random seed analyses etc. . In contrast Numba only takes 240-250ms, an impressive 2000% speed up. Running the code multiple times now seems much less onerous. . Cython is not quite as quick as the Numba implementation taking 390-400ms but still represents a significant speedup compared to Python. For practical applications the difference between Numba and Cython in this case may be insignificant. . It is worth noting that these times are from my machine on at a specific time. The times achieved on your machine might be slightly different. Similarly changing the size of the lattice may change the ordering of which option is &quot;quickest&quot; - as always it&#39;s worth checking the code how it will be used rather than performing a benchmark like this for determining which option to use. . Conclusion . From the results above it may be tempting to claim Numba is the obvious choice given it is not only easier to implement than cython but also offers faster speeds. However I selected the 2d Ising model as an example since I knew the code would work well in Numba (in a sense I have been p-value hacking the experiment!) In certain situations (e.g. a code relying very heavily on class structures) Numba is either unusable or requires a complete code overhaul whereas cython can require only a few lines of boilerplate code. . In other examples you can also see that Cython can severely outperform Numba, I am not sure why this is and the only real way to determine which will perform better is to perform testing (if somebody has an explanation/heuristic I&#39;d love to hear it). It is also possible to interface numba and cython which has been useful to me in the past. For a quick example suppose we want to perform an inverse transform of a Beta(2,0.5) distribution: . from scipy.stats import beta x = beta.ppf(0.1, 2, 0.5) x . 0.4681225665264196 . This cannot be optimised in Numba as it is (beta.ppf is currently not supported functionality - this may change by the time you read this). However we can take the address of the cython special function that this calls. We can then build the function in such a way as it can be seen by Numba: . import ctypes from numba import types, njit from numba.extending import get_cython_function_address betaaddr = get_cython_function_address(&quot;scipy.special.cython_special&quot;, &quot;btdtri&quot;) functype3d = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_double) beta_fn = functype3d(betaaddr) @njit def nb_beta_ppf(p, a, b): return beta_fn(a, b, p) x = nb_beta_ppf(0.1, 2.0, 0.5) x . 0.4681225665264196 . As we can see this is a little ugly but it works. . As a result of everything covered in these blog posts I do not believe that one option offers a significant advantage over the other and both offer valid tools for improving runtime. The choise of which to use in a given situation will depend on many factors and requires careful thought as to what is needed in a given setting. .",
            "url": "https://www.lewiscoleblog.com/cython-numba-2",
            "relUrl": "/cython-numba-2",
            "date": " • Feb 11, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Cython Vs Numba",
            "content": "Outline of the Problem . Python at its core is slow for certain things. By being a dynamically typed and interpreted language you incur certain runtime overheads. In some cases these are not much of an issue. At other times they can be critical. . Typically we will try and &quot;vectorize&quot; the code as much as possible (avoiding extraneous loops) and force as much code as we can into NumPy array operations which are typically &quot;quick&quot; (compiled C code). This is fine when it works, but it is not always possible to vectorize the code or, in some cases, the vectorization leads to code that is very hard to read/understand. . Both Numba and Cython (not to be confused with CPython) aim to provide tools to deal with such situations. . Outline of Cython . Cython is a programming language that is part Python and part C/C++, it can be compiled into a python extension and/or an executable. If you are familiar with Python it is reasonably easy to understand Cython code, it largely just has a few &quot;boiler-plate&quot; code blocks along with a few static type declarations (familiar to those who know C/C++/related languages). . Since there are no dynamic types (in well written Cython code) and it is compiled typically the resulting code is orders of magnitude faster than Python. Compared to vectorised NumPy there may not be a significant improvement but this depends on the exact implementation. . Cython itself is very flexible, if you can express the code in Python it is unlikely you will not be able to express it in Cython. Any arbitrary class structure can work within Cython, as a result it is used for many &quot;high performance&quot; Python packages (e.g. SciPy). . It is possible to parallelize the code or utilise GPU computation using Cython. This normally requires a bit of work but typically does not require nearly as much work as using Cuda in C++ (for example). As usual the normal caveats relating to multi-thread applications also apply to Cython code. . You can read the Cython documentation here! . Outline of Numba . Numba is a slightly different beast. It uses the concept of a &quot;just in time&quot; compiler (JIT). Essentially this means that code is compiled &quot;on the fly&quot; during runtime instead of requiring compilation prior to execution. Numba compiles the python code using a LLVM compiler. . The syntax is very simple and most of the time just requires a simple decorator on a Python function. It also allows for parallelisation and GPU computation very simply (typically just a &quot;target = &#39;cuda&#39;&quot; type statement in a decorator). In my experience a lot less thinking is required to set this up compared to Cython. . The downside of Numba (at least for me) is that it is a (comparatively) new package and as such does not have support for absolutely everything you would want, unlike with Cython it is possible to just &quot;hit the wall&quot; where you simply cannot use Numba without a major re-writing of the code. (One such example being you cannot call @guvectorize functions inside the @njit decorator). It is worth checking the github issues log regularly as often these issues are on the docket to be corrected in future releases. . Another downside of Numba is the lack of useful traceback, typically you need to &quot;switch off&quot; Numba and run in regular python to track down an error. This is typically only a minor inconvenience but if the code is particularly slow it can get frustrating trying to find an error without the Numba speed up. . You can read more about Numba here! . Which is better? . From a raw performance perspective I do not see either Cython nor Numba consistently beating the other in all situations. Typically the performance will be comparable and you will rarely find one being many orders of magnitude quicker (assuming you&#39;re using both correctly). . The choice of which to use, in my opinion, comes down to other factors. Convenience being a big one, I typically find Numba easier and quicker to implement when it works. As noted above however it doesn&#39;t always work (e.g. if using class structures, custom data types, etc.) With familiarity you do get an instinct as to whether a code will work or not. Cython on the other hand offers much more flexibility. . There is also the issue of how the code will be used. Cython is well established for creating efficient extension modules that sit nicely within the Python eco-system. Numba can be used in a similar way but I have found it a bit more finnicky to deal with (for example through Numba itself changing its API fairly regularly since it&#39;s a relatively new module, some code from previous iterations of Numba simply does not work at all with the later versions). . What is the catch? . Unfortunately things are not perfect, typically we will still be interfacing Cython/Numba functions via Python and so using repeated calls to these functions we will still incur overheads (typically through the conversion to Python types). This can mean that certain code is still significantly slower than C/C++ equivalents. These packages are therefore most useful for when you have profiled your code and can see that a handful of functions/operations are the real bottleneck. . These packages may not help if your code is particularly memory intensive, in which case it is better to spend time thinking about memory management instead. In some cases these packages provide some help in that respect also (e.g. NumPy is prone to creating many cached variables for simple operations, if the variables are large arrays this can become a pain.) . What about PyPy/etc? . Another option for performant Python code is to use PyPy instead of CPython. I have not used this very much yet, if I get the time to really kick the tyres I may write another blog on my findings. There are some features that appear useful but the eco-system is not as well supported (yet?) and so may require some additional work to recreate some high level functionality. . You can read more about PyPy here! . Why not just C/C++? . Ultimately if you require peak performance at all costs these options are still no substitute for well written C/C++. However as I often warn people: computation time is generally cheaper than human time - it is often better to use a slightly sub-optimal (but still respectable) code than devote months to R&amp;D and slow down the development cycle. Since Numba/Cython are so similar to Python (and it is possible to just &quot;tack on&quot; some Python to the end of these codes) you can prototype much more quickly in my experience. All these factors (along with many others such as where the code is to be deployed, what other tools are being used, etc.) need to be weighed up. . What about Julia? . Some readers may be familiar with the Julia language as an option for high performance scientific computing. I have a little experience (but am far from an expert). As I understand Julia is based around JIT (as with Numba), however being a language to itself it never needs to interface with Python and its limitations. It can therefore create more efficient code for larger scale projects since they never have to worry about Python overheads. Typically the benchmarks seen online are for smaller &quot;toy&quot; problems and so the performance does not appear to be too different from Cython/Numba. . I have not switched to Julia for a few reasons, firstly the popularity of Python - it is typically fairly easy to learn Cython/Numba for somebody who understands Python/NumPy making collaboration easier. Secondly the Python eco-system is well developed there is typically a package available to do almost anything you would want. Thirdly at this point it is fairly easy to get Python to &quot;speak&quot; with other systems if you need to turn something from a prototype to production. These concerns are ultimately just related to uptake however, as more people use Julia I see this becoming less of a concern. . You can read more about Julia here! . Conclusion . Hopefully now we can see that Cython/Numba provide useful tools for bridging the gap between Python and C/C++ runtimes. As the old saying goes &quot;you cannot have your cake and eat it too&quot; and so it may not be possible to get performance as quick using these options. However we can often get performance that is &quot;good enough&quot; in practical terms. .",
            "url": "https://www.lewiscoleblog.com/cython-numba",
            "relUrl": "/cython-numba",
            "date": " • Feb 4, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Jackknife Methods",
            "content": "In this blog post we are concerned with a specific problem: we have a Monte-Carlo type model that produces some simulated output. With this we want to estimate an arbitrary statistic (for example percentiles, expected shortfall or more complicated statistics relating to many variables). We know however that calculated in this way the calculated statistic is just one realisation of a distribution of outcomes. We would like to be able to say something about this distribution, in particular we would like to have some idea of the variability in the statistic. . The &quot;obvious&quot; (and most accurate) way to do this would be to re-run the model many times with different random seeds and create an empirical distribution of the statistic. However if the model is particularly complex this might mean a lot of compute time. Instead we would like to find an approximate method that does not require us to re-run the model at all. To do this in a general way we will introduce the Jackknife method. . It is worth noting that in certain situations other methods can be easier to implement/more accurate, for example if we only wanted to estimate the 90th percentile we can construct an argument using a binomial distribution (and normal approximation thereof) - however this method will not generalise to (for example) expected shortfall or even more complicated statistics. . Justification of the Method . We begin by supposing we have $N$ un-ordered observations $ { X_i }_{i=1}^{N}$ from our Monte-Carlo model. We use these observations to create an estimate of a statistic $ hat{Q}$. We denote the estimate from the model $ hat{Q}_{ {1:N }}$, which itself is a random variable. Through a Taylor expansion we can note: $$ mathbb{E} left( hat{Q}_{ {1:N }} right) = hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} + ...$$ For some constants $a_x$. . If we now consider partitioning the $N$ observations as: $N = mk$ for integers $m$ and $k$ - that is we create $m$ collections of $k$ obervations ($k gg m$). We denote a set: $A_i = { X_j | quad j &lt; (i-1) k , quad j geq i k }$ to contain all observations bar $k$, each set removes a different set of $k$ observations. Each has $|A_i| = (m-1)k$. We can then write: $$ mathbb{E} left( hat{Q}_{A_i} right) approx hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} $$ Via a second order approximation. . If we then define a new variable: $ hat{q}_i = m hat{Q}_{ {1:N }} - (m-1) hat{Q}_{A_i}$. Then via a second-order approximation we have: $$ mathbb{E} left( hat{q}_i right) approx m left( hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} right) - (m-1) left( hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} right)$$ Through some simplification this becomes: $$ mathbb{E} left( hat{q}_i right) approx hat{Q} - frac{a_2}{m(m-1)k^2} $$ Asymptotically this has bias $O(N^{-2})$. If the estimator only has bias $O(N^{-1})$ (i.e. $a_i =0$ for $i geq 2$) then the approximation is unbiased. . We can now define two new variables: $ hat{ hat{Q}}_N = frac{1}{m} sum_{i=1}^{m} hat{q}_i$ $ hat{ hat{V}}_N = frac{1}{m-1} sum_{i=1}^{m} left( hat{q}_i - hat{ hat{Q}}_N right)^2$ Then via CLT we have that $ hat{ hat{Q}}_N sim mathcal{N}( hat{Q}, hat{V})$ for some unknown variance $ hat{V}$. We can thus create an $X %$ confidence interval as: $ hat{Q} in left[ hat{ hat{Q}}_N - t sqrt{ frac{ hat{ hat{V}}_N}{m}}, hat{ hat{Q}}_N + t sqrt{ frac{ hat{ hat{V}}_N}{m}} right] $ With $t$ as the $(1-X) %$ point of a double-tail student-t distribution with $(m-1)$ degrees of freedom. . We can see from the construction of this confidence interval there has been no restriction on the type of statistic $ hat{Q}$ used, in this sense this is a generic method. . (Note this is strongly related to a bootstrap method, in fact it is a first order approximation to a bootstrap) . An Example . The explanation above is quite notation dense, it will be easier to look at an example. In this case we will take one of the simplest examples of a Monte-Carlo model: estimating the value of $ pi$. To do this we will take a unit square and a unit quarter circle inside it: . . We will simulate random points within the square and calculate the proportion $p$ of points landing within the quarter circle (red). If we simulate $N$ points we get an estimate $ pi approx frac{4p}{N}$. A simple vectorised numpy for this can be seen below: . # Estimating pi using Monte-Carlo import numpy as np def points_in_circle(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) y = np.random.random(N) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_circle &quot;&quot;&quot; return arr.sum() / arr.shape[0] * 4 SIMS = 10000 pts = points_in_circle(10000) print(&quot;Estimate of pi:&quot;, est_pi(pts)) . Estimate of pi: 3.1456 . We can use the jackknife method to now construct a confidence interval. (Obviously in this case we have the sample estimator of an average and so CLT applies, in practice we wouldn&#39;t use a jackknife here. But for this example I wanted something simple as to not distract attention from the jackknife method itself.) We know what the result &quot;should&quot; be in this example, however we shall pretend we don&#39;t have access to np.pi (or similar). . We can code up an implementation of the jackknife method as: . from scipy.stats import t def jackknife_pi(arr, pi_fn, m): &quot;&quot;&quot; This function implements the jackknife method outlined above The function takes an array (arr) and an estimate function (pi_fn) and a number of discrete buckets (m) - in this implementation m needs to divide size(arr) exactly The function returns Q-double hat, V-double hat, (m-1) &quot;&quot;&quot; Qn = pi_fn(arr) N = arr.shape[0] itr = np.arange(N) k = N / m q = np.zeros(m) for i in range(m): ID = (itr &lt; i*k) | (itr &gt;= (i+1)*k) temp = arr[ID] q[i] = m*Qn - (m-1)*pi_fn(temp) Qjk = q.sum() / m v = (q - Qjk)**2 Vjk = v.sum() / (m - 1) return Qjk, Vjk, (m-1) def conf_int(Q, V, X, dof): tpt = t.ppf(1-(1-X)/2, dof) up = Q + tpt*np.sqrt(V/(dof+1)) down = Q - tpt*np.sqrt(V/(dof+1)) print(round(X*100),&quot;% confidence interval: [&quot;,down,&quot;,&quot;,up,&quot;]&quot;) jk_pi = jackknife_pi(pts, est_pi, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . 95 % confidence interval: [ 3.103649740420917 , 3.187550259579082 ] . We can see that the 95% confidence interval range is fairly large (around 3.10 to 3.19) in this case. We will now show that by using a &quot;better&quot; method we can reduce this range. We start by reconsidering the square-quarter-circle: we notice that we can add 2 additional squares to this setup: . . Apart from looking like a Mondrian painting we can notice that all points generated in the yellow area will add &quot;1&quot; to the estimator array and all points in the black area will add &quot;0&quot;. The only areas of &quot;contention&quot; are the blue/red rectangles, if we focus only in generating points in these areas we will increase the accuracy of the estimator. By symmetry these 2 rectangles are identical, we only need to generate points within the rectangle: $ left { left(1, frac{1}{ sqrt{2}} right), left( 1,0 right), left( frac{1}{ sqrt{2}},0 right), left( frac{1}{ sqrt{2}}, frac{1}{ sqrt{2}} right) right }$. This has the area: $ frac{ sqrt{2}-1}{2}$ or both rectangles together having total area: $ sqrt{2}-1$. Therefore generating $N$ points within these rectangles is equivalent to generating: $ frac{N}{ sqrt{2}-1}$ points in the original scheme (approximately 2.5 times as many). This should reduce the standard deviation of the estimate by about a third (by CLT). We can code this estimator up in a similar way to before: . def points_in_rect(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise This uses the &quot;imporved&quot; method N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) * (1 - 1 / np.sqrt(2)) + (1/np.sqrt(2)) y = np.random.random(N) / np.sqrt(2) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi_2(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_rect This applies a correction since points are only simulated in smaller rectangles &quot;&quot;&quot; pct = arr.sum() / arr.shape[0] approx_pi = (pct*(np.sqrt(2)-1) + 0.5)*4 return approx_pi SIMS = 10000 pts = points_in_rect(10000) print(&quot;Estimate of pi:&quot;, est_pi_2(pts)) jk_pi = jackknife_pi(pts, est_pi_2, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . Estimate of pi: 3.144554915549336 95 % confidence interval: [ 3.1247314678035196 , 3.164378363295143 ] . As expected we can see the confidence interval has decreased significantly through an improved estimator. . Notes on Implementation . As we have seen the jackknife is a general tool. In certain situations other tools exist. There are a couple of points we have to keep in mind when implementing these methods: . The selection of m: this is fairly arbitrary, if the statistic being analysed is very cumbersome to calculate then a smaller choice of m is helpful (or if we wish to run this method over very many statistics). | Properties of the statistic: in some instances we know the statistic must be bounded (e.g. a correlation coefficient must be between $[-1,1]$) This additional information can and should be used to improve the confidence interval. It often becomes more art than science when deciding how to present the results of this method. | . Conclusion . In this post we have seen what a jackknife method is, why it works and a basic implementation. Hopefully now it is obvious the power these methods hold for reporting on the results of a Monte-Carlo simulation. In more sophisticated situations it can really give insight into which parts of a model suffer most from simulation error and also how confident we should be with an estimate it produces. .",
            "url": "https://www.lewiscoleblog.com/jackknife",
            "relUrl": "/jackknife",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Neuron Models 3: Ensembles",
            "content": ". This is the third blog post in a series - you can find the previous blog post here . . Justifying Gaussian White Noise . We first begin with a small diversion, in the previous HH and EIF neuron firing examples we have assumed some sort of Gaussian white noise as an input signal. We briefly mentioned that this is a reasonable assumption but we will justify this in a bit more detail here. First we note that each neuron will typically take input signal from the order of 10,000 neurons. As such even in a low firing rate scheme a neuron will likely receive relatively large amount of input spikes. We can express this signal as: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k)$$ Where: $N$ is the number of connected neurons $J_{i}$ is the synaptic connection strength from neuron $i$ $t_i^k$ is the time of the kth spike recieved from neuron i . If we assume that these spikes arrive in an uncorrelated, memoryless fashion in the form of a Poisson process and that the connection strengths are suitably small: $ langle J_i rangle ll V_{Th} - V_{Re}$ (where angle brackets denote population average). Then we can apply a diffusion approximation: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k) approx mu + sigma xi(t)$$ Where: $ mu = langle J_i rangle N nu $ $ sigma = langle J_i^2 rangle N nu $ $ nu$ is the mean firing rate over all connected neurons $ xi(t)$ is a Gaussian white noise process . Of course as with all approximations this is subject to &quot;small sample size&quot; and $N$ needs to be suitably large. . Fokker-Planck . Recall that we specified the EIF model with Gaussian white noise as having dynamics: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . This is nothing more than an Ito process of the form: $$dX_t = mu(X_t,t)dt + sigma(X_t, t)dW_t $$ With standard Wiener process $W_t$. The Fokker-Planck equation gives us a probability distribution of this process $p(x,t)$ through the PDE: $$ frac{ partial}{ partial t} p(x,t) = - frac{ partial}{ partial x} left[ mu(x,t)p(x,t) right] + frac{ partial^2}{ partial x^2} left[ frac{1}{2} sigma^2(x,t)p(x,t) right] $$ This formula can also be extended to higher dimensions in an obvious way. The derivation of this formula is fairly involved so not included in this blog post, most good textbooks on stochastic analysis should have a derivation for the interested reader. . In the case of the EIF model we can thus write down: $$ frac{ partial p}{ partial t} = frac{ sigma^2}{ tau} frac{ partial^2p}{ partial V_m^2} + frac{ partial}{ partial V_m} left[ frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) right] $$ With $ psi(V_m)$ represnting the exponential firing term. . By the continuity equation we can write: $$ frac{ partial p}{ partial t} = - frac{ partial J}{ partial V_m} $$ . Where $J$ represents the flux. By using this relation in the Fokker-Planck equation and integrating over voltage we get: $$ J(V_m, t) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . We can also note that: $$J(V_{Re}^+,t) = J(V_{Re}^-, t) + r(t)$$ . Where $V_{Re}^ pm$ represents the limit from above (+) or below (-) the reset voltage. the function $r(t)$ represents the average neuron firing rate. This is due to the implementaion of the voltage reset mechanism post spike. We can also note that for $V_m &lt; V_{Re}$ we have $J(V_m, t) = 0$ and for $V_m &gt; V_{Re}$ we have $J(V_m, t) = - r(t)$. We can then solve the flux equation to give: $$P(V_m, t) = frac{r(t) tau}{ sigma^2} int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du $$ . Since the probability measure needs to integrate to 1, we can then write: $$r(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m right)^{-1} $$ . (Note under the scheme presented there is no time dependence to any of these equations. Under time dependent signals we would have to be more careful and typically further approximations are made.) . So far we have not allowed for the refractory period, we have assumed that after reset the voltage trajectories continue as normal. Given we have chosen a deterministic refractory period we can just add this to the euqation above: $$r_{ref}(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m + T_{Ref} right)^{-1} $$ . We can see that this integral will not give rise to an analytic solution in the case of EIF neurons. The forward Euler scheme we relied upon in the past will not perform well here. Instead we will use a slightly different numerical scheme. . Numerical Integration . (This is taken from Richardson [2007] - see references for further details) Presented now is a numerical scheme for calculating the firing rate. Recall from above: $$ J(V_m, t) = - r(t) Theta(V - V_{Re}) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . Where $ Theta(V)$ is the Heaviside step-function. Re-arranged this gives: $$- frac{ partial p}{ partial V_m} = - frac { tau}{ sigma^2} r(t) Theta(V - V_{Re}) + sigma^{-2}(V_m - V_L - psi(V_m)) p(V_m,t) $$ . Which is of the form: $$ frac{ partial p}{ partial V_m} = G(V_m)p(V_m) + H(V_m) $$ . By applying a voltage discretization scheme: $V_k = V_{Lb} + k Delta_V $ with $V_n = V_{Th}$ we can write down: $$ p(V_{k-1}) = p(V_k) e^{ int^{V_k}_{V_{k-1}} G(V)dV} + int^{V_k}_{V_{k-1}} H(V) e^{ int^V_{V_{k-1}}G(U)dU} $$ . We can approximate this as: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V G(V_k)} + Delta_V H(V_k) left( frac{e^{ Delta_V G(V_k)} - 1}{ Delta_V G(V_k)} right) $$ . Substituting back in the necessary formulae for $G$ and $H$ gives: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2}r(t) Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right) $$ . However this still has unknown $r(t)$ in it. If we apply a transform: $q(V,t) = frac{p(V,t)}{r(t)}$ then: $ sum q(V_k) = (r(t))^{-1}$ and: $$ q(V_{k-1}) = q(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2} Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right)$$ . To simplify this expression we define functions $A$ and $B$ so that: $$ q(V_{k-1}) = q(V_k) A(V_k) + Theta(V_k - V_{Re}) B(V_k) $$ . And so we can calculate the firing rate. This scheme has a much better performance than an Euler scheme. We instantiate the scheme with $q(V_n) = 0$, we also select a value $V_{Lb}$ as a cut-off to stop iterating. An implementation of this method can be seen below: . # Evaluating the solution to the Fokker-Planck Equation to calculate the firing rate of an EIF neuron subject to Gaussian white noise import numpy as np # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up additional parameters for solving Fokker-Planck. DelV (mV) and VLb (mV) DelV = 0.001 VLb = -100 Steps = int(np.ceil((Vth - VLb) / DelV)) q = np.zeros(Steps) V = np.arange(Steps)*DelV + VLb # For ease define function psi def psi(V): return DelT * np.exp((V - VT) / DelT) def A(V): return np.exp(DelV * sig**-2 *(V - VL - psi(V))) def B(V): if A(V) == 1.0: return DelV * tau * sig**-2 else: return DelV * tau * sig**-2 * (A(V) - 1) / np.log(A(V)) # Shut off numpy divide errors np.seterr(divide=&#39;ignore&#39;) for i in range(Steps -1, 0, -1): if V[i] &gt; Vr: q[i-1] = q[i]*A(V[i]) + B(V[i]) else: q[i-1] = q[i]*A(V[i]) r = 1/(q.sum()/1000000 + Tref/1000) print(&quot;Firing rate:&quot;, round(r,1), &quot;Hz&quot;) . Firing rate: 21.6 Hz . We can modify the previous EIF firing code to estimate the firing rate, the results should be similar (note: for this I used 10m time steps, it is a slow running code!): . #collapse # Implementation of a noisy EIF neuron using a forward Euler scheme # Reduce N for quicker running code import numpy as np # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 10000000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up voltage Vold (mV) and spike count Sp Vold = Vr Sp = 0 # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: Vnew = Vr Tc -= 1 else: Vtemp = Vold + dt/tau*(VL - Vold) + DelT*dt/tau*np.exp((Vold - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: Vnew = Vr Tc = np.ceil(Tref/dt) Sp += 1 else: Vnew = Vtemp Vold = Vnew print(&quot;Estimated firing rate:&quot;, round(Sp/(N*dt/1000),1), &quot;Hz&quot;) . . Estimated firing rate: 20.2 Hz . Which we can see is similar to the solution of the Fokker-Planck equation. In the limit $N to infty$ and decreasing the lattice sizes these approximations should become much closer. . Conclusion . We have seen that by using the Fokker-Planck framework we are able to calculate the mean firing rate of the EIF neuron. We can also notice that the numerical scheme to integrate the Fokker-Planck runs significantly faster than taking a Monte-Carlo approximation by simulating the EIF directly. We can also notice that the Fokker-Planck framework is easy to extend (e.g. to modulated noise or other applied signals) and further we can extend this to allow for connected networks of neurons (I may write an additional blog post on this in the future but will likely end up being quite similar to this one). . References . https://neuronaldynamics.epfl.ch/online/Ch13.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | How Spike Generation Mechanisms Determine the Neuronal Response to Fluctuating Inputs - Nicolas Fourcaud-Trocme´, David Hansel, Carl van Vreeswijk, and Nicolas Brunel [2003] | Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based synaptic drive - Magnus J Richardson [2007] | .",
            "url": "https://www.lewiscoleblog.com/neuron-models-3",
            "relUrl": "/neuron-models-3",
            "date": " • Jan 21, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Neuron Models 2: Exponential Integrate and Fire Model",
            "content": ". This is the second blog post in a series - you can find the previous blog post here . . Integrate and Fire Models . Throughout this blog post we will focus on integrate and fire models. This class of model has been around for a long time, in fact longer than the Hodgkin-Huxley model. The first model was presented by Lapicque in 1907. Since then many alternative formulations have been presented. We can express the models in the form: $$ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $$ . Where: $ tau$ represents membrane time constant ($C_m / g_L$ in notation used previously) $E$ represents the rest potential $ psi(V)$ is the spike generating current term $R_m$ represents membrane resistance ($1/g_L$) $I(t)$ is a function representing an applied current (in the Hodgkin-Huxley example we took a Gaussian white noise) . With integrate and fire models we have the issue that (typically) the action potential will shoot off to infinity. In order to stop this we implement a threshold ($V_{Th}$), when the process reaches this value it is reset (to $V_{Re}$) and the dynamics start again. This is not a big issue because for our purposes we are noly interested in the dynamics of an onset of an action potential, the mechanism of returning to normal levels is not of (much) interest. When modelling we may wish to hold the voltage at $V_{Re}$ following a spike for a short time ($ Delta_{T_{Rf}}$) to reflect the refractory period of a neuron. The refractory period can be modelled stochastically but usually a static value is succficient. . Integrate and fire models are thus defined by the form of the function $ psi(V)$ - this function may be linear or non-linear. Examples include the leaky-integrate and fire model (linear), Fitzhugh-Nagumo (polynomial) and the exponential integrate and fire (non-linear). . From Hodgkin-Huxley to Integrate and Fire . We now take a quick de-tour to justify the use of the integrate and fire model as an approximation to the Hodgkin-Huxley dynamics. . First we notice that gate m operates on a much faster time scale than gates n or h (and similarly much faster than the leak channel which controls the potential dynamics with all gates closed.) Given it is so much faster we can apply an instantaneous approximation, namely: $m(t) = hat{m}(V_m(t))$ that is: the dynamics are defined by the membrane voltage. From plotting gate dynamics we can also observe that gates n and h are approximately translated reflections of each other. As an approximation we can create an adaptation variable $w$ with $n = aw$ and $h = b - w$ for constants $a, b$. We can then write down the equation: $$ C_m frac{dV_m}{dt} = I_p - overline{g_K}(aw)^4(V_m-V_K) - overline{g_{Na}}( hat{m}(V_m))^3(b - w)(V_m-V_{Na}) - overline{g_L}(V_m-V_L) $$ . There is a corresponding equation for the adaptation variable $w$ which we shall not concern ourselves with. We are only interested in the onset of spiking not the refractory period dynamics so we will take $w = w_{rest}$ to be a fixed value. We can therefore express the voltage dynamics as: $$ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) - lambda ( hat{m}(V_m))^3(V_m-V_{Na}) $$ . By collecting terms and some re-arrangement. $g_{eff}, V_{eff}$ and $ lambda$ are all constant values. Therefore via the approximations outlined above we are left with an equation of the form: $$ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $$ . Namely an integrate and fire model. . Exponential Integrate and Fire . Continuing with the line of reasoning above we shall consider the function: $ hat{m}(V_m)$. We assume the dynamics are so rapid that they are essentially in equilibrium: $$ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m = 0 $$ So: $$ hat{m}(V_m) = frac{ alpha_m(V_m)}{ alpha_m(V_m) + beta_m(V_m)}$$ . Since Hodgkin-Huxley suggested the following forms of these equations: $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ . We can approximate $ hat{m}(V_m)$ with a logistic function: $$ hat{m}(V_m) approx (1 + e^{- beta(V_m - theta)})^{-1}$$ We can then express the Taylor expansion of this as: $$ hat{m}(V_m) = sum (-1)^k e^{ beta(V_m - theta)(1+k)}$$ Which we can see from the expansion of $1/(1+y)$ with $y = e^{- beta(V_m - theta)}$. So a first order approximation is: $$ hat{m}(V_m) approx e^{ beta(V_m - theta)}$$ . Then the current corresponding to the sodium channel can be expressed approximately: $$I_{Na} = g_{Na}(b - w_{rest})(V_m - V_{Na}) e^{3 beta(V_m - theta)} $$ . If we take $(V_m - V_{Na}) approx (V_{rest} - V_{Na}) &lt; 0$ as an approximation we get approximate voltage dynamics as: $$ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) + hat{ lambda} e^{ beta (V_m - theta)} $$ . This is known as the exponential integrate and fire (EIF) model.This model has been shown to fit experimental data (and the Hodgkin-Huxley model) very well in practice. Typically we use a parameterization of the spiking term: $$ psi(V) = Delta_T e^{ left( frac{V - V_T}{ Delta_T} right)} $$ We can see this model is highly non-linear and without applying the threshold mechanics the membrane potential would shoot to infinity. The 2 new parameters in this model are: $V_T$ which represents the voltage scale at which the exponential term becomes significant in the dynamics $ Delta_T$ representing the sharpness of the spike . With a Gaussian white noise term $ xi_t$ we can fully specify the dynamics using: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . As before we can solve this using a foward Euler scheme (although the forcing term is non-linear this still provides a reasonable solution for small enough time steps.) An implementation of this can be seen below: . # Implementation of a noisy EIF neuron using a forward Euler scheme import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 50000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up arrays for time T (ms) and voltage V (mV) T = np.arange(N) * dt V = np.zeros(N) V[0] = Vr # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: V[i] = Vr Tc -= 1 else: Vtemp = V[i-1] + dt/tau*(VL - V[i-1]) + DelT*dt/tau*np.exp((V[i-1] - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: V[i] = Vr Tc = np.ceil(Tref/dt) else: V[i] = Vtemp # Plot voltage trajectory plt.plot(T, V) plt.xlabel(&quot;Time (ms)&quot;) plt.ylabel(&quot;Membrane Potential (mV)&quot;) plt.title(&quot;Membrane Voltage Trajectory&quot;) plt.show() . Conclusion . The voltage trajectory displays realistic neuron dynamics for the onset of spiking. However as expected through the use of the refractory period implementation the depolarizing phase is not captured well. This is ok since we consider the information to be carried by the spike itself not the behaviour shortly afterwards. We can also see that this implementation is considerably simpler than that of Hodgkin-Huxely since there is only one ODE. This is of benefit when modelling large networks of neurons where time/computational constraints become a consideration. . It has been shown that the EIF model can predict spiking behaviour very well in practice, despite the somewhat cavalier assumptions made during its derivation from the Hodgkin-Huxley. . In a future blog post we will make use of the mathematical tractability of the EIF model to analyse neurons in more depth. . References . https://neuronaldynamics.epfl.ch/online/Ch5.S2.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | . . This blog post is the second part of a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/neuron-models-2",
            "relUrl": "/neuron-models-2",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Neuron Firing Models: Hodgkin-Huxley Model",
            "content": "Some (very basic) Biology . We start with a very limited description of what a neuron is and the mechanism by which it fires. For sake of completeness by neuron we will refer to pyramidal neurons, they make up a large proportion of neurons within the cortex of mammals. Other types of neurons are specialised for different functions. This background will be very brief and not cover the biology in any great detail. The literature and data on this topic is vast and I will not do it justice so any interested readers should look towards biology textbooks for more detailed descriptions. . The neuron is made up of 3 main components: a soma (cell body), an axon and many dendrites. In laymans terms the axon is the &quot;output&quot; of the neuron while the dendrites are &quot;inputs&quot; to the soma. The axon is covered in a myelin sheath which acts &quot;insulation&quot; which can &quot;speed up&quot; signal flow. Dendrites can futher be broken down into the apical dendrite, basal dendrites and dendritic spines. Both dendrites and axons are highly branched and a single neuron can be linked to many 1000s of others. Signals are passed through synapses. Synaptic inputs can be either excitatory (making the target neuron more likely to fire) or inhibitory (less likely). . . The details of axons and dendrites are not that important for our purposes right now. Instead we are interested in the soma: essentially where the signal is generated. The cell wall contains many voltage gated ion channels, most notably: sodium (Na+) and potassium (K+) channels. There are 2 forces acting on ions inside/outside of the soma: namely the electrical potential within the body and the concentration gradient. Resting membrane potential is around -40mv to -90mv, in this state there is a higher concentration of potassium ions inside the cell than outside and the reverse for sodium ions. The channel for potassium is highly permeable and so potassium ions flow into the soma, the sodium channel is semi-permeable so sodium slowly flow out of the soma, to maintain a constant negative potential the cell &quot;pumps&quot; ions in the reverse direction to maintain equilibrium. . An action potential (neuron spike) is a shift away from the negative equilibrium membrane potential to a positive potential. This is as a result of ion flows due to voltage controlled gates. There are 3 gates of interest here: . Sodium activation gate - normally closed but opens with positive potential | Sodium inactivation gate - normally open but closes with positive potential | Potassium inactivation gate - normally closed but opens with large positive potential | We can break down the events that cause the action potential then as: . Depolarisation - A depolarisation event occurs (e.g. a signal from the dendrites) which brings the cell&#39;s membrane potential to 0mv. This is as a result of positive charged ions flowing into the cell. As the potential increases to some threshold the sodium activation gate is opened which allows more positively charged sodium ions to enter. The potential then becomes positive. | Repolarisation - The positive potential causes the sodium inactivation gate to close preventing more sodium ions entering. Meanwhile the potassium inactivation gate opens, since the concentration of potassium ions inside the cell is much greater than outside this leads to an outflow of potassium. The removal of positively charged ions moves the cell back towards equilibrium. | Refractory Period - The potassium channel stays open slightly past the equilibrium point and the membrane potential becomes too negative (hyperpolarises). As the potassium channel closes the potential tends back to equilibrium. There is a short period after an action potential where the neuron is unable to fire again. | (Source: https://teachmephysiology.com/wp-content/uploads/2018/08/action-potential.png) . In the rest of this blog we will look at models of action potentials and will try and keep this real world description of action potentials in mind. . Hodgkin Huxley Model . We now consider a spiking neuron model as presented by Alan Hodgkin and Andrew Huxley in 1952, this model won them the Nobel prize for physiology in 1963. The model was developed by studying the axon of a giant squid neuron. The model itself tries to mimic the gates and ionic channels described above. Diagramatically we can represent the model as: . The voltage controlled ionic gates conductances are represented in the diagram by gn (only one represented in the diagram) and there is a leak conductance represented by gl. Cm represents the membrane capacitance and there is an external stimulus Ip which represents inputs from other neurons (or a test current applied by a probe in experiment). We can then represent the model as a set of 4 interacting PDEs: $ frac{dV_m}{dt} = frac{I_p}{C_m} - frac{ overline{g_K}n^4}{C_m}(V_m-V_K) - frac{ overline{g_{Na}}m^3h}{C_m}(V_m-V_{Na}) - frac{ overline{g_L}}{C_m}(V_m-V_L) $ $ frac{dn}{dt} = alpha_n(V_m)(1-n) - beta_n(V_m)n $ $ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m $ $ frac{dh}{dt} = alpha_h(V_m)(1-h) - beta_h(V_m)h $ . Where the functions $ alpha_x , beta_x$ as suggested by Hodgkin and Huxley are: $ alpha_n(V_m) = frac{0.01(10-V_m)}{e^{(1.0-0.1V_m)}-1} $ $ beta_n(V_m) = 0.125 e^{-V_m / 80} $ $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ $ alpha_h(V_m) = 0.07 e^{-V_m / 20} $ $ beta_h(V_m) = frac{1}{e^{(3.0-0.1V_m)}+1} $ . The following values are suggested for the model constants: $C_m = 1 mu F /cm^2$ - Capacitance per unit surface area of neuron membrane $ overline{g_{Na}} = 120 mu S / cm^2$ - Voltage controlled sodium conductance per unit surface area $ overline{g_{K}} = 36 mu S / cm^2$ - Voltage controlled potassium conductance per unit surface area $ overline{g_{L}} = 0.336 mu S / cm^2$ - Voltage controlled leak conductance per unit surface area $V_{Na} = 115mV$ - Sodium voltage gradient $V_{K} = -12mV$ - Potassium voltage gradient $V_{L} = 10.613mV$ - Leak current voltage gradient . This is a non-linear system of differential equations and as such it is not possible to study analytically. However we can simulate this numerically. For this example we will assume that the external stimulus follows a white noise (Brownian motion). Under certain conditions this can be a reasonable assumption. We will define this as follows: $I_p(t) = sigma W_t$ for some positive constant $ sigma$ with units $ mu A / cm^2$ . By introducing stochastic noise such as this we unfortunately are unable to use any of the Python pre-made ODE integrators (e.g. scipy.integrate.ode) instead we will rely on a forward Euler scheme which will perform well enough for our purposes assuming the discrete time steps are selected to be sufficiently small. An example implementation of this can be seen below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed np.random.seed(123) # Set time step dt (ms) and number of steps for simulation N dt = 0.001 N = 50000 # Set up time array T T = np.arange(N) * dt # Set model inputs, using nomenclature: K - Potassium, Na - Sodium, L - Leak # Set membrane capacitance per unit area (uF/cm^2) Cm = 1.0 # Set applied current density volatility (uA/cm^2) sigma = 2 # Set channel conductance per unit area (mS/cm^2) gK = 36.0 gNa = 120.0 gL = 0.3 # Set voltage gradients (mV) VK = -12.0 VNa = 115.0 VL = 10.613 # Define ion channel rate functions def alpha_n(Vm): return (0.01 * (10.0 - Vm)) / (np.exp(1.0 - (0.1 * Vm)) - 1.0) def beta_n(Vm): return 0.125 * np.exp(-Vm / 80.0) def alpha_m(Vm): return (0.1 * (25.0 - Vm)) / (np.exp(2.5 - (0.1 * Vm)) - 1.0) def beta_m(Vm): return 4.0 * np.exp(-Vm / 18.0) def alpha_h(Vm): return 0.07 * np.exp(-Vm / 20.0) def beta_h(Vm): return 1.0 / (np.exp(3.0 - (0.1 * Vm)) + 1.0) # Define applied signal function - can be replaced to investigate different signals def Ip(sig, t, V): return np.sqrt(dt) * sig * np.random.normal(0, 1, 1) # Set up arrays for dynamic results Vm = np.zeros(N) n = np.zeros(N) m = np.zeros(N) h = np.zeros(N) Signal = np.zeros(N) # Initialize the system V0 = 0 Vm[0] = V0 n[0] = alpha_n(V0) / (alpha_n(V0) + beta_n(V0)) m[0] = alpha_m(V0) / (alpha_m(V0) + beta_m(V0)) h[0] = alpha_h(V0) / (alpha_h(V0) + beta_h(V0)) # Loop through Euler-Forward scheme for i in range(1, N): Signal[i] = Ip(sigma, i*dt, Vm[i-1])/Cm Vm[i] = Vm[i-1] + Signal[i] - gK*np.power(n[i-1],4)*(Vm[i-1]-VK)*dt/Cm - gNa*np.power(m[i-1],3)*h[i-1]*(Vm[i-1]-VNa)*dt/Cm - gL*(Vm[i-1]-VL)*dt/Cm n[i] = n[i-1] + (alpha_n(Vm[i-1])*(1 - n[i-1]) - beta_n(Vm[i-1])*n[i-1])*dt m[i] = m[i-1] + (alpha_m(Vm[i-1])*(1 - m[i-1]) - beta_m(Vm[i-1])*m[i-1])*dt h[i] = h[i-1] + (alpha_h(Vm[i-1])*(1 - h[i-1]) - beta_h(Vm[i-1])*h[i-1])*dt # Plot sample path plt.plot(T, Vm) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Membrane voltage mV&quot;) plt.title(&quot;Hodgkin-Huxley Spiking&quot;) plt.show() # Plot gate opening over time plt.plot(T, n, label=&quot;n - K&quot;) plt.plot(T, m, label=&quot;m - Na&quot;) plt.plot(T, h, label=&quot;h - Na&quot;) plt.legend(loc=&quot;upper right&quot;) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Gate Proportion&quot;) plt.title(&quot;Gate Dynamics&quot;) plt.show() # Plot Limit trajectories plt.plot(n, Vm, label=&quot;n-Vm&quot;) plt.plot(m, Vm, label=&quot;m-Vm&quot;) plt.plot(h,Vm, label=&quot;h-Vm&quot;) plt.legend() plt.title(&quot;Cycle Trajectories&quot;) plt.xlabel(&quot;Gate Proportion&quot;) plt.ylabel(&quot;Membrane Voltage mV&quot;) plt.show() . Conclusion . In this blog post we have seen some of the basic biolgical and physical processes behind generating an action potential. Through the Hodgkin-Huxley model we have looked at dynamics of 3 voltage controlled ionic gates which control the membrane potential and neuron spiking. We can see that this model is fairly complicated, if we wanted to model many neurons it can become problematic. In a future blog post we will look at ways to simplify this model. . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/neuron-models-1",
            "relUrl": "/neuron-models-1",
            "date": " • Jan 7, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Insurance Aggregation Model",
            "content": "Note: Thoughts epressed within this workbook are my own and do not represent any prior, current nor future employers or affiliates . Background - An Overview of Modelling in Specialty Insurance . Within the specialty insurance space we are typically insuring economic interests of relatively rare events of high impact (for example: buildings damaged by hurricanes, aircraft crashes, impact of CEO wrong-doing, and so on.) These events are typically broken up into 2 broad classes: . Property | Casualty | Hence why the term &quot;P&amp;C&quot; insurer is sometimes used. Property risks are, as the name suggests, related to property - historically phyiscal property but now can include non-physical property (e.g. data). Owing to the relative simplicity of these risks there is an entire universe of quantitative models that exist for risk management purposes, in particular there are a handful of vendors that create &quot;natural catastrophe&quot; (nat-cat) models. These models are sophisticated and all essentially rely on GIS style modelling: a portfolio of insured risks are placed on a geographical map (using lat-long co-ordinates) then &quot;storm tracks&quot; representing possible hurricane paths are run through the portfolio resulting in a statistical distribution of loss estimates. For other threats such as earthquakes, typhoons and wild-fires similar methods are used. . These nat-cat models allow for fairly detailed risk management procedures. For example it allows insurers to look for &quot;hot spots&quot; of exposure and can then allow for a reduction in exposure growth in these areas. They allow for counter-factual analysis: what would happen if the hurricane from last year took a slightly different track? It allows insurers to consider marginal impacts of certain portfolios, for example: what if we take on a portfolio a competitor is giving up, with our current portfolio will it aggregate or diversify? As a result of this explanatory power natural catastrophe risks are now well understood and for all intents and purposes these risks are now commodified and have allowed insurance linked securities (ILS) to form. &lt;/br&gt; . Before this analytics boom specialty insurers made their money in natural catastrophe and property insurance, as such there has been a massive growth in recent years in the Casualty side of the business. Unfortunately the state of modelling on that side is, to put it politely, not quite at the same level. . As one would expect nat-cat model vendors have tried, and continue to try, to force the casualty business into their existing natural catastrophe models. This is a recipe for disaster as the network structure for something like the economy does not naturally lend itself to a geogprahic spatial representation. There is also a big problem of available data. Physical property risks give rise to data that is easy to cultivate. Casualty data is either hard to find or impossible - why would any corporation want to divulge all the details of their interactions? As such it does not appear that these approaches will become useful tools in this space. . To fill this void there has been an increasing movement of actuaries into casualty risk modelling roles. While this overcomes some of the problems that face the nat-cat models they also introduce a whole new set of issues. Traditional actuarial models relying on statistical curve fitting to macro-level data. Even assuming a suitable distribution function can be constructed it is of limited use for risk management as it only informs them of the &quot;what&quot; but not the &quot;why&quot;, making it hard to orient a portfolio for a specific result. More recently actuaries have slowly began to model individual deals at a micro-level and aggregate them to get a portfolio view. To do this a &quot;correlation matrix&quot; is typically employed, this aproach also has issues: . Methods don&#39;t scale well with size, adding new risks often require the entire model to be recalibrated taking time and effort. | They either require a lot of parameters or unable to capture multi-factor dependency (e.g. a double trigger policy where each trigger has its own sources of accumulation). | It is usually not possible to vary the nature of dependency (e.g. add tail dependence or non-central dependency) | Results are often meaningless in the real world, it is usually impossible to perform counter-factual analysis | To bridge this gap I have developed a modelling framework that allows for the following: . Modelling occurs at an individual insured interest level | Modelling is scalable in the sense that adding new insured interests requires relatively few new parameters and calibrations | Counter-factual analysis is possible and the model can be interpreted in terms of the real world | The framework itself is highly parallelizable, whereas nat-cat models require teams of analysts, large servers and IT infrastructure this framework lends itself to being run by multiple people on regular desktop computers with little additional workflow requirements | A First Step: A Simple Driver Method . We will now look at a very stylised model of aggregation that will form a foundation on which we can build the more sophisticated model framework. We call this method of applying dependence a &quot;driver method&quot;, it is standard practice for applying dependence in banking credit risk models where there can be many thousands of risks modelled within a portfolio. The interpretation is that there is a central &quot;driver&quot;, each individual risk is &quot;driven&quot; by this and since this is common to all risks there is an induced dependence relation between them. . The model relies on the generalised inverse transform method of generating random variates. Stated very simply: if you apply the inverse CDF of a random variable to a random number (U[0,1] variate) you will have samples distributed as that random variable. Therefore in order to apply dependence in a general form we only need to apply dependence between U[0,1] variates. We will also exploit the fact that normal distributions are closed under addition (that is the sum of normals is normal). . We can now express the model as follows: . We sample standard normal (N(0,1)) variates to represent the &quot;driver&quot; variable | For each risk sample an additional set of normal variates | Take a weighted sum of the &quot;driver&quot; and the additional normal variates to give a new (dependent) normal variate | Standardise the result from step 3) and convert to a U[0,1] variable using the standard gaussian CDF | Use an inverse transform to convert the result of step 4) to a variate as specified by the risk model | We can see that this method is completely general, it does not depend on any assumption about the stand-alone risk model distributions (it is a &quot;copula&quot; method). Another observation is that the normal variates here are in some sense &quot;synthetic&quot; and simply a tool for applying the dependence. . For clarity an example is presented below: . # Simple driver method example # We model a central driver Z # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates X1 and X2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate temporary synthetic variable X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Use normal CDF to convert X synthetic variables to uniforms U U1 = norm.cdf(X1) U2 = norm.cdf(X2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.4628059800990357 . The example above shows we have correlated gamma variates with around a 50% correlation coefficient (in this case we could calculate the correlation coefficient analytically but it is not necessary for our purposes, as we create more sophisticated models the analytic solutions become more difficult/impossible). . Even from this example we can see how models of this form provide superior scalability: for each additional variable we only need to specify 1 parameter: the weight given to the central driver. In contrast a &quot;matrix&quot; method requires each pair-wise combination to be specified (and then we require a procedure to convert the matrix to positive semi-definite form in order to apply it). Say our model requires something more sophisticated: say the sum of a correlated gamma and a weibull distribution - the number of parameters in a matrix representation grows very quickly. However it is worth noting we do lose some control, by reducing the number of parameters in this way we lose the ability to express every possible correlation network. However in most cases this is not a big problem as there is insufficient data to estimate the correlation matrix anyway. . It is worth pointing out that the type of dependency applied here is a &quot;rank normal&quot; dependency - this is the same dependency structure as in a multi-variate normal distribution, albeit generalised to any marginal distribution. . An Extension to the Simple Driver Method . We can extend the model above by noticing the following: there is nothing stopping the &quot;synthetic&quot; variables being considered drivers in their own right. Gaussians being closed under addition does not require that each variable needs to be independent, sums of rank correlated normals are still normal! We can thus extend the model to: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Use normal CDF to convert sX synthetic variables to uniforms U U1 = norm.cdf(sX1) U2 = norm.cdf(sX2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7851999480298125 . As before we have ended up with rank-normal correlated gamma variates. This time we have 3 potential &quot;driver&quot; variables Z, X1, X2 - all correlated with each other. It is not hard to see how this procedure can be iterated repeatedly to give arbitrarily many correlated driver variables. Further we can imagine these variables being oriented in a hierarchy, Z being at the bottom layer, X1 and X2 being a layer above, and so on. . What is a Driver? . We should now take a step back and think about the implications for the insurance aggregation problem. As stated previously this method allows us to define dependency with far fewer parameters than using a matrix approach. When you start getting into the realms of 100,000s of modelled variables this becomes increasingly important from a calibration perspective. . However there are other benefits: for example we can look at how the model variables relate to the driver variables. For example we can ask questions such as: &quot;What is the distribution of modelled variables when driver Z is above the 75th percentile&quot; and so on. This is a form of counter-factual analysis that can be performed using the model, with the matrix approaches you get no such ability. For counter-factual analysis to be useful however we require real-world interpretations of the drivers themselves. By limiting ourselves to counter-factual analysis based on driver percentiles (e.g. after the normal cdf is applied to Z, X1, X2 - leading to uniformly distributed driver variables) we make no assumption about the distribution about the driver itself, only its relationship with other drivers. . By not making a distributional assumption a driver can represent any stochastic process. This is an important but subtle point. For example we could create a driver for &quot;global economy&quot; (Z) and by taking weighted sums of these create new drivers &quot;US economy&quot; (X1) and &quot;european economy&quot; (X2). In this example there may be data driven calibrations for suitable weights to select (e.g. using GDP figures) however it is also relatively easy to use expert judgement. In my experience it is actually easier to elicit parameters in this style of model compared to &quot;correlation&quot; parameters given this natural interpretation. . Given this natural interpretation we can quite easily begin to answer questions such as: &quot;What might happen to the insurance portfolio in the case of a european economic downturn?&quot; and so on. Clearly the detail level of the driver structure controls what sort of questions can be answered. . As stated previously we can repeat the mechanics of creating drivers to create new &quot;levels&quot; of drivers (e.g. moving from &quot;european economy&quot; to &quot;French economy&quot;, &quot;UK economy&quot; and so on). We can also create multiple &quot;families&quot; of driver, for example in addition to looking at economies we may consider a family relating to &quot;political unrest&quot;, again this could be broken down into region then country and so on. Other driver families may not have a geographic interpretation - for example commodity prices. In some cases the families may be completely independent of each other, in other cases they can depend on each other (e.g. commodity prices will have some relationship with the economy). . In the examples so far we have presented a &quot;top down&quot; implementation in our examples: we start by modelling a global phenomena and then build &quot;smaller&quot; phenomena out of these. There is nothing special about this, we could have just as easily presented a &quot;bottom up&quot; implementation: take a number of &quot;Z&quot; variables to represent regions and combine these to form an &quot;X&quot; representing a global variable. Neither implementation is necessarily better than another and mathematically they lead to equivalent behaviours (through proper calibration). In practice however I have found the &quot;top down&quot; approach works better, typically you will start with a simple model and through time it can iterate and become more sophisticated. The top down approach makes it easier to create &quot;backward compatability&quot; which is a very useful feature for any modelling framework (e.g. suppose the first iteration of the framework only considers economic regions, next time a model is added which requires country splits - with top down adding new country variables keeps the economic regions identical without requiring any addtional thought.) . The need for more Sophistication: Tail Dependence . Unfortunately the model presented so far is still quite a way from being useful. We may have found a way of calibrating a joint distribution using relatively few (O(N)) parameters and can (in some sense) perform counter-factual analysis, but there is still a big issue. . So far the method only allows for rank-normal joint behaviour. From the analysis of complex systems we know that this is not necessarily a good assumption (please see other blog posts for details). We are particularly interested in &quot;tail dependence&quot;, in layman&#39;s terms: &quot;when things go bad, they go bad together&quot;. Tail dependence can arise for any number of reasons: . Structual changes in the system | Feedback | State space reduction | Multiplicative processes | Herd mentality/other human behaviours | And many others | . Given the framework we are working within we are not particularly interested in how these effects occur, we are just interested in replicating the behaviour. . To do this we will extend the framework to cover a multivariate-student-T dependence structure. To do this we note the following: $$ T_{ nu} sim frac{Z} { sqrt{ frac{ chi^2_{ nu}} { nu}}} $$ Where: $ T_{ nu} $ follows a student-t distribution with $ nu$ degrees of freedom $ Z $ follows a standard normal $N(0,1)$ $ chi^2_{ nu} $ follows Chi-Square with $ nu$ degrees of freedom . Therefore we can easily extend the model to allow for tail dependence. . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu = 3 Chi = chi2.rvs(nu, size=SIMS) sX1 /= np.sqrt(Chi / nu) sX2 /= np.sqrt(Chi / nu) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu) U2 = t.cdf(sX2, df=nu) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7907911109201866 . Adding Flexibility . We can further extend this model by allowing each model variate to have its own tail-dependence. Why is this important one might ask? In the case of this framework we are spanning many different models, selecting a single degree of tail dependence might not be suitable for all variables. We can do this via applying another inverse transform: $$ T_{ nu} sim frac{Z} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ As before but where: $U$ follows a uniform U[0,1] distribution $F^{-1}_{ chi^2_{ nu}}$ is the inverse cdf of $ chi^2_{ nu}$ . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 U = np.random.rand(SIMS) Chi1 = chi2.ppf(U,df=nu1) Chi2 = chi2.ppf(U, df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7703228652641819 . There is a small practical issue relating to multivariate student-t distributions: namely that we lose the ability to assume independence. This is a direct result of allowing for tail dependence. In many situations this is not an issue, however within this framework we have models covering very disperate processes some of which may genuinely exhibit independence. To illustrate this issue we will re-run the existing model with zero driver weights (&quot;attempt to model independence&quot;): . Estimated Pearson Correlation Coefficient: 0.08220534833363176 . As we can see there is a dependence between Y1 and Y2 0 clearly through the chi-square variates. We can overcome this issue by &quot;copying&quot; the driver process. The common uniform distribution is then replaced a number of correlated uniform distributions. We can then allow for independence. An implemntation of this can be seen in the code sample below: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7406745557389065 . To show this allows full independence we repeat the zero-weight example: . Estimated Pearson Correlation Coefficient: -0.01456173215652803 . We can see that this is a much better scatter plot if we are looking for independence! . Non-Centrality . We now extend this model yet further. So far we have allowed for tail dependence however it treats both tails equally. In some instances this can be problematic. For example if we rely on output from the framework to do any kind of risk-reward comparison the upisde and downside behaviour are both important. While it is easy to think of structural changes leading to a downside tail dependence an upside tail dependence is typically harder to justify. We can allow for this with a simple change to the model, namely: $$ T_{ nu, mu} sim frac{Z + mu} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ The addition of the $ mu$ parameter means that $T_{ nu, mu}$ follows non-central student-t distribution with $ nu$ degrees of freedom and non-centrality $ mu$. Details of this distribution can be found on wikipedia. By selecting large positive values of $ mu$ we can create tail dependence in the higher percentiles, large negative values can create tail dependence in the lower percentiles and a zero value leads to a symmetrical dependency. Adjusting the code futher we get: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers # We add non-centrality through an additive scalar import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Specify the non-centrality values nc1 = -2 nc2 = -2 # Use non-central t CDF to convert sX synthetic variables to uniforms U U1 = nct.cdf(sX1+nc1, nc=nc1, df=nu1) U2 = nct.cdf(sX2+nc2, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7100911602838634 . In the code example we have selected a non-centrality of -2 which is a fairly large negative value, we can see the dependency increasing in the lower percentiles (clustering around (0,0) on the plot). . Temporal Considerations . So far we have essentially considered a &quot;static&quot; model, we have modelled a number of drivers which represent values at a specific time period. For the majority of insurance contracts this is sufficient: we are only interested in losses occuring over the time period the contract is active. However in some instances the contracts relate to multiple time periods and it does not make sense to consider losses over the entire lifetime. Moreover it is not ideal to model time periods as independent from one another, to take the US economy example: if in 2020 the US enters recession it is (arguably) more likely that the US will also stay in recession in 2021. Clearly the dynamics of this are very complex and constructing a detailed temporal model is very difficult, however for the sake of creating the drivers we do not need to know the exact workings. Instead we are looking for a simple implementation that gives dynamics that are somewhat justifiable. . Fortunately it is relatively easy to add this functionality to the model framework we have described so far. Essentially we will adopt a Markovian assumption whereby a driver in time period t+1 is a weighted sum of its value at time t and an idiosyncratic component. Of course this is not a perfect description of the temporal behaviour of every possible driver but it shouldn&#39;t be completely unjustifiable in most instances and the trajectories shouldn&#39;t appear to be totally alien (e.g. US economy being in the top 1% one year immediately followed by a bottom 1% performance very frequently). . To illustrate this please see the code example below, for brevity I will change the model code above to a functional definition to avoid repeating blocks of code. . # Creating temporally dependent variables import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Define function to create correlated normal distributions def corr_driver(): # Create driver Z Z = np.random.normal(0, 1, SIMS) # Create drivers X1, X2 X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) return np.array([X1, X2]) # Create drivers variables for time periods t0 and t1 driver_t0 = corr_driver() driver_t1 = 0.5 * driver_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Create copy of drivers for tail process time periods t0 and t1 tail_t0 = corr_driver() tail_t1 = 0.5 * tail_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Define a standardise function def standardise(x): return (x - x.mean()) / x.std() # Create sythetic variables sX1 sX2 for variable 1 and 2 at times t0 and t1 # Note depending on the model idiosyncratic components may also be dependent sX1t0 = standardise(0.25*driver_t0[0] + 0.5*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1t1 = standardise(0.25*driver_t1[0] + 0.5*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t0 = standardise(0.5*driver_t0[0] + 0.25*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t1 = standardise(0.5*driver_t1[0] + 0.25*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Repeat synthetic variable construction for tail process sX1tailt0 = standardise(0.25*tail_t0[0] + 0.5*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1tailt1 = standardise(0.25*tail_t1[0] + 0.5*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt0 = standardise(0.5*tail_t0[0] + 0.25*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt1 = standardise(0.5*tail_t1[0] + 0.25*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Simulate Chi-Square for tail-dependence t0 and t1 nu1 = 2 nu2 = 4 Chi1t0 = chi2.ppf(norm.cdf(sX1tailt0),df=nu1) Chi2t0 = chi2.ppf(norm.cdf(sX2tailt0), df=nu2) sX1t0 /= np.sqrt(Chi1t0 / nu1) sX2t0 /= np.sqrt(Chi2t0 / nu2) Chi1t1 = chi2.ppf(norm.cdf(sX1tailt1),df=nu1) Chi2t1 = chi2.ppf(norm.cdf(sX2tailt1), df=nu2) sX1t1 /= np.sqrt(Chi1t1 / nu1) sX2t1 /= np.sqrt(Chi2t1 / nu2) # Specify the non-centrality values nc1 = 2 nc2 = 2 # Use non-central t CDF to convert sX synthetic variables to uniforms U for t0 and t1 U1t0 = nct.cdf(sX1t0+nc1, nc=nc1, df=nu1) U2t0 = nct.cdf(sX2t0+nc2, nc=nc2, df=nu2) U1t1 = nct.cdf(sX1t1+nc1, nc=nc1, df=nu1) U2t1 = nct.cdf(sX2t1+nc1, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 at t0 and t1 Y1t0 = gamma.ppf(U1t0, 2) Y2t0 = gamma.ppf(U2t0, 3) Y1t1 = gamma.ppf(U1t1, 2) Y2t1 = gamma.ppf(U2t1, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1t0, Y1t1) plt.xlabel(&#39;Y1(t=t0)&#39;) plt.ylabel(&#39;Y1(t=t1)&#39;) plt.show() correl = np.corrcoef(Y1t0, Y1t1) print(&quot;Estimated Pearson Auto-Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Auto-Correlation Coefficient: 0.37600307233845764 . In this code example we created to variables Y1 and Y2, each one taking a value from a Gamma distribution at times t0 and t1. Y1 and Y2 have a dependency between eachother but also temporally. . As with any temporal model the time period chosen is very important, typically for insurance contracts yearly time periods make sense. However in one particular model I developed there was a need for monthly simulations, rather than re-parameterising the entire central driver structure to work on a monthly basis (creating lots of extra data that will not be used by the vast majority of the models) I applied a &quot;Brownian Bridge&quot; type argument to interpolate driver simulations for each month. . Notes on Implementation . In this blog post I have not included the code exactly as it is implemented in production since this is my employer&#39;s IP. The implementation presented here is not very efficient and trying to run large portfolios in this way will be troublesome. In the full production implementation I used the following: . Strict memory management as the this is a memory hungry program | Certain aspects of the implementation are slow in pure python (and even Numpy) Cython and Numba are used for performance | The Scipy stats module is convenient but restrictive, it is better to either use the Cython address for Scipy special functions or implement functions from scratch. By implementing extended forms of some of the distribution functions one is also able to allow for non-integer degrees of freedom which is useful | The model naturally lends itself to arrays (vectors, matrices, tensors) however these tend to be sparse in nature, it is often better to construct &quot;sparse multiply&quot; type operations rather than inbuilt functions like np.dot | Conclusion . This blog posts represents the current iteration of the aggregation framework I have developed. It is considered a &quot;version 0.1&quot; implementation and is expected to develop as we use it more extensively and uncover further properties or issues. For example it is clear regardless of parameters selected the joint behaviour will always be (approximately) elliptical, as presented it is not possible to implement non-linearities (e.g. the price of some asset will only attain a maximum/minimum value dependent on some other driver indicator). It is not difficult to implement ideas like this when the need arises, the difficulty becomes more around how to implement the idea in a seamless way. . There are a couple of additional benefits to this framework which we have not mentioned, I will outline these here briefly: . It is possible to parallelise this process quite effectively as there are minimal bottlenecks/race conditions | The driver variables can be generated centrally and models can link to this central variable repository. From a work-flow perspective this means that individual underwriting teams can run models independently (quickly) leaving the risk teams to collate an analyse the results. (Sometimes called a federated workflow.) | The federated workflow means no specialist hardware is required, even very large portfolios can be run on standard desktops/laptops. | The current production version of this framework has around 5-10,000 driver variables (&quot;X1, X2&quot;) over 5 different hierarchical layers. These influence dependence between around 500,000 individual modelled variables (&quot;Y1, Y2&quot;) with 20 time periods (&quot;t0, t1&quot;). The quality of risk management analysis and reporting has increased dramatically as a result. . There are still some things left to do in relation to this framework and the work is on-going. These include: . Work relating to calibration and how to do this as efficiently as possible | Further work on increasing code efficiency | Further mathematical study of the framework&#39;s parameters | Study of the implied network behaviour: since we&#39;re placing risks on a network (driver structure) can we gain additional insight by considering contagion, critical nodes, etc.? | Further improvements to the workflow, how the model data is stored/collated etc. |",
            "url": "https://www.lewiscoleblog.com/insurance-aggregation-model",
            "relUrl": "/insurance-aggregation-model",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Welcome",
          "content": "Lewis Cole 2020 . Hello, and welcome to my new blog. I have never written a blog before and so this is likely to be a bit of a work in progress slowly evolving in time. . Within this blog I aim to talk about some ideas that are of interest to me. My interests are quite broad and so there will likely be a wide variety of topics discussed. I suppose most, if not all, of these interests could be placed under the broad umbrella of “models” or “simulation”. Some particular areas or topics I wish to write about include: . Non-linear dynamics | Systems out of equilibrium | Fat-tail and extreme value statistics | Non-ergodicity | Path dependence | Individual vs Collective phenomena | Agent based modelling | Networks | Machine Learning / Data Analysis | Mathematics, Probability, Computer Science generally | Inter-disciplinary study | . Many of these areas could be considered “complex” or “complex systems” although I am not a big fan of the term due to lack of a consistent definition. In many cases we must rely on computational methods since traditional analytic methods tend to fall down. As a result most blog posts will contain sample code, I will write this in Python (with a variety of libraries/packages) owing to ease of understanding and it’s ubiquity. As such I will also write about more computational considerations such as: . Python packages | Other languages | Writing performant python | Optimization techniques | and so on | . From time to time I might also include more “thought pieces” on news/recent research or book reviews or similar. . Where applicable I will try and assume no specific knowledge of a particular subject and try and build up to a somewhat sophisticated level of understanding. However I will be forced to assume a certain level of mathematical/statistical/programming understanding as I would rather not write articles on the fundamentals, where possible I will try and mention the name of techniques used so if they appear unfamiliar it will at least be possible to search for resources online. . To create this blog I used the fast.ai fastpages which has made the job much easier. I have a great deal of gratitude to Jeremy Howard, Hamel Husain and the folks at fast.ai for making this simple as possible so I can focus on creating content instead of worrying about the technicalities of creating the blog. . Thank you for your time, I hope you enjoy my blog posts. . If you would like to get in contact with me you can via my twitter account or via this blog’s email: .",
          "url": "https://www.lewiscoleblog.com/welcome/",
          "relUrl": "/welcome/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

}