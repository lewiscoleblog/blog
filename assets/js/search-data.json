{
  
    
        "post0": {
            "title": "Spin Glass Models 4: Ising Model - Simulation",
            "content": ". This is the fourth blog post in a series - you can find the previous blog post here . . Setup . In this blog we will limit ourselves to trying to find minimum energy states of the Ising model - we could use these schemes in order to estimate other thermodynamic properties also. The models will be in 2-dimensions on a square lattice since the results of these will be easier to display, it should be fairly obvious how we could modify this code for other dimensions. We will also use Gaussian distributed interaction strengths since we do not require mathematical tractability when we are simulating. . Interactions . Unlike the Sherrington-Kirkpatrick model we will have to be a bit &quot;clever&quot; in defining the interaction strengths. I have decided to do this using 4 (NxN) arrays representing the up, down, left and right interaction strengths. This is a bit wasteful in terms of memory use but I think it is worth the trade-off to improve readability of the code. We then have the following symmetries: . up[i,j] == down[i-1,j] left[i,j] == right[i,j-1] . Where these indices exist, for the boundary coniditons we will take the following: . up[0,j] == 0 down[N-1,j] == 0 left[i,0] == 0 right[i,N-1] == 0 . For all possible $i, j$. This is a fixed boundary condition, the edge cases will have only 3 neighbours and corner cases only 2. This can introduce some instability for small spin-glasses but this is fine for our purposes. Modifying this to allow for periodic boundary conditions (i.e. a toroidal geometry) would not be particularly difficult. . Mixing, Convergence, etc. . The methods presented below are Markov-Chain Monte-Carlo (MCMC) methods. These methods are notoriously finicky and require parameter tuning, running multiple chains, etc. in order to ensure good performance. As to not get distracted in this blog post I will not mention these concerns but please keep them in mind when reading on. For now we will run a single chain and look at the results it produces, in practice one would not do this. . Local Update Methods . We begin by looking at local update methods. In the Sherrington-Kirkpatrick blog post we looked at one such (very bad) method: the greedy gradient descent. Local update methods mean at each simulation step we update 1 site only. . Metropolis-Hastings . First we look at the &quot;classic&quot; approach to simulating the Ising model. In many texts and online references this will be the only method that is presented, it leads some to believe (incorrectly) that the simulation method is somehow part of the Ising model itself. This is the method I presented (without elaboration) in my Cython/Numba blog post. . We present Metropolis-Hastings in it&#39;s most general form first (our application to the Ising model will appear below): . Initialize the system in state $x_t$ | Sample a new proposed state from some distribution $Q(x&#39;_{t+1} | x_t)$ | Accept this new state with probability $min left[ alpha, 1 right]$, else $x_{t+1} = x_t$, where: $$ alpha = frac{P(x&#39;_{t+1})Q(x_t |x&#39;_{t+1})} {P(x_t)Q(x&#39;_{t+1} |x_{t})} $$ | Repeat steps 2. and 3. | In this context $P(.)$ represents the probability distribution we wish to estimate. We notice that this does not need to be standardized (i.e. we can ignore any difficult partition functions) so this is a (relatively) easy algorithm to implement. We find that the sequence $x_t$ forms a Markov-Chain - hence the term Markov-Chain Monte Carlo (MCMC). . We can see that that Metropolis-Hastings consists of 2 steps: a proposal step then an acceptance/rejection step. We see that there are essentially no constraints on $Q(.)$ so we can use something easy to sample from. We only need to be careful that it has a support that contains the support of $P(.)$ - otherwise we will not be able to sample all possible values as required. Selecting a suitable $Q(.)$ is at the heart of the algorithm however, the best performance will be where $Q$ and $P$ are very similar. Since $P$ is generally unknown in advance this often requires a bit of trial and error. There are many ways to test whether the algorithm is doing a &quot;good job&quot; but we will not cover them here (in a later set of blog posts I should probably go deeper into MCMC methods - here we&#39;re just concerned with the Ising model in particular). . Moving back to the Ising model picture, we can apply the Metropolis-Hastings methodology via: . First pick a random site | Calculate the change in energy associated with &quot;flipping&quot; this sites spin (going from +1 to -1 or vice versa) | If the energy decreases accept the flipped state and start again with a new site | If energy increases accept the flipped state with some acceptance probability or keep the same state | Begin again | We can see that in contrast to the &quot;greedy hill climber&quot; approach in the Sherrington-Kirkpatrick blog post we are not simply declining into a local minima, there is some probability that we can jump to a state of higher energy and escape a local minima to find a &quot;better&quot; one. In theory if we wait long enough we should find ourselves in a global minimum state. We now derive the acceptance probability as given by Metropolis-Hastings: . Recall that the Gibbs distribution for the Ising model with a given configuration is: $$ P( sigma) = frac{exp(- beta H_{ sigma})}{Z_{ beta}} $$ . Since we are uniformly selecting new states the $Q(.)$ terms in $ alpha$ cancel. If we denote the propsed state as: $ hat{ sigma}$ then the relative likelihood is: $$ alpha = frac{P( hat{ sigma})}{P( sigma)} = frac{exp(- beta H_{ hat{ sigma}})}{exp(- beta H_{ sigma})} = exp left(- beta left(H_{ hat{ sigma}} - H_{ sigma} right) right) = exp(- beta Delta H) $$ . To summarise by Metropolis-Hastings we then accept the new configuration with probability: $$ p_{flip} = min left[1, exp(- beta Delta H) right] $$ . In this particular case we can calculate $ Delta H$ quite efficiently since we are changing the spin of only 1 site, the contributions of all other sites &quot;cancels out&quot;. We can write this down as: $$ Delta H = 2 sigma_{i,j} sum_{(x,y) in langle i, j rangle} J_{x,y} sigma_{x,y} $$ . This introduces some efficiency in a code implementation since we do not have to calculate the energy of the whole configuration each time we want to update a site&#39;s spin. . An example implementation of this can be seen below: . # An implementation of an Ising spin-glass of size NxN # With fixed boundary conditions using Metropolis-Hastings # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Updates occur at randomly selected sites import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 32 spins = np.random.choice([-1, 1], (N,N)) # Fix number of timesteps and some containers timesteps = 10000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction arrays # Have 4 arrays: up, down, left right # These represent the interaction strengths to the # up/down/left/right neighbours of a site # There is a symmetry between these matrices # This is not the most memory efficient solution s_h = 1 s_v = 1 up = np.zeros((N,N)) down = np.zeros((N,N)) left = np.zeros((N,N)) right = np.zeros((N,N)) up[1:N,:] = np.random.rand(N-1,N) * s_v down[0:N-1,:] = up[1:N,:] left[:,1:N] = np.random.rand(N,N-1) * s_h right[:,0:N-1] = left[:,1:N] mag[0] = spins.sum() for i in range(N): for j in range(N): if i == 0: up_neighbour = 0 down_neighbour = spins[i+1,j] elif i == N-1: up_neighbour = spins[i-1,j] down_neighbour = 0 else: up_neighbour = spins[i-1,j] down_neighbour = spins[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = spins[i,j+1] elif j == N-1: left_neighbour = spins[i,j-1] right_neighbour = 0 else: left_neighbour = spins[i,j-1] right_neighbour = spins[i,j+1] energy[0] += spins[i,j]*(up[i,j]*up_neighbour + down[i,j]*down_neighbour + left[i,j]*left_neighbour + right[i,j]*right_neighbour) # Avoid double count - each neighbour pair # counted twice in above since loop over each site energy[0] /= 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1 # Define proposal step def proposal(s_array): _N = s_array.shape[0] return np.random.choice(_N, 2) def energy_change(spin_site, bt, s_array, up_array, down_array, left_array, right_array): i = spin_site[0] j = spin_site[1] if i == 0: up_neighbour = 0 down_neighbour = s_array[i+1,j] elif i == N-1: up_neighbour = s_array[i-1,j] down_neighbour = 0 else: up_neighbour = s_array[i-1,j] down_neighbour = s_array[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = s_array[i,j+1] elif j == N-1: left_neighbour = s_array[i,j-1] right_neighbour = 0 else: left_neighbour = s_array[i,j-1] right_neighbour = s_array[i,j+1] dE_tmp = 2*s_array[i,j]*(up_array[i,j]*up_neighbour + down_array[i,j]*down_neighbour + left_array[i,j]*left_neighbour + right_array[i,j]*right_neighbour) return dE_tmp def acceptance(bt, energy): if energy &lt;= 0: return -1 else: prob = np.exp(-bt*energy) if prob &gt; np.random.random(): return -1 else: return 1 # Define update step dE = 0 dM = 0 def update(bt, s_array, up_array, down_array, left_array, right_array): global dE global dM # Proposal Step site = proposal(s_array) # Calculate energy change dE = energy_change(site, bt, s_array, up_array, down_array, left_array, right_array) dM = -2*s_array[site[0],site[1]] # Acceptance step accept = acceptance(bt, dE) if accept == -1: s_array[site[0], site[1]] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts, s_array, up_array, down_array, left_array, right_array): s_temp = s_array.copy() for i in range(ts): update_step = update(beta, s_temp, up_array, down_array, left_array, right_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, up, down, left, right) mag = mag / (N**2) energy = energy / (N**2) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . Gibbs Sampling . Many presentations of the Ising model only show the Metropolis-Hastings scheme, as such there is a misconception that the Metropolis-Hastings sampling is somehow part of the Ising model itself. This is not true, an alternative to Metropolis-Hastings is Gibbs-Sampling. We can describe Gibbs-Sampling in general terms as: . Pick an initial state $ pmb{x_t} = left(x^1_t, x^2_t, ... , x^N_t right)$ | For each $N$ dimension sample: $x^i_{t+1} sim P left(x^i_{t+1} | x^i_{t+1}, x^2_{t+1}, ... , x^{i-1}_{t+1}, x^{i+1}_t, ... x^N_i right)$ and define next state as: $ pmb{x_{i+1}} = left(x_1^{i+1}, x_2^{i+1}, ... , x_N^{i+1} right)$ | Repeat sampling for each successive state | We can see this is just a special case of Metropolis-Hastings, if we denote $ pmb{x_t^{-j}} = left(x_t^1, x_t^2, ... , x_t^{j-1}, x_t^{j+1}, ..., x_t^N right)$ (all components apart from the jth). Then we can set: $$Q(x_{t+1}^j pmb{x_t^{-j}} | pmb{x_t} ) = P(x_{t+1}^j | pmb{x_t^{-j}})$$ In the Metropolis-Hastings scheme, the acceptance probablilty then becomes: begin{align} min left[1 , frac{Q(x_{t+1}^j pmb{x_t^{-j}} | pmb{x_t} ) P( pmb{x_t})}{Q( pmb{x_t} | x_{t+1}^j pmb{x_t^{-j}} ) P(x_{t+1}^j pmb{x_t^{-j}})} right] &amp; = min left[1 , frac{P( pmb{x_t})P(x_{t+1}^j | pmb{x_t^{-j}})}{P(x_{t+1}^j pmb{x_t^{-j}})P(x_{t}^j | pmb{x_t^{-j}})} right] &amp;= min left[1 , frac{P(x_{t}^j | pmb{x_t^{-j}})P( pmb{x_t^{-j}})P(x_{t+1}^j | pmb{x_t^{-j}})}{P(x_{t+1}^j | pmb{x_t^{-j}})P( pmb{x_t^{-j}})P(x_{t}^j | pmb{x_t^{-j}})} right] &amp;= min left[1, 1 right] = 1 end{align} Which results in the Gibbs procedure as described above. . We can implement Gibbs sampling in the context of an Ising model as: . Pick a random site $(i,j)$ | Set spin to +1 with probability $p_{ij}$, or -1 with probability $(1-p_{ij})$ | Begin again | The probability is defined as: $$ p_{ij} = frac{1}{1+ exp(- beta Delta H / sigma_{i,j})} $$ . At lower temperatures this should behave similarly to the Metropolis-Hastings. However the Metropolis-Hastings method has approximately twice the probability of accepting energetically unfavourable states, as such the Gibbs might be less efficient. . Note: this is true for the Ising model with methods as descibed - it is note a general point on Gibbs/Metropolis-Hastings. When sampling from higher dimensional distributions Gibbs samplers sample each dimension independently whereas Metropolis-Hastings samples points from the high-dimensional space. In some situations the Gibbs sampler can perform significantly better. . Since this is only a minor update to the Metropolis-Hastings code we will not present it here. . Glauber Dynamics (and Heat-Bath) . Another confusion I have seen is that Metropolis-Hastings is the only acceptance-rejection scheme. This is not true, we can also define alternative acceptance probabilites. One such example is Glauber dynamics where we can express the acceptance probability as: $$ p_{flip} = frac{1}{2} left( 1 - tanh left( beta Delta H / 2 right) right) frac{exp(- beta Delta H/2)}{exp( beta Delta H/2) + exp(- beta Delta H/2)} $$ We will not derive this here nor simulate using Glauber dynamics (but we could modify 1 or 2 lines of the code above to ahcieve this), it is just to illustrate another option. For the Ising model Glauber dynamics has a lower acceptance probability for all possible states, as such its performance is likely to be slightly worse than Metropolis-Hastings. . For the Ising model Glauber-Dynamics is identical to the Heat-Bath method. . Simulated Annealing and Simulated Tempering . We now move onto our first &quot;improvement&quot; to the Metropolis-Hastings scheme: simulated annealing. . The concept is very simple and takes inspiration from physical systems. Essentially we just start the system in a high temperature and gradually cool it down. This makes intuitive sense since at higher temperatures we have an increased chance of jumping out of local-minima and get closer to a better overall minima. However we will struggle to actually locate the minima at high temperature for the same reason. In contrast with a low temperature we will be able to locate a nearby local-minima very accurately but will not be able to jump out of it to find a better one. By starting off &quot;hot&quot; the samples will jump between many local minima, as we slowly cool down there should be fewer jumps between local minima and it should eventually get stuck in the domain of a &quot;good&quot; local minima (not far from the global minima ideally) as we cool the temperature further we should get closer and closer to that minima. This is similar to the process of annealing metals by heating them then cooling them to reorganize the crystalline structure. . Simulated annealing has proved very useful in the field of combinatorial optimization in situations where we want to quickly generate &quot;good&quot; solutions (not necessarily &quot;best&quot;). Variations on the idea can be seen in many areas (e.g. variable learning rate algorithms in deep learning can be thought of as a form of simulated annealing). We can also note there is nothing in the method that is particular to Metropolis-Hastings (or the other variations presented) - it can be used with pretty much any simulation method. . We have not descibed &quot;how&quot; we would want to decrease the temperature over time. This is part of the &quot;issue&quot; with simulated annealing (and many MCMC algorithms in general) - there is just as much art as their is science to implementing them. There are no real hard fast rules for getting good results, one in essense has to just try various options until something works (or on well studied problems borrow schemes from others). . We will take a simple approach where we decrease temperature by 10% every 500 steps starting from a temperature of 4 (this was chosen arbitrarily - it is not a suggestion of what might work well in this situation!) We can make use of the code example above for Metropolis-Hastings to give a compact implementation of: . # An implementation of a Metropolis-Hastings algorithm # with simulated annealing applied to a 2d Ising spin glass # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 32 spins = np.random.choice([-1, 1], (N,N)) # Set up initial beta beta = 1/4 def _main_loop_SA(ts, bt_initial, s_array, up_array, down_array, left_array, right_array): s_temp = s_array.copy() bt_live = bt_initial for i in range(ts): if ts % 500 == 0: bt_live *= 1/0.9 update_step = update(bt_live, s_temp, up_array, down_array, left_array, right_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop_SA(timesteps, beta, spins, up, down, left, right) mag = mag / (N**2) energy = energy / (N**2) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . We can see that the system settled down to a lower energy state more quickly and smoothly than with the vanilla Metropolis-Hastings scheme. . Although simulated annealing can improve on vanilla Metropolis-Hastings it can still struggle to find a global minima of the system. There are various &quot;hacks&quot; that can further improve this however - one such example being the concept of restarting. Again this is a very &quot;obvious&quot; thing to try - we store the &quot;best&quot; state we&#39;ve visited so far in a simulation, if we &quot;get stuck&quot; somewhere above this energy level we &quot;jump back&quot; to this best state and try again. . We can extend the simulated annealing idea further to the concept of &quot;simulated tempering&quot;. Here we treat the temperature of the system as a variable in itself, the teperature can go up as well as down during the simulation. This can further improve convergence properties since it allows the system to escape energy boundaries more easily by increasing temperature. This can remove the need to use restarting since a higher temperature is always available as an option at all times. . One such simulated tempering scheme is &quot;parallel tempering&quot; - as the name suggests this involves running many Markov-Chains in parallel and &quot;jumping&quot; between chains as the algorithm progresses. In some instances the cost of running multiple chains is less than the improvement in performance. Again however there is an art to selecting the correct number of chains and temperatures to run in parallel, most times there is no substitute for just trying things and running tests on the results. For the interests of brevity we will not present a full code here - but we note that for 2 chains at temperatures $T_1$ and $T_2$ our proposed update is to switch the states between the two chains (or swap temperatures of the 2 chains) the acceptance probability is then: $$p_{flip} = min left[1, exp((H_1-H_2)( beta_1 - beta_2)) right] $$ Where $H_1$ is the energy as defined by the Hamiltonian for chain with temperature $T_1 = 1/ beta_1$ and similar for $H_2$. This can be easily adapted to more chains and temperatures. . Cluster Update Methods . We now have a few options for simulating the Ising model, however they are by no means perfect. The issue still remains of falling into local optima instead of a global optima. From our previous mathematical study we know that energy minima for the Ising model are &quot;far away&quot; from each other, that is they have very little overlapping spins. By flipping individual spins 1 by 1 it is very hard to make the chains explore the energy landscape fully. The natural way to solve this is to flip multiple spins simultaneously at each step. From the general definition of the Metropolis-Hastings method there is nothing stopping us in following this line of reasoning. . Unfortunately this makes things much harder, the complications arise in finding a valid scheme for flipping multiple spins at once. We have glossed over the mathematical foundations of MCMC here but the proposal/acceptance probabilities need to be selected in &quot;smart way&quot; in order for the resulting Markov chain to have certain properties. When looking at more than 1 spin at a time in the Ising model this proved fairly difficult. This is evidenced by the original Metropolis-Hastings scheme being proposed in 1953 yet the first multi-spin method not being proposed and justified until the late 1980s. . Wolff Algorithm . The main idea of the algorithm is to look for &quot;clusters&quot; of spin sites with the same spin. We then decide to flip the spin of all the sites within this cluster at once. We then pick a new cluster and repeat this process as necessary. The pseudocode for this algorithm as it applies to the Ising model is: . A site $x$ with spin $ sigma_x$ is selected at random and added to an empty cluster | For each neighbour $y$ of $x$ such that $ sigma_y = sigma_x$ we add $y$ to the cluster stack with probability $p_{xy} = 1 - exp(-2 beta J_{xy})$ else move onto next neighbour | After all neighbours are exhausted select next site in the cluster stack and repeat the previous step until the cluster stack is exhausted | Once the cluster is fully specified flip the spins of all sites in the cluster and begin again | We can see that like the Gibbs sampling algorithm, here the Wolff algorithm is &quot;rejection free&quot; - that is all proposed sites are flipped. We also note that there is nothing in this method that is incompatible with simulated annealing/tempering - these techniques are often used together. . Some of the more computer-science focussed readers may be thinking: &quot;creating clusters is computationally expensive, will a brute force local update method not be better?&quot; Which is a valid concern; there a 2 things to consider here - firstly there are many efficient cluster generating algorithms from percolation theory which can help speed up this process (we presented a very simple method above for clarity.) And secondly the local update methods will take a very long time to make &quot;big jumps&quot; away from the current configuration - even though there is some probability to make unfavourable movements most of the time these will not be accepted, to jump to a different local energy minima we would need many such unfavourable moves which means we could be waiting for a very long time! This is why spin glass models form a good test bed for optimization algorithms as they are one of the simplest to define models with &quot;difficult&quot; energy landscapes. . We should find this algorithm performs better in general, especially near the 2nd order phase transition whereby successive samples become increasingly correlated (whereas we would want more independent samples). We will try and produce plots of the 2nd order themodynamic variables: heat capacity and susceptibility. We should expect to see an approximate discontinuity at the critical temperature. To do this we will re-run the Wolff algorithm multiple times for each target temperature. For each target temperature we will run 1500 &quot;burn&quot; steps and then evaluate our variables over the next 2500 steps. This should be long enough to get some reasonable results. . Note that the Wolff algorithm does not &quot;converge&quot; to a low energy state like the preceeding algorithms, instead it samples from the entire space in a &quot;smart way&quot; - even if it finds itself in the global energy minima there is still a relatively high probability of escaping. As such the graphs we produced before will look more &quot;wiggly&quot; - I&#39;ve heard the term &quot;fat caterpillar&quot; used to describe the graph of a well mixed MCMC algorithm. If we were interested in finding a ground state we could keep track of the configuration corresponding to the lowest observed energy state so far (we will not do this in our code however). . In the example below we&#39;ll run a constant $J=1$ to try and reproduce the heat capacity we found analytically in the previous blog post as a proof of concept. I will leave the functionality for general interaction strengths should you wish to experiment. . An implementation of this method can be seen below (note: this is a very slow code since we&#39;re using Python lists! It would be a prime candidate for being sped up using Cython/Numba/etc.): . # An implementation of the Wolff algorithm # With simulated annealing applied to a # 2d Ising spin glass import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model NxN and initial spins N = 32 spins_initial = np.random.choice([-1, 1], (N,N)) # Fix time-steps burn = 1500 evaluation = 2500 time_steps = burn + evaluation # Initialize interaction arrays # Have 4 arrays: up, down, left right # These represent the interaction strengths to the # up/down/left/right neighbours of a site # There is a symmetry between these matrices # This is not the most memory efficient solution s_h = 1 s_v = 1 up = np.zeros((N,N)) down = np.zeros((N,N)) left = np.zeros((N,N)) right = np.zeros((N,N)) # Using J=1 constant so graphs are easier to generate # Replace with comments to give an EA spin glass up[1:N,:] = 1 #np.random.rand(N-1,N) * s_v down[0:N-1,:] = up[1:N,:] left[:,1:N] = 1 #np.random.rand(N,N-1) * s_h right[:,0:N-1] = left[:,1:N] # Create function to find neigbour sites def nbr_udlr(s_site, s_array): _N = s_array.shape[0] i = s_site[0] j = s_site[1] if i == 0: up_site = 0 down_site = [i+1, j] elif i == _N-1: up_site = [i-1,j] down_site = 0 else: up_site = [i-1,j] down_site = [i+1, j] if j == 0: left_site = 0 right_site = [i,j+1] elif j == N-1: left_site = [i,j-1] right_site = 0 else: left_site = [i,j-1] right_site = [i,j+1] return [up_site, down_site, left_site, right_site] # Create function to return interactions strength def int_strength(s_site, udlr, up_array, down_array, left_array, right_array): if udlr == 0: return up_array[s_site[0], s_site[1]] if udlr == 1: return down_array[s_site[0], s_site[1]] if udlr == 2: return left_array[s_site[0], s_site[1]] if udlr == 3: return right_array[s_site[0], s_site[1]] def energy_calc(s_array, up_array, down_array, left_array, right_array): _N = s_array.shape[0] energy = 0 for i in range(_N): for j in range(_N): if i == 0: up_neighbour = 0 down_neighbour = s_array[i+1,j] elif i == N-1: up_neighbour = s_array[i-1,j] down_neighbour = 0 else: up_neighbour = s_array[i-1,j] down_neighbour = s_array[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = s_array[i,j+1] elif j == N-1: left_neighbour = s_array[i,j-1] right_neighbour = 0 else: left_neighbour = s_array[i,j-1] right_neighbour = s_array[i,j+1] energy += s_array[i,j]*(up_array[i,j]*up_neighbour + down_array[i,j]*down_neighbour + left_array[i,j]*left_neighbour + right_array[i,j]*right_neighbour) return -energy/2 def wolff_step(bt, s_array, up_array, down_array, left_array, right_array): _N = s_array.shape[0] initial_site = np.random.choice(_N, 2) initial_site = [initial_site[0], initial_site[1]] old_spin = s_array[initial_site[0], initial_site[1]] cluster = [initial_site] stack = [initial_site] while stack != []: site = stack[np.random.choice(len(stack))] # Cycle neigbours nbr = nbr_udlr(site, s_array) for i in range(4): nbr_live = nbr[i] if nbr_live == 0: continue nbr_spin = s_array[nbr_live[0], nbr_live[1]] if nbr_spin == old_spin: if nbr_live not in cluster: p = 1 - np.exp(-2*bt*int_strength(site, i, up_array, down_array, left_array, right_array)) if np.random.random() &lt; p: cluster.append(nbr_live) stack.append(nbr_live) stack.remove(site) for site in cluster: s_array[site[0], site[1]] *= -1 return s_array ###### Main Code # Create useful constants N1 = evaluation*N*N N2 = evaluation*evaluation*N*N # Define temp ranges temp_steps = 20 temp_min = 1.75 temp_max = 2.75 temp_array = np.linspace(temp_min, temp_max, num=temp_steps) M = np.zeros(temp_steps) E = np.zeros(temp_steps) C = np.zeros(temp_steps) X = np.zeros(temp_steps) for t in range(temp_steps): spins = spins_initial.copy() M1 = 0 M2 = 0 E1 = 0 E2 = 0 beta = 1/temp_array[t] for i in range(time_steps): spins = wolff_step(beta, spins, up, down, left, right) if i &gt; burn: mag_tmp = abs(spins.sum()) M1 += mag_tmp M2 += mag_tmp**2 energy_tmp = energy_calc(spins, up, down, left, right) E1 += energy_tmp E2 += energy_tmp**2 M[t] = M1 / N1 E[t] = E1 / N1 C[t] = (E2/N1 - E1**2/N2)*beta**2 X[t] = (M2/N1 - M1**2/N2)*beta # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].scatter(temp_array, M, color=&#39;Red&#39;) axs[0, 0].set_title(&quot;Magnetism&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;Magnetism&quot;) axs[0, 1].scatter(temp_array, E, color=&#39;Blue&#39;) axs[0, 1].set_title(&quot;Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;Energy&quot;) axs[1, 0].scatter(temp_array, X, color=&#39;Red&#39;) axs[1, 0].set_title(&quot;Susceptibility&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;Susceptibility&quot;) axs[1, 1].scatter(temp_array, C, color=&#39;Blue&#39;) axs[1, 1].set_title(&quot;Heat Capacity&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;Heat Capacity&quot;) plt.show() . The plots here are a bit noisy but they loosely match our previous theoretical findings. . Swendsen-Wang Algorithm . Another cluster algorithm is the Swendsen-Wang. Unlike the Wolff algorithm Swendsen-Wang looks at multiple clusters concurrently and applies a spin-flip to all clusters. It was propsed 2 years prior to the Wolff method. . In pseudo code it can be presented as: . From an initialized spin configuration for each neighbour pair of sites $ langle x, y rangle$ we specify a bond: $b_{x,y} in {0, 1 }$ Where we sample according to: begin{align} &amp; mathbb{P}(b_{x,y} = 0 | sigma_x neq sigma_y) = 1 &amp; mathbb{P}(b_{x,y} = 1 | sigma_x neq sigma_y) = 0 &amp; mathbb{P}(b_{x,y} = 0 | sigma_x = sigma_y) = exp(-2 beta J_{xy}) &amp; mathbb{P}(b_{x,y} = 1 | sigma_x = sigma_y) = 1 - exp(-2 beta J_{xy}) end{align} | Generate clusters using bonds. If there exists a bond between sites $b_{x,y} = 1$ then the sites belong to same cluster | For each cluster with probability 1/2 flip all spins within the cluster to get a new configuration | Repeat process of generating bonds and clusters | We can see this is slightly different to the Wolff algorithm since it looks at multiple clusters within a given step. The performance of the Swendsen-Wang is slightly worse than that of the Wolff since it has a lower probability of flipping large clusters (in the case of Ising models). Both algorithms have been adpated and used to alternative spin glass models (as well as models outside of spin glasses). . We won&#39;t present an implementation of this method here since we already looked at the Wolff algorithm for an example of a cluster algorithm. . Conclusion . In this blog post we have looked at a variety of MCMC methods for simulating Ising models. We started by looking at various local update methods, which we now know do not behave optimally. We extended these ideas to cluster update methods which can show better performance for Ising models. We also looked at the very intuitive simulated annealing and simulated tempering methods, which have been used in optimization problems far outside the realms of spin glass models or even statsitical physics in general. .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-4",
            "relUrl": "/spin-glass-models-4",
            "date": " â€¢ Mar 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Spin Glass Models 3: Ising Model - Theory",
            "content": ". This is the third blog post in a series - you can find the previous blog post here . . In this blog post we are going to look at another spin-glass model. In the previous post we looked at the Sherrington-Kirkpatrick model which allowed us to study the spin-glass analytically. However there is a certain lack of &quot;realism&quot; in this model - the infinite interaction range means that the Sherrington-Kirkpatrick (in a sense) does not occupy any space, there is no concept of dimension nor geometry. While providing interesting mathematical results, some of which appear to be universal to spin-glass systems, we would like to look at a different model that captures more of the real world system and perhaps might uncover new behaviours. . Introducing the Ising Model . One model that we can look at is the Edwards-Anderson Ising model. We have actually looked at these models achronologically: Edwards-Anderson proposed the Ising model before Sherrington-Kirkpatrick proposed theirs. In fact Sherrington-Kirkpatrick developed their model partly due to the difficulty in deal with the Ising model. I have presented the models in this order in this series of blog posts because it feels more natural to look at models in increasing complexity order rather than presenting a historical account. . The main difference between Ising and Sherrington-Kirkpatrick is the extent over which the interactions can occur, instead of an infinite range the Ising model only allows &quot;nearest-neighbour&quot; interactions. This relaxation means that there is a concept of dimension and geometry to an Ising spin-glass. For example we could think of spins oriented on a line segment (a finite 1d Ising model), a square lattice (a 2d Ising model) or something more exotic like spins oriented on the nodes of a 32 dimensional hexagonal lattice. Through careful use of limits we could also consider infinite-dimensional Ising models too. The Hamiltonian follows the form one would expect: $$ H = - sum_{ lt x,y gt} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ Where we use the notation $ lt x,y gt$ to denote the sum occuring over neighbour pairs. . As before the selection of $J_{xy}$ is somewhat arbitrary, however unlike the Sherrington-Kirkpatrick we do not have to scale this with lattice size to retain meaningful &quot;average energy&quot; or &quot;average magnetism&quot; measurements when we increase the size of the system. If we take $J_{xy} = J$ for some fixed $J$ we get the Ising model of Ferromagnetism, if we allow $J_{xy}$ to vary according to some distribution we get the Edwards-Anderson Ising Model. In this blog we will use the term &quot;Ising model&quot; to refer to either situation, the mathematics of moving from the fixed $J$ case usually involves integrating against the density function, this can lead to more complicated formulae so in this blog we will mainly focus on the ferromagnetic case unless otherwise stated. . As before spins can only take values $ pm 1$ - in some literature this is referred to as &quot;Ising spins&quot; (e.g. you can read references to &quot;Sherrington-Kirkpatrick with Ising spins&quot; - this means an infinite interaction range with binary spins). . A pictorial representation of a 3d square lattice Ising spin-glass can be seen below: . For the rest of this article we will limit ourselves to talking about &quot;square&quot; lattices since this is where the majority of research is focussed. Using different lattice types won&#39;t change the story too much. . In studying these models it is also worth noting what happens on the &quot;boundary&quot; of the lattice: for finite size systems this can become an issue. In this blog post we will skirt over the issue, where necessary we will assume periodic boundary conditions (i.e. there is a toroidal type geometry to the system) so each &quot;edge&quot; of the lattice is connected to an opposing one, so in a sense there are no boundary conditions to specify. . 1D Ising Model . We will now consider a 1 dimensional Ising model where spins are located on a line segment. This simplication means that finding the ground state (minimal energy spin configuration) is trivial: we pick a site in the middle of the line segment (the origin) we will pick a spin ($ pm 1$) at random. We then propagate out from the origin in the left and right direction, we pick the spin for the next site according to the interaction strength for example: if $ sigma_0 = +1$ and $J_{0,1} &gt; 0$ then $ sigma_1 = +1$ and so on. We can see that (as with all spin glass systems) there is a symmetry: if we flip all the spins direction we end up with an equivalent energy - we call these &quot;pairs&quot; of configurations. It doesn&#39;t take much convincing to realise that the pair of configurations we constructed is a unique minima for the system. To see this imagine we have only one pair of spins that opposes the direction indicated by the interaction strength. For a large enough system we can eventually find another interaction strength that is greater in magnitude (by definition having a spin pair as indicated by the sign of the interaction strength). By simply flipping the relative orientation of the pairs we will end up with a configuration of lower energy - which is a contradiction. . We will now look to solve the 1d Ising model analytically. For simplicity we will assume no external magnetic field, this just makes the formulae look &quot;prettier&quot;. The crux of understanding any spin glass system is finding the Gibbs distribution, as before: $$P( sigma) = frac{exp(- beta H_{ sigma})}{Z_{ beta}}$$ Where: $$H = - sum_{ lt x,y gt} J_{xy} sigma_x sigma_y$$ And $ beta$ is the inverse temperature. We can write an expression for the partition function as: $$ Z_{ beta} = sum_{ sigma} exp(- beta H_{ sigma}) = sum_{ sigma} exp(- beta sum_{ lt x,y gt} J_{xy} sigma_x sigma_y) $$ Suppose we look at a line segment with $N$ spins, we can then write this as: $$ Z_{ beta} = sum_{ sigma_1, ..., sigma_N} exp(- beta (J_{1,2} sigma_1 sigma_2 + J_{2,3} sigma_2 sigma_3 + ... + J_{N-1,N} sigma_{N-1} sigma_N))$$ We notice that we can factorise this summation as: $$ Z_{ beta} = sum_{ sigma_1, ..., sigma_{N-1}} exp(- beta (J_{1,2} sigma_1 sigma_2 + ... + J_{N-2,N-1} sigma_{N-2} sigma_{N-1})) sum_{ sigma_N}exp(- beta J_{N-1,N} sigma_{N-1} sigma_N) $$ The internal sum can then be evaluated: $$ sum_{ sigma_N}exp(- beta J_{N-1,N} sigma_{N-1} sigma_N) = exp( beta J_{N-1,N} sigma_{N-1} ) + exp(- beta J_{N-1,N} sigma_{N-1} ) = 2cosh( beta J_{N-1,N} sigma_{N-1}) = 2cosh( beta J_{N-1,N}) $$ The last equality making use of the fact $ sigma_{N-1} = pm 1$ and that $cosh(x) = cosh(-x)$. By repeating this process we can express the partition function as: $$ Z_{ beta} = 2 prod_{i=1}^{N-1} 2 cosh( beta J_{i,i+1}) $$ We can evaluate this exactly. If we are sampling the interaction strengths according to some distribution we can find expected values (or other statistical properties) by integrating against the density function as usual, since this adds to notational complexity we will assume that the interaction strengths are fixed for now. We can then calculate thermal properties using the equations: . begin{align} F &amp;= - frac{1}{ beta} ln Z_{ beta} = - T ln 2 - T sum_{i=1}^{N-1} ln (2 cosh( beta J_{i,i+1})) U &amp;= - frac{ partial}{ partial beta} ln Z_{ beta} = - sum_{i=1}^{N-1} J_{i,i+1} tanh( beta J_{i,i+1}) C &amp;= frac{ partial U}{ partial T} = sum_{i=1}^{N-1} ( beta J_{i,i+1})^2 sech^2( beta J_{i,i+1}) S &amp;= frac{U - F}{T} = ln2 + sum_{i=1}^{N-1} left( - beta J_{i,i+1} tanh( beta J_{i,i+1}) + ln(2cosh( beta J_{i,i+1}) right) end{align}Where: F - is the Helmholtz-Free Energy U - is the thermodynamic energy (ensemble average) C - is the heat capacity S - is the entropy . We can find the expected value of a given instantiation of interactions through an integral such as: $$ mathbb{E}(U) = - sum_{i=1}^{N-1} int J_{i,i+1} tanh( beta J_{i,i+1}) Q(J_{i,i+1}) dJ_{i,i+1} $$ With $Q(J)$ being the density function of the distribution and the integral occuring over its support. Similar formulae exist for the other thermodynamic variables and you can calculate other statistics (e.g. variance) in the usual way. . We can plot the values of these variables for a given set of interaction weights: . # This code creates a 4 figure plot showing how # Thermodynamic variables change with temperature # For a 1d Ising Model with Gaussian interactions import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix seed np.random.seed(123) # Fix number of points N = 100 # Instantiate interaction strength J = np.random.normal(0,1,size=N-1) # Set temperature ranges T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min beta = 1 / T # Set up holders for variables F = np.zeros(T_steps) U = np.zeros(T_steps) C = np.zeros(T_steps) S = np.zeros(T_steps) # Loop over T_steps and calculate at each step for i in range(T_steps): F[i] = - T[i] * np.log(2) - T[i] * (np.log(2*np.cosh(beta[i]*J))).sum() U[i] = - (J * np.tanh(J * beta[i])).sum() C[i] = ((beta[i] * J)**2 * (np.cosh(beta[i] * J))**-2).sum() S[i] = (U[i] - F[i]) / T[i] # Divide by number of points to give a scale invariant measure F = F / N U = U / N C = C / N S = S / N # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].plot(T, F) axs[0, 0].set_title(&quot;Free Energy&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;F / N&quot;) axs[0, 1].plot(T, U) axs[0, 1].set_title(&quot;Average Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;U / N&quot;) axs[1, 0].plot(T, C) axs[1, 0].set_title(&quot;Heat Capacity&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;C / N&quot;) axs[1, 1].plot(T, S) axs[1, 1].set_title(&quot;Entropy&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;S / N&quot;) plt.show() . From these plots we can see there is no phase transition taking place - this is true for all 1d Ising models. . We can also calculate the correlation function for the system. By following a similar line of logic as before we find: $$ langle sigma_n sigma_{n+r} rangle = prod_{i=0}^{r} tanh( beta J_{n+i, n+i+1})$$ . We can plot this as a function of $r$ starting at the first position: . # This code creates a plot displaying # The correlation function # For a 1d Ising Model with Gaussian interactions import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix seed np.random.seed(123) # Fix number of points N = 100 # Instantiate interaction strength J = np.random.normal(0,1,size=N-1) # Create holders for variables r = np.arange(N) corr_array = np.zeros(N) # Fix beta beta = 1 # Calculate correlations tanh_array = np.tanh(beta * J) for i in range(N): corr_array[i] = tanh_array[:i].prod() plt.plot(r[0:20], corr_array[0:20]) plt.title(&quot;Correlation Function&quot;) plt.ylabel(r&quot;$ langle sigma_{1} sigma_{r} rangle$&quot;) plt.xlabel(&quot;r&quot;) plt.xticks(np.arange(11)*2) plt.ylim((-1,1)) plt.xlim((0,20)) plt.show() . As we can see there is a specific &quot;structure&quot; to the correlation function given an instantiation - this is not particularly surprising. If we picked an alternate starting state (e.g. the 2nd site) this graph can look totally different, the decay in absolute value will be similar however. We also notice that the correlation decays to zero fairly rapidly suggesting there isn&#39;t a long range structure to the model. . 2d Ising Model . We now turn our attention to the 2d Ising model. The mathematics here will, understandably, get more complicated. We will require a bit more sophistication to solve this system. The solution was originally published by Lars Onsager in 1944, with alternative proofs and derivations published later. The derivation itself is fairly involved and would take a blog post (at least) by itself to cover - I may come back to this at a later date. For now I will simply present the result for the free energy (F) in absence of a magnetic field ($h=0$): $$ - frac{ beta}{N} F = ln2 + frac{1}{8 pi^2} int^{2 pi}_{0} int^{2 pi}_{0} ln left[ cosh(2 beta J_H) cosh(2 beta J_V) - sinh(2 beta J_H)cos( theta_1) - sinh(2 beta J_V)cos( theta_2) right]d theta_1 d theta_2 $$ Where instead of $J_{xy}$ being sampled from a gaussian distribution we assume fixed interaction strengths $J_H$ and $J_V$ in the horizontal and vertical directions. . For simplicity we will take: $J_H = J_V = J$ and we can derive the thermodynamic properties (per site - e.g. $f= frac{F}{N}$) of this system as: begin{align} f &amp;= frac{-ln2}{2 beta} - frac{1}{2 pi beta} I_0( beta, J) u &amp;= -J coth(2 beta J) left[ 1 + frac{2}{ pi} left( 2tanh^2(2 beta J) -1 right) I_1( beta, J) right] c &amp;= 2J beta^2 left[U csch(2 beta J)sech(2 beta J) + frac{8J}{ pi} sech^2(2 beta J) I_1( beta, J) - frac{2 beta J}{ pi}(cosh(4 beta J)-3)^2 sech^6(2 beta J) I_2( beta, J) right] s &amp;= frac{U - F}{T} end{align} . For convenience I created 3 new functions $I_0( beta,J), I_1( beta, J)$ and $I_2( beta, J)$ to make the equations a little shorter. These functions are defined as: begin{align} I_0( beta, J) &amp;= int^ pi_0 ln left[ cosh^2(2 beta J) + sinh^2(2 beta J) sqrt{1 + csch^4(2 beta J) - 2 csch^2(2 beta J) cos(2 theta)} right] d theta I_1( beta, J) &amp;= int^{ frac{ pi}{2}}_0 left[1 - 4csch^2(2 beta J)( 1 + csch^2(2 beta J))^{-2} sin^2( theta) right]^{- frac{1}{2}} d theta I_2( beta, J) &amp;= int^{ frac{ pi}{2}}_0 sin^2( theta) left[1 - 4csch^2(2 beta J)( 1 + csch^2(2 beta J))^{-2} sin^2( theta) right]^{- frac{3}{2}} d theta end{align} . As before we can produce plots of these: . # This code creates a 4 figure plot showing how # Thermodynamic variables change with temperature # For a 2d Ferromagnetic Ising Model with fixed interaction import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad %matplotlib inline # Fix seed np.random.seed(123) # Instantiate interaction strength J = 1 # Set temperature ranges T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min beta = 1 / T # Set up holders for variables f = np.zeros(T_steps) u = np.zeros(T_steps) c = np.zeros(T_steps) s = np.zeros(T_steps) # Set up integrands for I0, I1, I2 def integrand0(x, b, j): return np.log(np.cosh(2*b*j)**2 + np.sinh(2*b*j)**2 * np.sqrt(1 + np.sinh(2*b*j)**(-4) - 2*np.sinh(2*b*j)**(-2) * np.cos(2*x))) def integrand1(x, b, j): return (1 - 4*np.sinh(2*b*j)**(-2)*(1 + np.sinh(2*b*J)**(-2))**(-2)*np.sin(x)**2)**(-0.5) def integrand2(x, b, j): return np.sin(x)**2 * integrand1(x, b, j)**3 # Loop over T_steps and calculate at each step for i in range(T_steps): bt = beta[i] I0 = quad(integrand0, 0, np.pi, args=(bt, J)) I1 = quad(integrand1, 0, np.pi/2, args=(bt, J)) I2 = quad(integrand2, 0, np.pi/2, args=(bt, J)) f[i] = - np.log(2) / (2 * bt) - I0[0] / (2 * np.pi * bt) u[i] = -J*np.tanh(2*bt*J)**(-1) * ( 1 + (2/np.pi)*(2*np.tanh(2*bt*J)**2 -1)*I1[0]) c[i] = 2*bt**2*J*( u[i]*np.sinh(2*bt*J)**(-1)*np.cosh(2*bt*J)**(-1) + (8*J / np.pi)*np.cosh(2*bt*J)**(-2)*I1[0] - (2*bt*J/np.pi)*((np.cosh(4*bt*J)-3)**2)*np.cosh(2*bt*J)**(-6)*I2[0] ) s[i] = (u[i] - f[i])*bt # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].plot(T, f) axs[0, 0].set_title(&quot;Free Energy&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;f&quot;) axs[0, 1].plot(T, u) axs[0, 1].set_title(&quot;Average Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;u&quot;) axs[1, 0].plot(T, c) axs[1, 0].set_title(&quot;Heat Capacity&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;c&quot;) axs[1, 1].plot(T, s) axs[1, 1].set_title(&quot;Entropy&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;s&quot;) plt.show() . We find there is a critical temperature $T_c$ where the specific heat equation diverges. We can compute the value of this as satisfying: $$ sinh left( frac{2J_H}{T_c} right)sinh left( frac{2J_V}{T_c} right) = 1 $$ In the case where $J_H = J_V = J$ we have: $$T_c = frac{2 J}{ln left(1+ sqrt{2} right)} $$ This is an example of a second order phase transition since the discontinuity only arises under a second derivative. . For temperatres under this critical temperature we have that the spontaneous magnetization can be calculated as: $$ m = left[ 1 - csch^2 left( frac{2J_H}{T} right)csch^2 left( frac{2J_V}{T} right) right]^{ frac{1}{8}} $$ . We can plot this (for $J_H = J_V = J = 1$) as: . # This code plots the spotaneous magnetization of # a 2d Ising model on a square-lattice import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set up temperature array T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min # Fix interaction strength constant J = 1 m = np.power(np.maximum(1 - np.sinh(2*J/T)**-4,0), 1/8) plt.plot(T,m) plt.xlabel(&quot;T&quot;) plt.ylabel(&quot;Magnetization&quot;) plt.title(&quot;2d Ising Model Spontaneous Magnetization&quot;) plt.xlim((0,5)) plt.show() print(&quot;Critical Temp (Tc):&quot;, (2*J) / np.log(1 + np.sqrt(2))) . Critical Temp (Tc): 2.269185314213022 . The correlation function is harder to compute, here we just present the correlation function between elements on the diagonal of the lattice: . $$ langle sigma_{0,0} sigma_{N,N} rangle = det left[ A_N right]$$ Where $A_N = (a_{n,m})_{n,m=1}^N$ is an $NxN$ matrix with entries: $$ a_{n,m} = frac{1}{2 pi} int_0^{2 pi} e^{i(n-m) theta} sqrt{ frac{sinh^2(2 beta J) - e^{-i theta}}{sinh^2(2 beta J) - e^{i theta}} } d theta $$ . We can plot this as so: . # This code plots the diagnoal correlation function # For the 2d square-lattice Ising model # Lattice size = NxN # At the critical temperature import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad %matplotlib inline # Set up integrand function imag = complex(0,1) def integrand(x, n, m, beta, J): return np.exp(imag*(n-m)*x)*np.sqrt((np.sinh(2*beta*J)**2-np.exp(-imag*x))/(np.sinh(2*beta*J)**2-np.exp(imag*x))) # Set up constants J = 1 beta = np.log(1 + np.sqrt(2)) / (2*J) # Set up arrays N_max = 20 N_array = np.arange(N_max+1) correl_array = np.zeros(N_max+1) for x in range(N_max + 1): N = N_array[x] A_N = np.zeros((N, N)) for n in range(N): for m in range(N): I = quad(integrand, 0, 2*np.pi, args=(n, m, beta, J)) A_N[n,m] = I[0]/(2*np.pi) correl_array[x] = np.linalg.det(A_N) # Plot correlations plt.plot(N_array, correl_array) plt.xlabel(&quot;N&quot;) plt.ylabel(r&quot;$ langle sigma_{0,0} sigma_{N,N} rangle$&quot;) plt.title(&quot;Correlation Function&quot;) plt.xticks(np.arange(11)*2) plt.ylim((0,1)) plt.xlim((0,20)) plt.show() . We can see that even though we have short range interactions (nearest neighbour) this gives rise to longer-range structure within the model. In fact we find for temperatures below the critical temperature there is an infinite correlation length, whereas for temperatures above the critical temperature the correlation length is finite. . Infinte Dimension Ising Model . Next we consider the situation where the Ising model exists in infinite dimensions. In this case each site has infinitely many neighbours, as such a mean-field approximation is valid and we have a model that is somewhat similar to the Sherrington-Kirkpatrick fully connected geometry. (Please excuse the lack of rigour here; one has to be careful in how limits are defined and it is a non-trivial exercise. For this blog post I don&#39;t want to go down into that sort of detail.) . If each site has infinitely many neighbours we only need to concern ourselves with the ratio of positive and negative spin neighbours. By mean field if we take the probability of a positive spin as $p$ then via the Gibbs distribution we have: $$ frac{p}{1-p} = exp(2 beta H)$$ The average magnetization can then be calculated: $$ mathbb{E} left[M right] = (1)p + (-1)(1-p) = 2p - 1 = frac{1 - exp(2 beta H)}{1 + exp(2 beta H)} = tanh(2 beta H)$$ . By investigating this function we can gain insight into spontaenous magnetization. Other similar arguments can be invoked for the other themodynamic properties. It is possible to show, as with the Sherrington-Kirkpatrick, that a phase transition occurs. . n-d Ising Model ($n geq3$) . Finally we look at the case where the dimension of the model is finite but strictly greater than 2. In this situation things get much trickier, in fact there are not many defined mathematical results in these systems and this is the subject of current research. As such I will just briefly outline some of the approaches that have been suggested to study these systems and their properties (presented in chronological order from when they were proposed): . Replica-Symmetry Breaking - From our previous post on Sherrington-Kirkpatrick models we briefly looked at this sort of method. They were an obvious first choice in trying to deal with short range interactions. It has been shown that the &quot;standard&quot; techniques are insufficient but there have been extensions proposed that have shown some promise. Like in the infinite range model it suggests states have an ultrametric structure and uncountably many pure states. | Droplet Scaling - The first such argument being presented by Rudolf Peierls. The main idea is to consider the arisal of &quot;loops&quot; or &quot;islands&quot; of spins (clusters of atoms all with the same spin being enclosed by some boundary). We then aim to count the number of such loops, often in high or low temperature ranges via approximation. This leads to only 2 pure states. | Chaotic Pairs - Has properties somewhat similar to a combination of the preceeding 2 methods, like replica symmetry breaking there are infinitely many thermodynamic states however the relationship between them is much simpler and has simple scaling behaviour - much like droplet scaling. | TNT - This interpretation does not itself specify the number of pure states, however it has been argued that it most naturally exists with 2 pure states - much like droplet scaling. However it has scaling properties more similar to that of replica symmetry breaking. | . For completeness: a pure state of a system is a a state that cannot be expressed as a convex combination of other states. . As far as I am aware there is currently no conesensus on which (if any) of the options presented is the correct interpretation. . One of the main questions that we would like to answer is whether there is a phase transition (first order). This is unresolved currently. Another question we might ask is whether there exists multiple ground state pairs (i.e. we can find 2 different configurations that have minimal energy that are not merely negative images of each other) - again this is unresolved. In infinite dimensions we can see this would be true, in 1d we know this cannot be true - for other dimensions it is unclear (although it is believed it is probably not true for d=2). . In addition to this there are many other unanswered questions that are the subject of current research into Ising type models. . Conclusion . In this blog post we have investigated some of the properties of square-lattice Ising models in various dimensions. In particular we have seen that there is no phase transition in 1d, a second order phase tranisition in 2d, a phase transition in infinite dimension and currently other dimensions are unresolved. We can see that the short-range interactions cause a lot of headaches when trying to analyse these systems. In the next blog post in this series we will begin to look at ways of simulating Ising models (and other spin glass models generally). . . This is the third blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-3",
            "relUrl": "/spin-glass-models-3",
            "date": " â€¢ Mar 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Spin Glass Models 2: Sherrington-Kirkpatrick",
            "content": ". This is the second blog post in a series - you can find the previous blog post here . . From a previous blog post we now have a reasonable understanding of what a spin glass is, some of the (frankly) bizarre behaviours they exhibit and some motivation behind why we might want to wish to study them. In this blog post we will embark on studying our first spin glass model due to Sherrington-Kirkpatrick. (Note: this model is sometimes also called the fully-connected Ising model.) . For our purposes we will just consider finding the properties and behaviours of interest rather than trying to capture exact behaviour of specific materials. As such we will assume measurement units are such that any constants &quot;disappear&quot; this should help with clarity. It is worth keeping this in mind if you look at any research on the topic (particularly in Physics journals) where extra terms may appear, these are typically to correct for units (e.g. to get energy measurements in Joules, distances in metres, etc.) . It is also worth noting that this model was not the first spin glass model and some other models appeared before this one. I have chosen to start with this as it is in some ways the &quot;simplest&quot; model. . Simplification . As with any good model we want to simplify the real world to a set of minimal requirements to capture the behaviour of interest. Recall the Hamiltonian for a general spin glass as: $$ H = - sum_{x,y} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ In a real world spin glass $ sigma$ represent spins in the form of vectors denoting the direction the magnetic moment faces. It turns out that allowing for all range of orientation of spins is unnecessary to observe interesting behaviour. For modelling purposes $ sigma_x = pm 1$ is usually sufficient. . Turning our attention to interacting pairs now, we can simplify significantly here by assuming a fully connected model. That is each atom interacts with every other. This is particularly useful since it allows us to use a range of mathematical &quot;tricks&quot; through mean field type approaches. This is the key to the Sherrington-Kirkpatrick model and what differentiates it from other models (which we will review in a later blog post). . This leaves us with one final assumption to make (excluding the external magnetic field): what values should $J_{xy}$ take. There are two options that we could use here to make our lives easier: the first is $J_{xy} = pm frac{J}{ sqrt{N}}$ - that is we select each interaction to be positive (ferromagnetic) or negative (antiferromagnetic) at random. Another option is to take values via a continuous probability distribution, typically a normal distribution owing to its &quot;nice&quot; mathematical properties. Typically we will want a mean of zero since we do not want the material to exhibit any magnetism in a ground state. We want to scale the standard deviation by $ frac{1}{ sqrt{N}}$ as in the bernoulli case. If we are using simulation we could try more &quot;exotic&quot; distributions (such as double fat-tail, assymetric distributions, etc) The scalings applied to the interaction weights is merely a convenience to allow us to scale the size of the spin glass in such a way as to allow population averages to remain comparable between glass size. . We will thus define the Sherrington-Kirkpatrick Hamiltonian as: $$ H_N = - sum_{x,y} J_{xy} sigma_x sigma_y $$ With $J_{xy} sim N(0, frac{s^2}{N})$ iid (value of $s$ is somewhat irrelevant as long as it&#39;s not too large/small). We have removed the external magnetic field as it isn&#39;t crucial for the story we are telling here. . An Alternate Interpretation . For those that are finding spin glasses and magnet terminology a little confusing or alien for this particular model we can introduce another interpretation in the form of the &quot;Dean Problem&quot;. Imagine you are the dean of a college hall, you have a number of students ($x, y$) who can like or dislike each other ($J_{xy}$). Your job is to place students in one of two groups ($ sigma = pm 1$) so as to maximise the overall happiness of the students ($ sum J_{xy} sigma_x sigma_y$). We can note that this maximization problem is equivalent to the energy minimization problem presented by the spin glass (i.e. the minimization of a negative quantity is equivalent to the maximization of its modulus) . Mathematical Analysis . The nice thing about the assumptions made by Sherrington-Kirkpatrick is that it allows for (comparatively) easy mathematical analysis. This is largely due its regularity (every atom looks like every other), by assuming arbitrarily large spin glasses we can also take limits and look at asymptotic behaviour. In other models this is not always possible and if it is it becomes increasingly more difficult. . First question we will ask is what is the minimum value attained by the Hamiltonian? This is equivalent to: $$M_N = max_{ sigma} sum J_{xy} sigma_x sigma_y = max_{ sigma} left[ - H_N right] $$ Where we are considering $N$ atoms in the system. We are using a slighlty sloppy notation for $max_{ sigma}$ to represent the maximum over all possible configurations. . Studying maximum quantities mathematically is often difficult. To overcome this difficulty we instead look at the Helmholtz free energy function ($F_N$) instead: $$F_N( beta) = frac{1}{N beta} mathbb{E} left[ log sum_{ sigma} exp(- beta H_N) right] $$ . We have done this since we can write the following inequality: $$ frac{1}{N} mathbb{E} left[ M_N right] leq F_N( beta) leq frac{1}{N} mathbb{E} left[ M_N right] + frac{log(2)}{ beta} $$ . This is a deceptively simple inequality, to see why it holds for the lower bound we replace the summation in $F_N$ by the maximum value in the sum (1 term). For the upper bound we replace every term in the sum with this attained maximum ($2^N$ values). The logs/exponents/beta/etc. all cancel leaving the result. This is useful to us because it means in the limit $ beta to infty$ we get the relation: $F_N( beta) to frac{1}{N} mathbb{E} left[ M_N right] $ which is what we are interested in studying. . This is closely related to the Gibbs distribution of the system. This gives us a probability distribution of states of the spin glass: $$G_N( sigma) = frac{exp(- beta H_N( sigma))}{Z_N( beta)} $$ Where $Z_N( beta)$ is the partition function, it is a normalizing constant (i.e. it is a sum over all possible terms of the numerator of $G_N$). We can think of the Gibbs distribution as weighting the configurations according to their free energy. We can see that finding the partition function is the crux of understanding the Gibbs distribution (and through comparison to the free energy the ground states). . One of the first ways this was investigated mathematically was to use a &quot;replica trick&quot;, this is just a result of using the identity: $$ln(Z) = lim_{n to 0} frac{Z^n - 1}{n} $$ . On the partition function. Essentially one takes $n$ independent copies (replicas) of the system and computes an average over all of them. Approximations are then used to take the limit $n$ to zero. For certain system behaviours this method works well but for others it can be inaccurate. In particular looking at very low temperatures (small beta near ground state) the approximations do not perform well. This method assumes certain symmetries between atoms which are not true in practice (due to taking independent replicas and averaging), these methods have been extended to &quot;replica symmetry breaking&quot; (RSB) methods. These methods were further superceded by the work of Parisi using variational principles. The mathematical details would take too long to put in a blog like this. Please see the references for links to papers on the topic. . What we are really interested in with spin glass systems is when a phase transition occurs. To do this physicists look at an order parameter which captures all behaviours of the system. Edwards and Anderson suggested the following order parameter: $$ q = frac{1}{N} sum_x hat{ sigma}_x^2 $$ Where $ hat{ sigma}_x$ represents the average over time of spin $x$. This order parameter is such that for $q=0$ the configuration is non-magnetic. For $q&gt;0$ then it is in a spin glass phase. Using the replica method (and some work!) we can show that under full symmetry we have that $q$ satisfying the self-consistency formula: $$ q = 1 - frac{1}{ sqrt{2 pi}} int_{- infty}^{ infty} exp(-z^2/2) sech^2( beta s sqrt{q} z) dz $$ Using this we can find a phase transition occurs at $T=s$ - temperatures below this the system exhibits glassy behaviour and above this the system is not magnetic (in equilibrium). . Unfortunately assuming this sort of symmetry this $q$ does not behave well at lower temperatures, it does not display all the characteristics of the system. . If we introduce symmetry breaking we re-write the order parameter of the form: $$ q_{ alpha beta} = frac{1}{N} sum_x hat{ sigma}^{ alpha}_x hat{ sigma}^{ beta}_x$$ For two states $ alpha$ and $ beta$ - This is also sometimes called the &quot;spin overlap function&quot;. If we consider the likelihood of observing a system state $ alpha$ as $W_{ alpha}$ (so that $ sum_{ alpha} W_{ alpha} = 1$) we can define the overlap density: $$P_ tilde{J}(q) = sum_{ alpha beta} W_{ alpha}W_ beta delta(q - q_{ alpha beta})$$ Where $ delta(.)$ is the Dirac delta function and the density is defined for some fixed realisation of interaction strengths $ tilde{J}$. Finally we can use this to define the Parisi order parameter function as: $$P(q) = int prod_{xy} Q(J_{xy}) P_ tilde{J}(q) dJ_{xy}$$ With $Q(J_{xy})$ representing the density by which the interaction strength is chosen (e.g. Gaussian). This order parameter does not suffer from the issues of symmetry like the order parameter function above. . In addition to uncovering a phase transition, the new Parisi order parameter uncovers some other interesting properties: . The broken symmetry of the spin glass requires an infinite number of order parameters to characterize | In the limit of large-N there is no self-averaging in the spin glass state. That is there exists distinct samples even as N approaches infinity. This is unlike most other systems where there is no concept of a &quot;sample&quot; when N increases. | Given 2 states of the spin glass, there is no &quot;inbetween&quot; spin glass state - this is called ultrametric structure. This gives the space of spin glass states a very interesting structure. In some sense the states are clustered. More than that it is clustered at any scale you look at - if you look at states within a distance of $d$ of each other you get a number of clusters, if you look at a larger scale $d&#39; &gt; d$ then these small clusters will merge into larger ones. (Distance here is defined using overlap of states). | . Simulation . After all the theory we will now look at a simple simulation of this model. . We start by doing a very naive Monte-Carlo method - we will generate random configurations, calculate the energy and a Gibbs measure. With the results we will estimate the average energy, the average magnetism, ground state energy and the partition function. If we want to find the ground state this would be a very bad method, for an $N$ size spin glass there will be $2^N$ possible configurations so finding any one will be difficult! We can implement this: . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Very naive Monte-Carlo approach import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N N = 1000 # Fix number of timesteps and some containers timesteps = 10000 gibbs = np.zeros(timesteps) energy = np.zeros(timesteps) mag = np.zeros(timesteps) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Fix Temperature for Gibbs distribution beta = 1/(s*0.5) for i in range(timesteps): configuration = np.random.choice([-1, 1], N) energy[i] = -1 * np.dot(configuration, np.dot(configuration, interaction)) / 2 gibbs[i] = np.exp(-beta*energy[i]) mag[i] = configuration.sum() print(&quot;Estimated Ground State Energy: &quot;, energy.min()) print(&quot;Estimated Average Energy:&quot;, energy.mean()) print(&quot;Estimated Partition Function:&quot;, gibbs.mean()) print(&quot;Estimated Average Magnetism:&quot;, mag.mean()) . Estimated Ground State Energy: -78.55263447711921 Estimated Average Energy: 0.1314391563670086 Estimated Partition Function: 1.7717002620362574e+64 Estimated Average Magnetism: 0.5468 . Next we take another bad method: a greedy hill climber (or greedy gradient descent). The idea behind this algorithm is that when we are in one configuration we pick a site at random and look at the change in energy associated with flipping the spin. If the energy is lower we accept the change, if higher we ignore and pick another site at random. This algorithm will converge to some local minima but it will not necessarily be a good global minima. We can code this up as: . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Greedy gradient descent import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 1000 spins = np.random.choice([-1, 1], N) # Fix number of timesteps and some containers timesteps = 100000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Calculate initial values mag[0] = spins.sum() energy[0] = -1 * np.dot(spins, np.dot(spins, interaction)) / 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1/(0.5*s) # Define update step dE = 0 dM = 0 def update(s_array, i_array): &quot;&quot;&quot; update function performs 1 update step to the model inputs: s_array - an array of N spins (+-1) i_array - an array of interaction strengths NxN &quot;&quot;&quot; global dE global dM _N = s_array.shape[0] old_s = s_array.copy() # Select a spin to update site = np.random.choice(_N, 1)[0] # Get interaction vector i_vector = i_array[site,:] # Calculate energy change associated with flipping site spin dE = 2*np.dot(i_vector, s_array)*s_array[site] dM = -2*s_array[site] # Sample random number and update site if dE &lt;= 0: s_array[site] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts , s_array, i_array): s_temp = s_array.copy() for i in range(ts): update_step = update(s_temp, i_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, interaction) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . We can see after about 25,000 steps the system is &quot;stuck&quot; in a local energy minima. If we re-ran the code starting from a different spot we would likely end up in a vastly different configuration. . The last method we will look at is the Metropolis-Hastings algorithm. Given a configuration of spins we will perform an update step by picking a site at random, we will compute the probability of flipping the spin and then accept/reject this change based on a random draw. This process will be repeated for a set number of steps. We will keep track of the energy of the system (the Hamiltonian) and the overall magnetism ($ sum_x sigma_x$). . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Updates occur at randomly selected sites import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 1000 spins = np.random.choice([-1, 1], N) # Fix number of timesteps and some containers timesteps = 100000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Calculate initial values mag[0] = spins.sum() energy[0] = -1 * np.dot(spins, np.dot(spins, interaction)) / 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1/(0.75*s) # Define update step dE = 0 dM = 0 def update(s_array, i_array): &quot;&quot;&quot; update function performs 1 update step to the model inputs: s_array - an array of N spins (+-1) i_array - an array of interaction strengths NxN &quot;&quot;&quot; global dE global dM _N = s_array.shape[0] old_s = s_array.copy() # Select a spin to update site = np.random.choice(_N, 1)[0] # Get interaction vector i_vector = i_array[site,:] # Calculate energy change associated with flipping site spin dE = 2*np.dot(i_vector, s_array)*s_array[site] dM = -2*s_array[site] # Calculate gibbs probability of flip prob = np.exp(-beta*dE) # Sample random number and update site if dE &lt;= 0 or prob &gt; np.random.random(): s_array[site] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts , s_array, i_array): s_temp = s_array.copy() for i in range(ts): update_step = update(s_temp, i_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, interaction) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . In this code we can see in the beginning the magnetism of the system fluctating and the energy decreasing. This stablises to some sort of &quot;quasi-equilibrium&quot;. . Since the energy always decreases it suggests the system is slowly finding its way to a local energy minimum rather than exploring to find a better energy minima. This is to be expected with such a basic implementation. To observe this better we will re-run the code for a second time from a different starting spot, we will compare the resulting spin array - we should notice that there is quite a large discrepency between the 2 runs - this shows that the system is settling down to a different local energy minima. . old_spins = spins old_energy = energy[timesteps] old_mag = mag[timesteps] spins = np.random.choice([-1, 1], N) #### Run Main Loop _main_loop(timesteps, spins, interaction) # Calculate a distance metric dist = ((old_spins * spins).sum() / N + 1) / 2 print(&quot;Proportion of sites with the same spin is:&quot;, dist) print(&quot;Resting energies of the 2 systems are:&quot;, old_energy, &quot;and:&quot;, energy[timesteps]) print(&quot;Resting magnetism of the 2 systems are:&quot;, old_mag, &quot;and:&quot;, mag[timesteps]) . Proportion of sites with the same spin is: 0.508 Resting energies of the 2 systems are: -608.7546315240614 and: -589.7510068842638 Resting magnetism of the 2 systems are: -84.0 and: 56.0 . The proportion of sites having the same spin is high (around 50%!) and the energy attained is different, suggesting the system has converged to 2 different local minima that are &quot;far apart&quot; from each other. If we want to find the global minima (or at least a &quot;better&quot; minima) we will have to adopt a better strategy. We will introduce some options when we look at the next spin glass model. The code above should act as a warning about blindly simulating and relying on computational power/time to solve complex problems: it doesn&#39;t always work! . We&#39;ll finish this blog post by looking at the Edwards-Anderson order parameter. We will use a basic numerical technique to solve the self consistency integral equation. . from scipy.integrate import quad def integrand(x, c): return np.exp(-x**2/2)*np.cosh(c*x)**(-2) n_approx = 100 beta_min = 0 beta_max = 2 beta_array = np.arange(n_approx + 1)*(beta_max - beta_min)/n_approx + beta_min q_array = np.zeros(n_approx+1) thresh = 0.001 n_max = 100 for i in range(n_approx+1): beta_tmp = beta_array[i] q_old = 0 q_tmp = 1 j = 0 while np.abs(q_old - q_tmp) &gt; thresh and j &lt; n_max: q_old = q_tmp c = beta_tmp*s*np.sqrt(q_old) I = quad(integrand, -np.inf, np.inf, args=(c)) q_tmp = 1 - I[0] / (np.sqrt(2*np.pi)) j =+ 1 q_array[i] = q_tmp plt.plot(beta_array, q_array) plt.xlabel(r&quot;$ beta$&quot;) plt.ylabel(&quot;q&quot;) plt.title(&quot;Edwards-Anderson Order Parameter (s=1)&quot;) plt.show() . This displays the behaviour we expect (approximately) for low beta below $1/s$ the temperature is above $s$ and so $q=0$ (non-magnetic) above this point the system is in the glassy phase. Since we have only approximated here there is some noise around the transition point. As we add more approximation points the transition should become sharper at $ beta = 1/s$. . Conclusion . In this blog post we have introduced the assumptions of the Sherrington-Kirkpatrick (fully connected Ising) spin glass model. We have seen that although fairly involved we can &quot;solve&quot; this model analytically to uncover its properties. We have also a basic implementation in Python - however as we noted this has bad convergence properties so shouldn&#39;t really be used other than for illustration. . References . This blog post was inspired by chapter 5 of &quot;Spin Glasses and Complexity&quot; by Daniel L Stein and Charles M Newman. . Some relevant papers include: . The original paper: &quot;Solvable Model of a Spin-Glass&quot; - Sherrington, Kirkpatrick 1975 | Summary of Parisi Method: &quot;The Sherrington-Kirkpatrick model: an overview&quot; - Panchenko 2012 | . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-2",
            "relUrl": "/spin-glass-models-2",
            "date": " â€¢ Mar 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Spin Glass Models",
            "content": "What is a Spin Glass? . A spin glass is not something you find down the pub (well it could be, but thats not what we&#39;re talking about here). Instead spin glasses are models of certain magnetic materials. Very loosely we can think of the atoms of a magnetic material as having a &quot;spin&quot; relating to the magnetic polarity (&quot;north&quot; or &quot;south&quot; ends of a bar magnet) - we typically call these up-spin and down-spin. In this context we use the term &quot;atom&quot; loosely, it may be an atom in a chemical sense but it could also be a molecule - essentially a minimal element of the material. . In a ferromagnetic material (such as iron) spins orient in the same direction. In contrast antiferromagnetic materials the spins orient to oppose each other. In a spin glass we have some ferromagnetic interactions and some antiferromagnetic interactions, we say the system is &quot;disordered&quot;. . . In the image above we can see various spin configurations. The spins are indicated by arrows (yellow for up and green for down) the ferromagnetic interactions are denoted by blue lines while antiferromagnetic interactions by red. However it is somewhat misleading to think of these occuring only within a square lattice in 2d such as this. Spins also do not have to be 180 degree rotations of each other - they can be at arbitrary angles from each other (but this would make for a messy diagram). A spin glass can also occur in arbitrarily many dimensions, and the interactions do not only have to occur between &quot;nearest neighbours&quot; any atom can have any number of interacting partners. . We can represent the energy of a spin glass system using the Hamiltonian: $$ H = - sum_{x,y} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ Where $J_{xy}$ denotes an interaction strength between atoms $x$ and $y$. $ sigma_x$ represents the spin/magnetism of an atom $x$ (can be a vector in which case all products are dot products). A system will tend to a state of lowest energy and so $J_{xy} &gt; 0$ represents a ferromagnetic interaction (minimised for pairs of matching spin) and $J_{xy} &lt; 0$ for antiferromagnetic interactions. The range over which the sum applies has been left purposely ambiguous as to allow for various lattice topologies. The second summation reflects an interaction with an external magnetic field (denoted by $h$) if there is no external field the term can be ignored. (It is worth noting that magnetic spins for a specific atom can only take one of two values (rotated through 180 degrees) since electrons have half integer spin) . This allows us to define an important term relating to spin glasses. We can note that at a minimal energy (&quot;ground state&quot;) the spin glass will not be &quot;ordered&quot; (it will appear &quot;random&quot;). We call this property &quot;quenched disorder&quot; - this is due to the similarity to glass materials that are essentially cooled down liquids that get &quot;frozen&quot; into a state of disorder. . It is important to note this is different to pure &quot;randomness&quot;. If we think of a continuum whereby we have complete order on one side (think of a crystalline type structure as an example) and pure randomness on the other the spin glass lives somewhere in the middle - partially structured and ordered but partially random. This is a particularly interesting place to be: in the pure ordered end of the scale there are many established mathematical tools to deal with this situation. Similarly in a pure random situation probability and statistics can provide us tools for study. In the middle things get complicated since you cannot necessarily assume that any one element will &quot;look the same&quot; as any other - thus mean field methods fall down. This occurs in many &quot;real world&quot; phenomena, this can result in a lack of study since it &quot;falls inbetween&quot; different disciplines. . . There is another important term relating to spin glasses called &quot;frustration&quot;. This is where an atom has interactions with other atoms that are in conflict with each other - one interaction would suggest a lowest energy state for the atom is an up spin and another interaction suggests a down spin. An example of this can be seen below: . . We can see that the spin of the centre atom is not clearly defined by interaction with its neighbours. The vertical neighbours interactions suggest the lowest energy state would be a yellow up spin, while horizontal neighbours suggest a green down spin. In a large spin glass with random (or nearly random) configuartions there may be many such frustrated atoms. This gives rise to complexity and questions such as &quot;what is the lowest energy state for a system?&quot; becomes very difficult to answer. In many cases we are not able to determine this analytically. The energy landscape (the Hamiltonian energy for a given configuration of spins given a fixed topology of interactions) can become very complex with lots of local minima, which means &quot;typical&quot; optimization procedures based around greedy hill climbing (and the like) will struggle to find the global minimum. See the plot of energy landscape below as an example: . . If one ends up in a configuration near one of these local minima it requires relatively large changes to the configuration to escape the valley. This leads to a kind of &quot;metastability&quot; in the system where the configuration will &quot;stick&quot; around these points for a long time. As such spin glasses tend to violate the Ergodic principle, this again adds to the mathematical complication in dealing with these systems. . Although the system would &quot;prefer&quot; to be in a lower energy state through the application of a temperature (or placing the system within an external magnetic field) the atoms can have sufficient energy to escape this lower energy state. For high enough temperatures this means a ferromagnetic material can become antiferromagnetic. In most cases there exists a critical temperature where a phase transition occurs. Phase transitions are interesting examples of emergence - one example of a phase transition that everybody is familiar with is the phenomena of melting a solid to create a liquid. It is interesting that this is a very sharp transition - why is it not the case that a solid gradually becomes &quot;softer&quot; and more liquid like? Instead small temperature fluctuations can cause the state of matter to change. It is not immediately obvious why this is the case, other phase transitions exist in other systems and they are often interesting to analyse. . The eagle eyed amongst you may notice that we have ignored the interference between spins themselves. It is true that this will have an impact but in most mathematical models of spin glasses it can be ignored. As with all mathematical models we look for a &quot;minimal description&quot; that captures the behaviour of interest, it turns out that this complication tends not to add much to the model (although I&#39;m sure there exists research with interesting results capturing this interference). . So what are some physical examples of a spin glass in the real world? Technically any iron magnet subject to rust (which is antiferromagnetic) will be a spin glass, however typically the ferromagnetic atoms will still be so prevelent that we can think of it as a ferromagnet. There are other &quot;exotic&quot; molecules (e.g. europium strontium sulphide) that are spin glasses also. Many of the experiments on spin glasses involve melting down a noble metal (e.g. gold or silver) and adding a small amount of dispersed molten iron (typically around 0.1-5%) and cooling the mixture very quickly. Many counter-intuitive and contradictory properties have been found through these experiments including: . By cooling quickly one can avoid the transition from liquid to solid - creating a viscous liquid spin glass | Relaxation times (how long it takes the system to adjust to changes in temperature) can be very slow, way beyond experimental time frames | Interactions with magnetic fields are odd. Absent of a magnetic field a spin glass is not magnetic. By carefully applying and removing external magnetic fields one can create a magnetic spin glass with varying properties (decays, apparent permanence etc.) | Spin glasses appear to have a &quot;memory&quot; of previous states and undergo something akin to an aging process Creating theoretical explanations of these (and other) phenomena is the subject of much research on the subject. | . Why do we care? . Ok, so at this point we may have a base understanding of what a spin glass is and some of the complications and properties therein, but you may be thinking: &quot;but who cares about magnets anyway?&quot; (Unless of course you are Charlie Kelly) It does seem like a lot of work and as a non-physicist it might seem interesting. However in dealing with the complications of spin glasses we can gain a lot of insight into other systems. In the next few bullet points I will try and convince you that it is worth time playing around with spin glasses: . They&#39;re interesting! - Spin glasses exhibit a number of properties that I personally find very interesting, for example: emergent behaviour, &quot;in between&quot; order and randomness, simple concepts to explain but difficult to write down mathematically, etc. | Non-ergodic systems are everywhere! - Although spin glasses themselves are quite stylised if they can give insight into the behaviour of non-Ergodic systems this is very useful. Loosely speaking an Ergodic system does not exhibit path depedence (e.g. whatever the state at present eventually any other state can be reached). When looking at complex systems this is typically not the case. | Frustration occurs more than we would like - We often end up in the situation with &quot;conflicting&quot; information and dealing with this gives rise to many opportunities. | They&#39;re easy to simulate - while some of the properties above make mathematical analysis difficult in all but a few special cases, spin glasses are fairly easy to code up and simulate. If you wonder &quot;what would happen if....?&quot; you can quickly modify a model and play around to see what happens, you don&#39;t need to spend much time thinking about boundary/initial conditions or other technical aspects. | There are many different applications - Given the ubiquity of some of the complications relating to spin glasses the techniques and theory have been applied to many situations including (but not limited to): optimization techniques, neural networks (biological and artificial), machine learning, protein folding, materials science, evolutionary models. The study of quantum spin glasses is also fairly active with applications in quantum computing. | . Conclusion . In this blog post we have been on a whistle-stop tour of the very basic concepts of spin glasses and models of spin glasses. We have seen some of the difficulties with them and what makes them interesting and useful to study. In future blog posts we will look at specific models and mathematical techniques used to study them. . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models",
            "relUrl": "/spin-glass-models",
            "date": " â€¢ Mar 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Standing Ovation Model",
            "content": "Standing Ovations - as a Phenomena . Although not particularly &quot;exciting&quot; for a subject to model, standing ovations have many interesting properties. A truly fantastic performance may elicit an instantaneous standing ovation whereby all audience members instinctively decide to stand. However this cannot be the only factor, we can observe in other performances a few hardcore fans may initially stand and then pass through the crowd as a &quot;wave&quot; through social pressure (or perhaps the inability to see the stage if others in front of you stand!) Eventually resulting in a standing ovation. At other times maybe only a handful of audience members stand, notice very few people around them are standing and then sheepishly sit down again. There is a clear temporal aspect to standing ovations but there is also some sense of criticality whereby a phase transition occurs. While understanding the mechanisms behind this for standing ovations may not have much &quot;value&quot; in of itself these principles turn up in many complex systems and so understanding here can be transferable to more &quot;interesting&quot; situations. . Modelling Standard Ovations . We now move onto the question: how can we model standing ovations? We first consider the initial (instinctive) decision of standing or not following a performance. Clearly for a &quot;perfect&quot; performance (however that may be defined) everybody would show their appreciation and stand. However when the performance is not &quot;perfect&quot; why do some stand and some not? We could model each audience member as having some &quot;error function&quot; that clouds their judgement of the performance, so for (objectively) a good performance we could have that some percieve the performance as bad owing to their error function, and so they decide not to stand. We can express this mathematically: we denote the performance quality as $P$. Each individual has an error-rate $ epsilon$ - this defines a signal $S$ as: $$ S = P + epsilon $$ We can then state that an individual will stand for a signal that is in excess of their threshold $T$. We can take this threshold to be fixed for all individuals by adjusting their $ epsilon$ accordingly. . But what does the error-rate represent? There are a number of interpretations. For example it could represent &quot;stubborness&quot;, some may be difficult to impress. It could also represent differences in knowledge, somebody unfamiliar with jazz-club etiquette may stand/stay seated at the wrong times whereas somebody more seasoned will be more closely tied to the quality of the performance. . For arguments sake let&#39;s say that $P$ can vary from 0 to 1. The error rates are uniformly distributed. We can then code up this initial standing ovation model as below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set audience size N = 50 M = 100 # Fix performance quality as 50% P = 0.5 # Fix seed np.random.seed(123) # Fix all audience as seated (0) initially audience = np.zeros((N, M)) # Set error functions for audience members epsilon = np.random.random((N, M)) - 0.5 def initial_stand(error, p, t): signal = p + error out = signal &gt; t return out * 1 # We set up an array to vary the threshold T = np.arange(100) / 100 Pct = np.zeros(100) # Loop over all thresholds and plot the percentage of audience standing for i in range(100): Pct[i] = initial_stand(epsilon, P, T[i]).mean() plt.plot(T, Pct) plt.xlabel(&quot;Threshold&quot;) plt.ylabel(&quot;Percent Standing&quot;) plt.show() . As expected we get a straight line decreasing with threshold. From the discussion before we know that standing ovations are not entirely determined by the performance quality (and resulting threshold) behaviour. There is a social aspect to the phenomena. We now move onto looking at a temporal model. One way we could do this is that an individual takes a survey of all their neighbours and if enough of them are standing they also decide to stand (somewhat similar to the movement mechanism in a Schelling Segregation Model). However if you are sat in the audience it is unlikely you&#39;ll turn round to see what the people behind you are doing, you will only notice those in front of you. You may also notice those from many rows in front of you rather than just proximal neighbours. Page/Miller suggest a viewing &quot;funnel&quot; where each agent can see as: . Where we can adjust the number of rows somebody can see as a paramter (with 3 presented here). We will make a small adjustment to this method; let&#39;s place more weight on the audience members closest to us. We will calculate the proportion standing in each row in or field of vision and take a weighted sum of these according to the square distance away (e.g. row 2 will be weighted 1/4 of row 1, row 3 will be 1/9 weighted and so on.) This should capture the behaviour that there is more social pressure from those around us, this might lead to a slow &quot;wave&quot; propagating through the audience as this pressure grows. For this model lets also assume that the lights are on in the theatre and we can see all rows up to the front in detail. . For this &quot;funnel&quot; we will ignore the proximal &quot;next door&quot; neighbours from this calculation. We will assume there is a different mechanism for these individuals, we will look at the proportion of neighbours standing (either 0, 0.5 or 1) and we will apply a &quot;neighbour weight&quot; which will weight between this score and the funnel score to give an overall social pressure score as: $$ Social _Pressure = (1 - neighbour _weight) times Funnel _Pressure + neighbour _weight times Neighbour _Pressure $$ . We will use this mechanism of seperating out next door neighbours since if we go to a performance with others they will typically be sat next to us. Further one could argue that somebody is more likely to stand/sit based on their friend/partner&#39;s behaviour than those of a stranger. The neighbour weight parameter allows us to vary the relative importance. . Using the overall social pressure metric a sitting individual will stand if the social pressure exceeds a fixed threshold, similarly a standing individual will sit if the social pressure is smaller than one-less the fixed threshold. We are assuming that there is a symmetry invoked here which may not be the case. For example the &quot;embarrassment&quot; of standing while others are sitting might mean people are quicker to sit if they&#39;re in a small minority standing. However for a first quick and dirty implementation of a model the symmetry assumption will suffice (it will be easy enough to modify later should we desire). . We will also add 2 random components to this model: the first being a rate that if one is sitting down they stand up regardless of the information presented. The other being the reverse of this: a rate by which individuals sit down if they&#39;re standing. We will denote these rates as $ delta, gamma$ respectively. This might allow for the possiblity of a &quot;spontaneous ovation&quot; occuring. . We now have one choice remaining before we can implement this model: how will the updates be made? There are 2 main classes of updates in an agent based model: asynchronous and synchronous. The former essentially means each agent updates &quot;one by one&quot;, the order of upates can be random or by some defined order (in this example we could update those nearest the stage and iterate backwards for example). In a synchronous updating scheme all agents updated their state together once per time-step, of course by doing this one has to be careful in some situations (e.g. if there is an asset that can get depleted who gets to use it first?) but in this simple model there are no such concerns. For this model I think synchronous modelling is appropriate since I believe this is how ovations work in practice (e.g. we do not wait for &quot;our turn&quot; to make a decision), we could investigate how this affects the outcome later if we wish to. . My implementation of this model can be seen below. We will look at how the proportion of audience members standing evolves over time (functions use njit decorator to simply improve run time - looping over the array multiple times in the funnel calculation is fairly slow in pure python): . import numpy as np from numba import njit import matplotlib.pyplot as plt %matplotlib inline # Set audience size N = 50 M = 100 # Fix performance quality as 50% P = 0.5 # Fix Performance threshold T = 0.6 # Fix seed np.random.seed(123) # Fix all audience as seated (0) initially audience = np.zeros((N, M)) # Set error functions for audience members epsilon = np.random.random((N, M)) - 0.5 # Fix number of timesteps T_steps = 100 Pct_Hold = np.zeros(T_steps+1) # Fix social pressure threshold Pressure_T = 0.4 # Fix neighbour weight N_weight = 0.5 # Fix probaility of spontaneous standing delta = 0.1 # Fix probability of spontaneous sitting gamma = 0.05 # Calculate initial reactions to performance @njit def initial_stand(error, p, t): signal = p + error out = signal &gt; t return out * 1 # Function to calculate social pressure @njit def pressure(i, j, aud): rows = aud.shape[0] cols = aud.shape[1] pct_sum = 0 norm = 0 for x in range(i): active_row = i - x - 1 left = max(j - (x+1), 0) right = min(j + (x+1), cols) pct_sum += aud[active_row, left:right+1].mean() / (x+1)**2 norm += (x+1)**-2 if norm == 0: res = 0 else: res = pct_sum / norm return res # Calculate pressure from (nextdoor) neighbours @njit def neighbour_pressure(i, j, aud): cols = aud.shape[1] count = 0 left = 0 right = 0 if j != 0: left = aud[i, j-1] count += 1 if j != cols -1: right = aud[i, j+1] count += 1 return (left + right) / count # Calculate overall pressure score @njit def pressure_score(i, j, aud, n_weight): return pressure(i, j, aud)*(1 - n_weight) + neighbour_pressure(i, j, aud)*n_weight # Spontaneous sitting/standing function @njit def spontaneous(i, j, aud, dlt, gma): rnd = np.random.random() if aud[i,j] == 0 and rnd &gt; (1-dlt): aud[i,j] = 1 elif aud[i,j] == 1 and rnd &gt; (1-gma): aud[i,j] = 0 return aud # Main Code Loop audience = initial_stand(epsilon, P, T) Pct_Hold[1] = audience.mean() for x in range(2, T_steps+1): audience_old = audience.copy() for i in range(N): for j in range(M): up_score = pressure_score(i, j, audience_old, N_weight) down_score = pressure_score(i, j, 1 -audience_old, N_weight) if up_score &gt; Pressure_T and audience_old[i,j] == 0: audience[i,j] = 1 if down_score &gt; Pressure_T and audience_old[i,j] == 1: audience[i,j] = 0 spontaneous(i, j, audience, delta, gamma) Pct_Hold[x] = audience.mean() plt.plot(Pct_Hold) plt.xlabel(&quot;Time Step&quot;) plt.ylabel(&quot;Percent Standing&quot;) plt.show() . By selecting particular parameters we can see a few different behaviours in the model. These include an inital peak of enthusiasm that dies down, a gradual increase as the applause trickles from the front of the auditorium to the back and a rapturous applause leading to nearly all participants standing up very quickly. These are all behaviours that we do see in real life audiences. If we were particularly motivated we could look for real world phenomena in standing ovations and see if these rules are able to replicate them. . Potential Improvements . With agent based models there are always &quot;extra&quot; things that could be added/modified. A few select suggestions include: . Revisit the &quot;funnel&quot; mechanism and compare various schemes | What happens if there are various levels/balconies or perhaps aisles between seats? | What happens with &quot;stooges&quot;? That is if we add agents who always applaud regardless, what is the minimum number required to guarantee a standing ovation? Where are they best placed (perhaps taking into account ticket cost - 10 in the middle compared to 2 at the front), etc. | We could code in &quot;partners&quot; or &quot;groups&quot; more precisely to investigate the impact that has | How does changing to asynchronous updating change the behaviour? | What if every so often an agent &quot;turns round&quot; to view the seats behind them? | and so on | . Conclusion . We have seen how we can code up a basic agent based model for standing ovations. While the subject itself isn&#39;t particularly &quot;useful&quot; in of itself the principles are common to many other systems. We have also seen some of the considerations that go into an agent based model. . As a final remark it is worth noting that this is not how you would produce an agent based model in practice. Typically you would start with a simple model and add features gradually and test the impact, I did not want to write a series of long posts on this model so I &quot;skipped to the end&quot; coding in what I thought some pertinent features could be all at once (although I did not spend long thinking about them!) . References . The original paper on standing ovation models presented by Scott Page and John Miller (can be seen here! for those wanting to read more about their version of the model. .",
            "url": "https://www.lewiscoleblog.com/standing-ovation",
            "relUrl": "/standing-ovation",
            "date": " â€¢ Feb 25, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Barnes-Hut Algorithm",
            "content": "The N-Body Problem . It is well understood that for $N &gt; 2$ that the problem of determining trajectories under gravitation for N-bodies cannot be solved analytically. Instead we must rely on computation to create trajectories. From Newton&#39;s equations we can write the force $ pmb{F_{i,j}}$ between two bodies $i, j$ with masses $m_i, m_j$ and positions $ pmb{p_i}, pmb{p_j}$ as: $$ pmb{F_{i,j}} = frac{G m_i m_j ( pmb{p_i} - pmb{p_j})} { lVert pmb{p_i} - pmb{p_j} rVert^3} $$ . For $N$ bodies we therefore have $ frac{N(N-1)}{2}$ forces to be calculated. For small $N$ this is not a problem, however if we want to model galaxy dynamics with thousands (or more) of bodies this becomes a computational issue. . Barnes Hut Process . With Barnes Hut we can move from $O(N^2)$ complexity to $O(Nln(N))$ - this is a significant improvement for large $N$. . The crux of the Barnes Hut scheme is to approximate the effect of &quot;far away&quot; bodies but calculate exactly the forces attributed to &quot;near&quot; bodies. To do this a quadtree data structure is used (in 3d an octree or higher dimensions a $2^d$tree). To do this the space is partitioned into 4 equal squares at depth 1 (or 8 cubes in 3d or $2^d$ hypercubes in higher dimension) - at depth 2 each of these squares is then partitioned into 4 equal squares and so on. From this a tree is constructed with each node representing the number of bodies within the quadrant at that depth. This is repeated recursively until a depth is reached where each quadrant has 1 or 0 bodies in it. It is easier to understand this with a visual example: . . With this data structure in place we can begin to simulate. For each quadrant in the quadtree we can calculate a centre of mass (in the obvious way). Then given a body we can calculate the force acting on it from a given quadrant by applying the Newtonian force calculation using the centre of mass. We could, for example, apply this to the 4 quadrants at depth 1 to approximate the force acting on the body. This of course might not be particularly accurate if there are many bodies present. To overcome this the Barnes Hut algorithm specifies a critical distance $ theta$ if the distance beween the body and the centre of mass of the quadrant is greater than $ theta$ then it is used as an approximation, if not the algorithm moves to the next depth of the quadtree and tries again, this happens recursively until the distance becomes below $ theta$ or there is only 1 body in the quadrant (as such setting $ theta = 0$ returns to the $O(N^2)$ brute force approach). . While this is presented as an algorithm for the N-body problem it can be used in other situations that involve a spatial dimension and many entities. I am currently considering an application to speed up an agent based model (potentially the subject of another blog post). . Implementation . Initially I will code up an example in Python, I may revisit this and create a more efficient implementation in Cython/C++/etc. I will ignore all &quot;complications&quot; for example I will not give each body a size nor will it consider collisions, etc. Each body is a &quot;point mass&quot; in this example. For now I&#39;ll also ignore the issues relating to plotting trajectories. . We first create a node class to represent nodes within the quadtree. Using this data structure we will then apply a verlet time step to calculate positions: . from copy import deepcopy import numpy as np class node: &quot;&quot;&quot; A class for a node within the quadtree. We use the terminology &quot;child&quot; for nodes in the next depth level - consistent with tree nomenclature If a node is &quot;childless&quot; then it represents a body &quot;&quot;&quot; def __init__(self, x, y, px, py, m): &quot;&quot;&quot; Initializes a childless node m - Mass of node x - x-coordinate centre of mass y - y-coordinate centre of mass px - x- coordinate of momentum py - y-coordinate of momentum pos - centre of mass array mom - momentum array child - child node s - side-length (depth=0 s=1) relpos = relative position &quot;&quot;&quot; self.m = m self.pos = np.array([x,y]) self.mom = np.array([px,py]) self.child = None def next_quad(self): &quot;&quot;&quot; Places node in next quadrant and returns quadrant number &quot;&quot;&quot; self.s = 0.5*self.s return self.divide_quad(1) + 2*self.divide_quad(0) def divide_quad(self, i): &quot;&quot;&quot; Places node in next level quadrant and recomputes relative position &quot;&quot;&quot; self.relpos[i] *= 2.0 if self.relpos[i] &lt; 1.0: quadrant = 0 else: quadrant = 1 self.relpos[i] -= 1.0 return quadrant def reset_quad(self): &quot;&quot;&quot; Repositions to the zeroth depth quadrant (full space) &quot;&quot;&quot; self.s = 1.0 self.relpos = self.pos.copy() def dist(self, other): &quot;&quot;&quot; Calculates distance between node and another node &quot;&quot;&quot; return np.linalg.norm(self.pos - other.pos) def force_ap(self, other): &quot;&quot;&quot; Force applied from current node to other &quot;&quot;&quot; d = self.dist(other) return (self.pos - other.pos) * (self.m * other.m / d**3) def add_body(body, node): &quot;&quot;&quot; Adds body to a node of quadtree. A minimum quadrant size is imposed to limit the recursion depth. &quot;&quot;&quot; new_node = body if node is None else None min_quad_size = 1.e-5 if node is not None and node.s &gt; min_quad_size: if node.child is None: new_node = deepcopy(node) new_node.child = [None for i in range(4)] quad = node.next_quad() new_node.child[quad] = node else: new_node = node new_node.m += body.m new_node.pos += body.pos quad = body.next_quad() new_node.child[quad] = add_body(body, new_node.child[quad]) return new_node def force_on(body, node, theta): if node.child is None: return node.force_ap(body) if node.s &lt; node.dist(body) * theta: return node.force_ap(body) return sum(force_on(body, c, theta) for c in node.child if c is not None) def verlet(bodies, root, theta, G, dt): for body in bodies: force = G * force_on(body, root, theta) body.mom += dt * force body.pos += dt * body.mom / body.m def model_step(bodies, theta, g, step): root = None for body in bodies: body.reset_quad() root = add_body(body, root) verlet(bodies, root, theta, g, step) ########## Main Code ########## # Parameters Theta = 0.7 G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X0 = np.random.random(N_bodies) Y0 = np.random.random(N_bodies) PX0 = np.random.random(N_bodies) - 0.5 PY0 = np.random.random(N_bodies) - 0.5 # Initialize Bodies = [node(x0, y0, pX0, pY0, masses) for (x0, y0, pX0, pY0, masses) in zip(X0, Y0, PX0, PY0, Masses)] # Main Model Loop def Model_Loop_BH(n): for i in range(n): model_step(Bodies, Theta, G, dt) %timeit Model_Loop_BH(N_steps) . 7.97 s Â± 446 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) . This code is not the most efficient possible for this model (in all likelihood one would only use Barnes-Hut for large galaxy simulations and so it&#39;s unlikely that Python would be the best choice). For 100 bodies and 1000 time steps this code runs on my machine in around 8s. . We can also code up a &quot;brute force&quot; approach (calculating forces between all pairs of bodies) to compare, it is likely that the cost of computing the quadtree could be more costly than just computing all forces directly when the number of bodies is suitably small. We could run these codes multiple times to find where this is the case. . import numpy as np # from numba import njit G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X = np.random.random(N_bodies) Y = np.random.random(N_bodies) PX = np.random.random(N_bodies) - 0.5 PY = np.random.random(N_bodies) - 0.5 pos = np.array((X,Y)) mom = np.array((PX, PY)) #@njit def force_array(pos_arr, m_array): n = pos_arr.shape[1] force_arr = np.zeros((2 ,n, n)) for i in range(n): for j in range(i): force_arr[:, i, j] = G * m_array[i] * m_array[j] * (pos[:,i] - pos[:, j]) / np.abs((pos[:,i] - pos[:, j]))**3 force_arr[:, j, i] = - force_arr[:, i, j] return force_arr #@njit def update_mom(step, mom_arr, force_arr): n = mom_arr.shape[1] del_mom = np.zeros_like(mom_arr) for i in range(n): for j in range(n): del_mom[:, i] += step * force_arr[:, i, j] return mom_arr + del_mom #@njit def update_pos(step, pos_arr, new_mom, m_arr): return pos_arr + step * new_mom / m_arr #@njit def main_loop(n, pos_arr, mom_arr): for i in range(n): force = force_array(pos_arr, Masses) mom_arr = update_mom(dt, mom_arr, force) pos_arr = update_pos(dt, pos_arr, mom_arr, Masses) return pos_arr %timeit main_loop(N_steps, pos, mom) . 1min 18s Â± 1.89 s per loop (mean Â± std. dev. of 7 runs, 1 loop each) . This brute force code is not implemented in a particularly efficient way, however it is good enough for illustration purposes. Even for relatively small model sizes (100 bodies for 1000 time steps) the code takes considerably longer than the Barnes-Hut algorithm, with timeit giving an approximate timing of around 78s - about 10 times slower than the Barnes-Hut implementation. . If we use Numba with the njit decorator we can improve this to 3s - which while quicker is not considerably better than the Barnes-Hut algorithm. As the number of bodies increases (and perhaps some fine tuning of the Barnes-Hut parameters) we would expect even more drastic improvements. . import numpy as np from numba import njit G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X = np.random.random(N_bodies) Y = np.random.random(N_bodies) PX = np.random.random(N_bodies) - 0.5 PY = np.random.random(N_bodies) - 0.5 pos = np.array((X,Y)) mom = np.array((PX, PY)) @njit def force_array(pos_arr, m_array): n = pos_arr.shape[1] force_arr = np.zeros((2 ,n, n)) for i in range(n): for j in range(i): force_arr[:, i, j] = G * m_array[i] * m_array[j] * (pos[:,i] - pos[:, j]) / np.abs((pos[:,i] - pos[:, j]))**3 force_arr[:, j, i] = - force_arr[:, i, j] return force_arr @njit def update_mom(step, mom_arr, force_arr): n = mom_arr.shape[1] del_mom = np.zeros_like(mom_arr) for i in range(n): for j in range(n): del_mom[:, i] += step * force_arr[:, i, j] return mom_arr + del_mom @njit def update_pos(step, pos_arr, new_mom, m_arr): return pos_arr + step * new_mom / m_arr @njit def main_loop(n, pos_arr, mom_arr): for i in range(n): force = force_array(pos_arr, Masses) mom_arr = update_mom(dt, mom_arr, force) pos_arr = update_pos(dt, pos_arr, mom_arr, Masses) return pos_arr %timeit main_loop(N_steps, pos, mom) . 3 s Â± 33.1 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) . Conclusion . We have seen that even for relatively modest models the Barnes-Hut algorithm provides fairly efficient computation for the N-body problem. We would expect the improvement to be even more marked for larger models. However Python on its own is not the best place to implement this and a C++ implementation would offer better performance. With some work a Cython approach should be possible and offer performance improvements. The code would require some major re-working if Numba is to be used instead owing to the reliance on classes to generate the quadtree. .",
            "url": "https://www.lewiscoleblog.com/barnes-hut",
            "relUrl": "/barnes-hut",
            "date": " â€¢ Feb 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Cython Vs Numba: An Example",
            "content": "Following on from the previous blog post here comparing the relative merits of Cython Vs Numba I thought I&#39;d illustrate this with implementations of a relatively simple model: a vanilla 2d Ising Model. This is a prime target for a performance boost since it is a very &quot;loopy&quot; code. I will not cover what the Ising Model is/how it works (you can check that here!). It is a very interesting model and I may return to it at a later date to look at some of its properties and explore it more deeply (and maybe spin glass models more generally). For now it will just be a test subject to explore performant python type code. . In this blog I will present a few different versions, I won&#39;t throw the kitchen sink at them to get the absolute best performance but will adopt an 80/20 principle. I will not try any parallelisation or clever memory management outside of what comes pre-canned. . Python . This is a basic python implementation using a lot of looping. Even for a relatively small models this code will likely be fairl slow due to the looping. . import numpy as np # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours def neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res def dE(i, j, spin_array): return 2*spin_array[i, j]*neighbour_sum(i, j, spin_array) def update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array def _main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = update(spin_tmp) return spin_tmp . Numba . For this code we will just take the above code and use the njit decorator (forcing the use of LLVM - a jit decorator will fall back to Python object mode if it cannot work out how to use LLVM). This is literally a few seconds of coding updates. . import numpy as np from numba import njit # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @njit def nb_neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res @njit def nb_dE(i, j, spin_array): return 2*spin_array[i, j]*nb_neighbour_sum(i, j, spin_array) @njit def nb_update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = nb_dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array @njit def nb_main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = nb_update(spin_tmp) return spin_tmp . Cython . Again we will modify the python code. We will see that while not too onerous it does require a little more work than the Numba example. The code is largely boilerplate but requires a little more thinking than the Numba example (e.g. in implementing this I initially forgot a static type definition of 1 variable which was causing a 300% increase in runtime). . %load_ext Cython . %%cython cimport cython import numpy as np cimport numpy as cnp from libc.math cimport exp from libc.stdlib cimport rand cdef extern from &quot;limits.h&quot;: int RAND_MAX # Grid size: N (int) NxN square latice cdef int N = 1000 # Fix Temp cdef float kT = 2 / np.log(1 + np.sqrt(2)) cdef float kTinv = 1 / kT # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_neighbour_sum(int i, int j, cnp.int32_t[:, :] spin_array): cdef int north = 0 cdef int south = 0 cdef int east = 0 cdef int west = 0 cdef int max_height = spin_array.shape[0] cdef int max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] cdef int res = north + south + east + west return res @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_dE(int i, int j, cnp.int32_t[:, :] spin_array): return 2*spin_array[i, j]*cy_neighbour_sum(i, j, spin_array) @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_update(cnp.int32_t[:, :] spin_array): cdef int height = spin_array.shape[0] cdef int width = spin_array.shape[1] cdef int y_offset, x_offset cdef int i, j cdef int dEtmp for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = cy_dE(i, j, spin_array) if dEtmp &lt;= 0: spin_array[i, j] *= -1 elif exp(-dEtmp * kTinv) * RAND_MAX &gt; rand(): spin_array[i, j] *= -1 return spin_array @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_main_code(int M, cnp.int32_t[:, :] spin_array): cdef int x for x in range(M): spin_array = cy_update(spin_array) return spin_array @cython.boundscheck(False) @cython.wraparound(False) cpdef cnp.int32_t[:, :] cpy_main_code(int M, cnp.int32_t[:, :] spin_array): return cy_main_code(M, spin_array) . Timing Results . # Python %timeit _main_code(10, spins) # Numba %timeit nb_main_code(10, spins) # Cython %timeit cpy_main_code(10, spins) . 52 s Â± 2.53 s per loop (mean Â± std. dev. of 7 runs, 1 loop each) 244 ms Â± 4.48 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) 395 ms Â± 6.92 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) . For our purposes the %timeout magic will be good enough a proxy for performance. We can see that the intial python implementation is very slow. For a 1000x1000 lattice and looping over 10 times, for my machine the python interation takes 50-55s. This would be fairly problematic if we wanted to sweep over the parameter space, find critical temperatures, perform random seed analyses etc. . In contrast Numba only takes 240-250ms, an impressive 2000% speed up. Running the code multiple times now seems much less onerous. . Cython is not quite as quick as the Numba implementation taking 390-400ms but still represents a significant speedup compared to Python. For practical applications the difference between Numba and Cython in this case may be insignificant. . It is worth noting that these times are from my machine on at a specific time. The times achieved on your machine might be slightly different. Similarly changing the size of the lattice may change the ordering of which option is &quot;quickest&quot; - as always it&#39;s worth checking the code how it will be used rather than performing a benchmark like this for determining which option to use. . Conclusion . From the results above it may be tempting to claim Numba is the obvious choice given it is not only easier to implement than cython but also offers faster speeds. However I selected the 2d Ising model as an example since I knew the code would work well in Numba (in a sense I have been p-value hacking the experiment!) In certain situations (e.g. a code relying very heavily on class structures) Numba is either unusable or requires a complete code overhaul whereas cython can require only a few lines of boilerplate code. . In other examples you can also see that Cython can severely outperform Numba, I am not sure why this is and the only real way to determine which will perform better is to perform testing (if somebody has an explanation/heuristic I&#39;d love to hear it). It is also possible to interface numba and cython which has been useful to me in the past. For a quick example suppose we want to perform an inverse transform of a Beta(2,0.5) distribution: . from scipy.stats import beta x = beta.ppf(0.1, 2, 0.5) x . 0.4681225665264196 . This cannot be optimised in Numba as it is (beta.ppf is currently not supported functionality - this may change by the time you read this). However we can take the address of the cython special function that this calls. We can then build the function in such a way as it can be seen by Numba: . import ctypes from numba import types, njit from numba.extending import get_cython_function_address betaaddr = get_cython_function_address(&quot;scipy.special.cython_special&quot;, &quot;btdtri&quot;) functype3d = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_double) beta_fn = functype3d(betaaddr) @njit def nb_beta_ppf(p, a, b): return beta_fn(a, b, p) x = nb_beta_ppf(0.1, 2.0, 0.5) x . 0.4681225665264196 . As we can see this is a little ugly but it works. . As a result of everything covered in these blog posts I do not believe that one option offers a significant advantage over the other and both offer valid tools for improving runtime. The choise of which to use in a given situation will depend on many factors and requires careful thought as to what is needed in a given setting. .",
            "url": "https://www.lewiscoleblog.com/cython-numba-2",
            "relUrl": "/cython-numba-2",
            "date": " â€¢ Feb 11, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Cython Vs Numba",
            "content": "Outline of the Problem . Python at its core is slow for certain things. By being a dynamically typed and interpreted language you incur certain runtime overheads. In some cases these are not much of an issue. At other times they can be critical. . Typically we will try and &quot;vectorize&quot; the code as much as possible (avoiding extraneous loops) and force as much code as we can into NumPy array operations which are typically &quot;quick&quot; (compiled C code). This is fine when it works, but it is not always possible to vectorize the code or, in some cases, the vectorization leads to code that is very hard to read/understand. . Both Numba and Cython (not to be confused with CPython) aim to provide tools to deal with such situations. . Outline of Cython . Cython is a programming language that is part Python and part C/C++, it can be compiled into a python extension and/or an executable. If you are familiar with Python it is reasonably easy to understand Cython code, it largely just has a few &quot;boiler-plate&quot; code blocks along with a few static type declarations (familiar to those who know C/C++/related languages). . Since there are no dynamic types (in well written Cython code) and it is compiled typically the resulting code is orders of magnitude faster than Python. Compared to vectorised NumPy there may not be a significant improvement but this depends on the exact implementation. . Cython itself is very flexible, if you can express the code in Python it is unlikely you will not be able to express it in Cython. Any arbitrary class structure can work within Cython, as a result it is used for many &quot;high performance&quot; Python packages (e.g. SciPy). . It is possible to parallelize the code or utilise GPU computation using Cython. This normally requires a bit of work but typically does not require nearly as much work as using Cuda in C++ (for example). As usual the normal caveats relating to multi-thread applications also apply to Cython code. . You can read the Cython documentation here! . Outline of Numba . Numba is a slightly different beast. It uses the concept of a &quot;just in time&quot; compiler (JIT). Essentially this means that code is compiled &quot;on the fly&quot; during runtime instead of requiring compilation prior to execution. Numba compiles the python code using a LLVM compiler. . The syntax is very simple and most of the time just requires a simple decorator on a Python function. It also allows for parallelisation and GPU computation very simply (typically just a &quot;target = &#39;cuda&#39;&quot; type statement in a decorator). In my experience a lot less thinking is required to set this up compared to Cython. . The downside of Numba (at least for me) is that it is a (comparatively) new package and as such does not have support for absolutely everything you would want, unlike with Cython it is possible to just &quot;hit the wall&quot; where you simply cannot use Numba without a major re-writing of the code. (One such example being you cannot call @guvectorize functions inside the @njit decorator). It is worth checking the github issues log regularly as often these issues are on the docket to be corrected in future releases. . Another downside of Numba is the lack of useful traceback, typically you need to &quot;switch off&quot; Numba and run in regular python to track down an error. This is typically only a minor inconvenience but if the code is particularly slow it can get frustrating trying to find an error without the Numba speed up. . You can read more about Numba here! . Which is better? . From a raw performance perspective I do not see either Cython nor Numba consistently beating the other in all situations. Typically the performance will be comparable and you will rarely find one being many orders of magnitude quicker (assuming you&#39;re using both correctly). . The choice of which to use, in my opinion, comes down to other factors. Convenience being a big one, I typically find Numba easier and quicker to implement when it works. As noted above however it doesn&#39;t always work (e.g. if using class structures, custom data types, etc.) With familiarity you do get an instinct as to whether a code will work or not. Cython on the other hand offers much more flexibility. . There is also the issue of how the code will be used. Cython is well established for creating efficient extension modules that sit nicely within the Python eco-system. Numba can be used in a similar way but I have found it a bit more finnicky to deal with (for example through Numba itself changing its API fairly regularly since it&#39;s a relatively new module, some code from previous iterations of Numba simply does not work at all with the later versions). . What is the catch? . Unfortunately things are not perfect, typically we will still be interfacing Cython/Numba functions via Python and so using repeated calls to these functions we will still incur overheads (typically through the conversion to Python types). This can mean that certain code is still significantly slower than C/C++ equivalents. These packages are therefore most useful for when you have profiled your code and can see that a handful of functions/operations are the real bottleneck. . These packages may not help if your code is particularly memory intensive, in which case it is better to spend time thinking about memory management instead. In some cases these packages provide some help in that respect also (e.g. NumPy is prone to creating many cached variables for simple operations, if the variables are large arrays this can become a pain.) . What about PyPy/etc? . Another option for performant Python code is to use PyPy instead of CPython. I have not used this very much yet, if I get the time to really kick the tyres I may write another blog on my findings. There are some features that appear useful but the eco-system is not as well supported (yet?) and so may require some additional work to recreate some high level functionality. . You can read more about PyPy here! . Why not just C/C++? . Ultimately if you require peak performance at all costs these options are still no substitute for well written C/C++. However as I often warn people: computation time is generally cheaper than human time - it is often better to use a slightly sub-optimal (but still respectable) code than devote months to R&amp;D and slow down the development cycle. Since Numba/Cython are so similar to Python (and it is possible to just &quot;tack on&quot; some Python to the end of these codes) you can prototype much more quickly in my experience. All these factors (along with many others such as where the code is to be deployed, what other tools are being used, etc.) need to be weighed up. . What about Julia? . Some readers may be familiar with the Julia language as an option for high performance scientific computing. I have a little experience (but am far from an expert). As I understand Julia is based around JIT (as with Numba), however being a language to itself it never needs to interface with Python and its limitations. It can therefore create more efficient code for larger scale projects since they never have to worry about Python overheads. Typically the benchmarks seen online are for smaller &quot;toy&quot; problems and so the performance does not appear to be too different from Cython/Numba. . I have not switched to Julia for a few reasons, firstly the popularity of Python - it is typically fairly easy to learn Cython/Numba for somebody who understands Python/NumPy making collaboration easier. Secondly the Python eco-system is well developed there is typically a package available to do almost anything you would want. Thirdly at this point it is fairly easy to get Python to &quot;speak&quot; with other systems if you need to turn something from a prototype to production. These concerns are ultimately just related to uptake however, as more people use Julia I see this becoming less of a concern. . You can read more about Julia here! . Conclusion . Hopefully now we can see that Cython/Numba provide useful tools for bridging the gap between Python and C/C++ runtimes. As the old saying goes &quot;you cannot have your cake and eat it too&quot; and so it may not be possible to get performance as quick using these options. However we can often get performance that is &quot;good enough&quot; in practical terms. .",
            "url": "https://www.lewiscoleblog.com/cython-numba",
            "relUrl": "/cython-numba",
            "date": " â€¢ Feb 4, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Jackknife Methods",
            "content": "In this blog post we are concerned with a specific problem: we have a Monte-Carlo type model that produces some simulated output. With this we want to estimate an arbitrary statistic (for example percentiles, expected shortfall or more complicated statistics relating to many variables). We know however that calculated in this way the calculated statistic is just one realisation of a distribution of outcomes. We would like to be able to say something about this distribution, in particular we would like to have some idea of the variability in the statistic. . The &quot;obvious&quot; (and most accurate) way to do this would be to re-run the model many times with different random seeds and create an empirical distribution of the statistic. However if the model is particularly complex this might mean a lot of compute time. Instead we would like to find an approximate method that does not require us to re-run the model at all. To do this in a general way we will introduce the Jackknife method. . It is worth noting that in certain situations other methods can be easier to implement/more accurate, for example if we only wanted to estimate the 90th percentile we can construct an argument using a binomial distribution (and normal approximation thereof) - however this method will not generalise to (for example) expected shortfall or even more complicated statistics. . Justification of the Method . We begin by supposing we have $N$ un-ordered observations $ { X_i }_{i=1}^{N}$ from our Monte-Carlo model. We use these observations to create an estimate of a statistic $ hat{Q}$. We denote the estimate from the model $ hat{Q}_{ {1:N }}$, which itself is a random variable. Through a Taylor expansion we can note: $$ mathbb{E} left( hat{Q}_{ {1:N }} right) = hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} + ...$$ For some constants $a_x$. . If we now consider partitioning the $N$ observations as: $N = mk$ for integers $m$ and $k$ - that is we create $m$ collections of $k$ obervations ($k gg m$). We denote a set: $A_i = { X_j | quad j &lt; (i-1) k , quad j geq i k }$ to contain all observations bar $k$, each set removes a different set of $k$ observations. Each has $|A_i| = (m-1)k$. We can then write: $$ mathbb{E} left( hat{Q}_{A_i} right) approx hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} $$ Via a second order approximation. . If we then define a new variable: $ hat{q}_i = m hat{Q}_{ {1:N }} - (m-1) hat{Q}_{A_i}$. Then via a second-order approximation we have: $$ mathbb{E} left( hat{q}_i right) approx m left( hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} right) - (m-1) left( hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} right)$$ Through some simplification this becomes: $$ mathbb{E} left( hat{q}_i right) approx hat{Q} - frac{a_2}{m(m-1)k^2} $$ Asymptotically this has bias $O(N^{-2})$. If the estimator only has bias $O(N^{-1})$ (i.e. $a_i =0$ for $i geq 2$) then the approximation is unbiased. . We can now define two new variables: $ hat{ hat{Q}}_N = frac{1}{m} sum_{i=1}^{m} hat{q}_i$ $ hat{ hat{V}}_N = frac{1}{m-1} sum_{i=1}^{m} left( hat{q}_i - hat{ hat{Q}}_N right)^2$ Then via CLT we have that $ hat{ hat{Q}}_N sim mathcal{N}( hat{Q}, hat{V})$ for some unknown variance $ hat{V}$. We can thus create an $X %$ confidence interval as: $ hat{Q} in left[ hat{ hat{Q}}_N - t sqrt{ frac{ hat{ hat{V}}_N}{m}}, hat{ hat{Q}}_N + t sqrt{ frac{ hat{ hat{V}}_N}{m}} right] $ With $t$ as the $(1-X) %$ point of a double-tail student-t distribution with $(m-1)$ degrees of freedom. . We can see from the construction of this confidence interval there has been no restriction on the type of statistic $ hat{Q}$ used, in this sense this is a generic method. . (Note this is strongly related to a bootstrap method, in fact it is a first order approximation to a bootstrap) . An Example . The explanation above is quite notation dense, it will be easier to look at an example. In this case we will take one of the simplest examples of a Monte-Carlo model: estimating the value of $ pi$. To do this we will take a unit square and a unit quarter circle inside it: . . We will simulate random points within the square and calculate the proportion $p$ of points landing within the quarter circle (red). If we simulate $N$ points we get an estimate $ pi approx frac{4p}{N}$. A simple vectorised numpy for this can be seen below: . # Estimating pi using Monte-Carlo import numpy as np def points_in_circle(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) y = np.random.random(N) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_circle &quot;&quot;&quot; return arr.sum() / arr.shape[0] * 4 SIMS = 10000 pts = points_in_circle(10000) print(&quot;Estimate of pi:&quot;, est_pi(pts)) . Estimate of pi: 3.1456 . We can use the jackknife method to now construct a confidence interval. (Obviously in this case we have the sample estimator of an average and so CLT applies, in practice we wouldn&#39;t use a jackknife here. But for this example I wanted something simple as to not distract attention from the jackknife method itself.) We know what the result &quot;should&quot; be in this example, however we shall pretend we don&#39;t have access to np.pi (or similar). . We can code up an implementation of the jackknife method as: . from scipy.stats import t def jackknife_pi(arr, pi_fn, m): &quot;&quot;&quot; This function implements the jackknife method outlined above The function takes an array (arr) and an estimate function (pi_fn) and a number of discrete buckets (m) - in this implementation m needs to divide size(arr) exactly The function returns Q-double hat, V-double hat, (m-1) &quot;&quot;&quot; Qn = pi_fn(arr) N = arr.shape[0] itr = np.arange(N) k = N / m q = np.zeros(m) for i in range(m): ID = (itr &lt; i*k) | (itr &gt;= (i+1)*k) temp = arr[ID] q[i] = m*Qn - (m-1)*pi_fn(temp) Qjk = q.sum() / m v = (q - Qjk)**2 Vjk = v.sum() / (m - 1) return Qjk, Vjk, (m-1) def conf_int(Q, V, X, dof): tpt = t.ppf(1-(1-X)/2, dof) up = Q + tpt*np.sqrt(V/(dof+1)) down = Q - tpt*np.sqrt(V/(dof+1)) print(round(X*100),&quot;% confidence interval: [&quot;,down,&quot;,&quot;,up,&quot;]&quot;) jk_pi = jackknife_pi(pts, est_pi, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . 95 % confidence interval: [ 3.103649740420917 , 3.187550259579082 ] . We can see that the 95% confidence interval range is fairly large (around 3.10 to 3.19) in this case. We will now show that by using a &quot;better&quot; method we can reduce this range. We start by reconsidering the square-quarter-circle: we notice that we can add 2 additional squares to this setup: . . Apart from looking like a Mondrian painting we can notice that all points generated in the yellow area will add &quot;1&quot; to the estimator array and all points in the black area will add &quot;0&quot;. The only areas of &quot;contention&quot; are the blue/red rectangles, if we focus only in generating points in these areas we will increase the accuracy of the estimator. By symmetry these 2 rectangles are identical, we only need to generate points within the rectangle: $ left { left(1, frac{1}{ sqrt{2}} right), left( 1,0 right), left( frac{1}{ sqrt{2}},0 right), left( frac{1}{ sqrt{2}}, frac{1}{ sqrt{2}} right) right }$. This has the area: $ frac{ sqrt{2}-1}{2}$ or both rectangles together having total area: $ sqrt{2}-1$. Therefore generating $N$ points within these rectangles is equivalent to generating: $ frac{N}{ sqrt{2}-1}$ points in the original scheme (approximately 2.5 times as many). This should reduce the standard deviation of the estimate by about a third (by CLT). We can code this estimator up in a similar way to before: . def points_in_rect(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise This uses the &quot;imporved&quot; method N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) * (1 - 1 / np.sqrt(2)) + (1/np.sqrt(2)) y = np.random.random(N) / np.sqrt(2) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi_2(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_rect This applies a correction since points are only simulated in smaller rectangles &quot;&quot;&quot; pct = arr.sum() / arr.shape[0] approx_pi = (pct*(np.sqrt(2)-1) + 0.5)*4 return approx_pi SIMS = 10000 pts = points_in_rect(10000) print(&quot;Estimate of pi:&quot;, est_pi_2(pts)) jk_pi = jackknife_pi(pts, est_pi_2, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . Estimate of pi: 3.144554915549336 95 % confidence interval: [ 3.1247314678035196 , 3.164378363295143 ] . As expected we can see the confidence interval has decreased significantly through an improved estimator. . Notes on Implementation . As we have seen the jackknife is a general tool. In certain situations other tools exist. There are a couple of points we have to keep in mind when implementing these methods: . The selection of m: this is fairly arbitrary, if the statistic being analysed is very cumbersome to calculate then a smaller choice of m is helpful (or if we wish to run this method over very many statistics). | Properties of the statistic: in some instances we know the statistic must be bounded (e.g. a correlation coefficient must be between $[-1,1]$) This additional information can and should be used to improve the confidence interval. It often becomes more art than science when deciding how to present the results of this method. | . Conclusion . In this post we have seen what a jackknife method is, why it works and a basic implementation. Hopefully now it is obvious the power these methods hold for reporting on the results of a Monte-Carlo simulation. In more sophisticated situations it can really give insight into which parts of a model suffer most from simulation error and also how confident we should be with an estimate it produces. .",
            "url": "https://www.lewiscoleblog.com/jackknife",
            "relUrl": "/jackknife",
            "date": " â€¢ Jan 28, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Neuron Models 3: Ensembles",
            "content": ". This is the third blog post in a series - you can find the previous blog post here . . Justifying Gaussian White Noise . We first begin with a small diversion, in the previous HH and EIF neuron firing examples we have assumed some sort of Gaussian white noise as an input signal. We briefly mentioned that this is a reasonable assumption but we will justify this in a bit more detail here. First we note that each neuron will typically take input signal from the order of 10,000 neurons. As such even in a low firing rate scheme a neuron will likely receive relatively large amount of input spikes. We can express this signal as: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k)$$ Where: $N$ is the number of connected neurons $J_{i}$ is the synaptic connection strength from neuron $i$ $t_i^k$ is the time of the kth spike recieved from neuron i . If we assume that these spikes arrive in an uncorrelated, memoryless fashion in the form of a Poisson process and that the connection strengths are suitably small: $ langle J_i rangle ll V_{Th} - V_{Re}$ (where angle brackets denote population average). Then we can apply a diffusion approximation: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k) approx mu + sigma xi(t)$$ Where: $ mu = langle J_i rangle N nu $ $ sigma = langle J_i^2 rangle N nu $ $ nu$ is the mean firing rate over all connected neurons $ xi(t)$ is a Gaussian white noise process . Of course as with all approximations this is subject to &quot;small sample size&quot; and $N$ needs to be suitably large. . Fokker-Planck . Recall that we specified the EIF model with Gaussian white noise as having dynamics: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . This is nothing more than an Ito process of the form: $$dX_t = mu(X_t,t)dt + sigma(X_t, t)dW_t $$ With standard Wiener process $W_t$. The Fokker-Planck equation gives us a probability distribution of this process $p(x,t)$ through the PDE: $$ frac{ partial}{ partial t} p(x,t) = - frac{ partial}{ partial x} left[ mu(x,t)p(x,t) right] + frac{ partial^2}{ partial x^2} left[ frac{1}{2} sigma^2(x,t)p(x,t) right] $$ This formula can also be extended to higher dimensions in an obvious way. The derivation of this formula is fairly involved so not included in this blog post, most good textbooks on stochastic analysis should have a derivation for the interested reader. . In the case of the EIF model we can thus write down: $$ frac{ partial p}{ partial t} = frac{ sigma^2}{ tau} frac{ partial^2p}{ partial V_m^2} + frac{ partial}{ partial V_m} left[ frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) right] $$ With $ psi(V_m)$ represnting the exponential firing term. . By the continuity equation we can write: $$ frac{ partial p}{ partial t} = - frac{ partial J}{ partial V_m} $$ . Where $J$ represents the flux. By using this relation in the Fokker-Planck equation and integrating over voltage we get: $$ J(V_m, t) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . We can also note that: $$J(V_{Re}^+,t) = J(V_{Re}^-, t) + r(t)$$ . Where $V_{Re}^ pm$ represents the limit from above (+) or below (-) the reset voltage. the function $r(t)$ represents the average neuron firing rate. This is due to the implementaion of the voltage reset mechanism post spike. We can also note that for $V_m &lt; V_{Re}$ we have $J(V_m, t) = 0$ and for $V_m &gt; V_{Re}$ we have $J(V_m, t) = - r(t)$. We can then solve the flux equation to give: $$P(V_m, t) = frac{r(t) tau}{ sigma^2} int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du $$ . Since the probability measure needs to integrate to 1, we can then write: $$r(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m right)^{-1} $$ . (Note under the scheme presented there is no time dependence to any of these equations. Under time dependent signals we would have to be more careful and typically further approximations are made.) . So far we have not allowed for the refractory period, we have assumed that after reset the voltage trajectories continue as normal. Given we have chosen a deterministic refractory period we can just add this to the euqation above: $$r_{ref}(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m + T_{Ref} right)^{-1} $$ . We can see that this integral will not give rise to an analytic solution in the case of EIF neurons. The forward Euler scheme we relied upon in the past will not perform well here. Instead we will use a slightly different numerical scheme. . Numerical Integration . (This is taken from Richardson [2007] - see references for further details) Presented now is a numerical scheme for calculating the firing rate. Recall from above: $$ J(V_m, t) = - r(t) Theta(V - V_{Re}) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . Where $ Theta(V)$ is the Heaviside step-function. Re-arranged this gives: $$- frac{ partial p}{ partial V_m} = - frac { tau}{ sigma^2} r(t) Theta(V - V_{Re}) + sigma^{-2}(V_m - V_L - psi(V_m)) p(V_m,t) $$ . Which is of the form: $$ frac{ partial p}{ partial V_m} = G(V_m)p(V_m) + H(V_m) $$ . By applying a voltage discretization scheme: $V_k = V_{Lb} + k Delta_V $ with $V_n = V_{Th}$ we can write down: $$ p(V_{k-1}) = p(V_k) e^{ int^{V_k}_{V_{k-1}} G(V)dV} + int^{V_k}_{V_{k-1}} H(V) e^{ int^V_{V_{k-1}}G(U)dU} $$ . We can approximate this as: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V G(V_k)} + Delta_V H(V_k) left( frac{e^{ Delta_V G(V_k)} - 1}{ Delta_V G(V_k)} right) $$ . Substituting back in the necessary formulae for $G$ and $H$ gives: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2}r(t) Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right) $$ . However this still has unknown $r(t)$ in it. If we apply a transform: $q(V,t) = frac{p(V,t)}{r(t)}$ then: $ sum q(V_k) = (r(t))^{-1}$ and: $$ q(V_{k-1}) = q(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2} Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right)$$ . To simplify this expression we define functions $A$ and $B$ so that: $$ q(V_{k-1}) = q(V_k) A(V_k) + Theta(V_k - V_{Re}) B(V_k) $$ . And so we can calculate the firing rate. This scheme has a much better performance than an Euler scheme. We instantiate the scheme with $q(V_n) = 0$, we also select a value $V_{Lb}$ as a cut-off to stop iterating. An implementation of this method can be seen below: . # Evaluating the solution to the Fokker-Planck Equation to calculate the firing rate of an EIF neuron subject to Gaussian white noise import numpy as np # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up additional parameters for solving Fokker-Planck. DelV (mV) and VLb (mV) DelV = 0.001 VLb = -100 Steps = int(np.ceil((Vth - VLb) / DelV)) q = np.zeros(Steps) V = np.arange(Steps)*DelV + VLb # For ease define function psi def psi(V): return DelT * np.exp((V - VT) / DelT) def A(V): return np.exp(DelV * sig**-2 *(V - VL - psi(V))) def B(V): if A(V) == 1.0: return DelV * tau * sig**-2 else: return DelV * tau * sig**-2 * (A(V) - 1) / np.log(A(V)) # Shut off numpy divide errors np.seterr(divide=&#39;ignore&#39;) for i in range(Steps -1, 0, -1): if V[i] &gt; Vr: q[i-1] = q[i]*A(V[i]) + B(V[i]) else: q[i-1] = q[i]*A(V[i]) r = 1/(q.sum()/1000000 + Tref/1000) print(&quot;Firing rate:&quot;, round(r,1), &quot;Hz&quot;) . Firing rate: 21.6 Hz . We can modify the previous EIF firing code to estimate the firing rate, the results should be similar (note: for this I used 10m time steps, it is a slow running code!): . #collapse # Implementation of a noisy EIF neuron using a forward Euler scheme # Reduce N for quicker running code import numpy as np # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 10000000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up voltage Vold (mV) and spike count Sp Vold = Vr Sp = 0 # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: Vnew = Vr Tc -= 1 else: Vtemp = Vold + dt/tau*(VL - Vold) + DelT*dt/tau*np.exp((Vold - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: Vnew = Vr Tc = np.ceil(Tref/dt) Sp += 1 else: Vnew = Vtemp Vold = Vnew print(&quot;Estimated firing rate:&quot;, round(Sp/(N*dt/1000),1), &quot;Hz&quot;) . . Estimated firing rate: 20.2 Hz . Which we can see is similar to the solution of the Fokker-Planck equation. In the limit $N to infty$ and decreasing the lattice sizes these approximations should become much closer. . Conclusion . We have seen that by using the Fokker-Planck framework we are able to calculate the mean firing rate of the EIF neuron. We can also notice that the numerical scheme to integrate the Fokker-Planck runs significantly faster than taking a Monte-Carlo approximation by simulating the EIF directly. We can also notice that the Fokker-Planck framework is easy to extend (e.g. to modulated noise or other applied signals) and further we can extend this to allow for connected networks of neurons (I may write an additional blog post on this in the future but will likely end up being quite similar to this one). . References . https://neuronaldynamics.epfl.ch/online/Ch13.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | How Spike Generation Mechanisms Determine the Neuronal Response to Fluctuating Inputs - Nicolas Fourcaud-TrocmeÂ´, David Hansel, Carl van Vreeswijk, and Nicolas Brunel [2003] | Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based synaptic drive - Magnus J Richardson [2007] | .",
            "url": "https://www.lewiscoleblog.com/neuron-models-3",
            "relUrl": "/neuron-models-3",
            "date": " â€¢ Jan 21, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Neuron Models 2: Exponential Integrate and Fire Model",
            "content": ". This is the second blog post in a series - you can find the previous blog post here . . Integrate and Fire Models . Throughout this blog post we will focus on integrate and fire models. This class of model has been around for a long time, in fact longer than the Hodgkin-Huxley model. The first model was presented by Lapicque in 1907. Since then many alternative formulations have been presented. We can express the models in the form: $$ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $$ . Where: $ tau$ represents membrane time constant ($C_m / g_L$ in notation used previously) $E$ represents the rest potential $ psi(V)$ is the spike generating current term $R_m$ represents membrane resistance ($1/g_L$) $I(t)$ is a function representing an applied current (in the Hodgkin-Huxley example we took a Gaussian white noise) . With integrate and fire models we have the issue that (typically) the action potential will shoot off to infinity. In order to stop this we implement a threshold ($V_{Th}$), when the process reaches this value it is reset (to $V_{Re}$) and the dynamics start again. This is not a big issue because for our purposes we are noly interested in the dynamics of an onset of an action potential, the mechanism of returning to normal levels is not of (much) interest. When modelling we may wish to hold the voltage at $V_{Re}$ following a spike for a short time ($ Delta_{T_{Rf}}$) to reflect the refractory period of a neuron. The refractory period can be modelled stochastically but usually a static value is succficient. . Integrate and fire models are thus defined by the form of the function $ psi(V)$ - this function may be linear or non-linear. Examples include the leaky-integrate and fire model (linear), Fitzhugh-Nagumo (polynomial) and the exponential integrate and fire (non-linear). . From Hodgkin-Huxley to Integrate and Fire . We now take a quick de-tour to justify the use of the integrate and fire model as an approximation to the Hodgkin-Huxley dynamics. . First we notice that gate m operates on a much faster time scale than gates n or h (and similarly much faster than the leak channel which controls the potential dynamics with all gates closed.) Given it is so much faster we can apply an instantaneous approximation, namely: $m(t) = hat{m}(V_m(t))$ that is: the dynamics are defined by the membrane voltage. From plotting gate dynamics we can also observe that gates n and h are approximately translated reflections of each other. As an approximation we can create an adaptation variable $w$ with $n = aw$ and $h = b - w$ for constants $a, b$. We can then write down the equation: $$ C_m frac{dV_m}{dt} = I_p - overline{g_K}(aw)^4(V_m-V_K) - overline{g_{Na}}( hat{m}(V_m))^3(b - w)(V_m-V_{Na}) - overline{g_L}(V_m-V_L) $$ . There is a corresponding equation for the adaptation variable $w$ which we shall not concern ourselves with. We are only interested in the onset of spiking not the refractory period dynamics so we will take $w = w_{rest}$ to be a fixed value. We can therefore express the voltage dynamics as: $$ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) - lambda ( hat{m}(V_m))^3(V_m-V_{Na}) $$ . By collecting terms and some re-arrangement. $g_{eff}, V_{eff}$ and $ lambda$ are all constant values. Therefore via the approximations outlined above we are left with an equation of the form: $$ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $$ . Namely an integrate and fire model. . Exponential Integrate and Fire . Continuing with the line of reasoning above we shall consider the function: $ hat{m}(V_m)$. We assume the dynamics are so rapid that they are essentially in equilibrium: $$ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m = 0 $$ So: $$ hat{m}(V_m) = frac{ alpha_m(V_m)}{ alpha_m(V_m) + beta_m(V_m)}$$ . Since Hodgkin-Huxley suggested the following forms of these equations: $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ . We can approximate $ hat{m}(V_m)$ with a logistic function: $$ hat{m}(V_m) approx (1 + e^{- beta(V_m - theta)})^{-1}$$ We can then express the Taylor expansion of this as: $$ hat{m}(V_m) = sum (-1)^k e^{ beta(V_m - theta)(1+k)}$$ Which we can see from the expansion of $1/(1+y)$ with $y = e^{- beta(V_m - theta)}$. So a first order approximation is: $$ hat{m}(V_m) approx e^{ beta(V_m - theta)}$$ . Then the current corresponding to the sodium channel can be expressed approximately: $$I_{Na} = g_{Na}(b - w_{rest})(V_m - V_{Na}) e^{3 beta(V_m - theta)} $$ . If we take $(V_m - V_{Na}) approx (V_{rest} - V_{Na}) &lt; 0$ as an approximation we get approximate voltage dynamics as: $$ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) + hat{ lambda} e^{ beta (V_m - theta)} $$ . This is known as the exponential integrate and fire (EIF) model.This model has been shown to fit experimental data (and the Hodgkin-Huxley model) very well in practice. Typically we use a parameterization of the spiking term: $$ psi(V) = Delta_T e^{ left( frac{V - V_T}{ Delta_T} right)} $$ We can see this model is highly non-linear and without applying the threshold mechanics the membrane potential would shoot to infinity. The 2 new parameters in this model are: $V_T$ which represents the voltage scale at which the exponential term becomes significant in the dynamics $ Delta_T$ representing the sharpness of the spike . With a Gaussian white noise term $ xi_t$ we can fully specify the dynamics using: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . As before we can solve this using a foward Euler scheme (although the forcing term is non-linear this still provides a reasonable solution for small enough time steps.) An implementation of this can be seen below: . # Implementation of a noisy EIF neuron using a forward Euler scheme import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 50000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up arrays for time T (ms) and voltage V (mV) T = np.arange(N) * dt V = np.zeros(N) V[0] = Vr # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: V[i] = Vr Tc -= 1 else: Vtemp = V[i-1] + dt/tau*(VL - V[i-1]) + DelT*dt/tau*np.exp((V[i-1] - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: V[i] = Vr Tc = np.ceil(Tref/dt) else: V[i] = Vtemp # Plot voltage trajectory plt.plot(T, V) plt.xlabel(&quot;Time (ms)&quot;) plt.ylabel(&quot;Membrane Potential (mV)&quot;) plt.title(&quot;Membrane Voltage Trajectory&quot;) plt.show() . Conclusion . The voltage trajectory displays realistic neuron dynamics for the onset of spiking. However as expected through the use of the refractory period implementation the depolarizing phase is not captured well. This is ok since we consider the information to be carried by the spike itself not the behaviour shortly afterwards. We can also see that this implementation is considerably simpler than that of Hodgkin-Huxely since there is only one ODE. This is of benefit when modelling large networks of neurons where time/computational constraints become a consideration. . It has been shown that the EIF model can predict spiking behaviour very well in practice, despite the somewhat cavalier assumptions made during its derivation from the Hodgkin-Huxley. . In a future blog post we will make use of the mathematical tractability of the EIF model to analyse neurons in more depth. . References . https://neuronaldynamics.epfl.ch/online/Ch5.S2.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | . . This blog post is the second part of a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/neuron-models-2",
            "relUrl": "/neuron-models-2",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Neuron Firing Models: Hodgkin-Huxley Model",
            "content": "Some (very basic) Biology . We start with a very limited description of what a neuron is and the mechanism by which it fires. For sake of completeness by neuron we will refer to pyramidal neurons, they make up a large proportion of neurons within the cortex of mammals. Other types of neurons are specialised for different functions. This background will be very brief and not cover the biology in any great detail. The literature and data on this topic is vast and I will not do it justice so any interested readers should look towards biology textbooks for more detailed descriptions. . The neuron is made up of 3 main components: a soma (cell body), an axon and many dendrites. In laymans terms the axon is the &quot;output&quot; of the neuron while the dendrites are &quot;inputs&quot; to the soma. The axon is covered in a myelin sheath which acts &quot;insulation&quot; which can &quot;speed up&quot; signal flow. Dendrites can futher be broken down into the apical dendrite, basal dendrites and dendritic spines. Both dendrites and axons are highly branched and a single neuron can be linked to many 1000s of others. Signals are passed through synapses. Synaptic inputs can be either excitatory (making the target neuron more likely to fire) or inhibitory (less likely). . . The details of axons and dendrites are not that important for our purposes right now. Instead we are interested in the soma: essentially where the signal is generated. The cell wall contains many voltage gated ion channels, most notably: sodium (Na+) and potassium (K+) channels. There are 2 forces acting on ions inside/outside of the soma: namely the electrical potential within the body and the concentration gradient. Resting membrane potential is around -40mv to -90mv, in this state there is a higher concentration of potassium ions inside the cell than outside and the reverse for sodium ions. The channel for potassium is highly permeable and so potassium ions flow into the soma, the sodium channel is semi-permeable so sodium slowly flow out of the soma, to maintain a constant negative potential the cell &quot;pumps&quot; ions in the reverse direction to maintain equilibrium. . An action potential (neuron spike) is a shift away from the negative equilibrium membrane potential to a positive potential. This is as a result of ion flows due to voltage controlled gates. There are 3 gates of interest here: . Sodium activation gate - normally closed but opens with positive potential | Sodium inactivation gate - normally open but closes with positive potential | Potassium inactivation gate - normally closed but opens with large positive potential | We can break down the events that cause the action potential then as: . Depolarisation - A depolarisation event occurs (e.g. a signal from the dendrites) which brings the cell&#39;s membrane potential to 0mv. This is as a result of positive charged ions flowing into the cell. As the potential increases to some threshold the sodium activation gate is opened which allows more positively charged sodium ions to enter. The potential then becomes positive. | Repolarisation - The positive potential causes the sodium inactivation gate to close preventing more sodium ions entering. Meanwhile the potassium inactivation gate opens, since the concentration of potassium ions inside the cell is much greater than outside this leads to an outflow of potassium. The removal of positively charged ions moves the cell back towards equilibrium. | Refractory Period - The potassium channel stays open slightly past the equilibrium point and the membrane potential becomes too negative (hyperpolarises). As the potassium channel closes the potential tends back to equilibrium. There is a short period after an action potential where the neuron is unable to fire again. | (Source: https://teachmephysiology.com/wp-content/uploads/2018/08/action-potential.png) . In the rest of this blog we will look at models of action potentials and will try and keep this real world description of action potentials in mind. . Hodgkin Huxley Model . We now consider a spiking neuron model as presented by Alan Hodgkin and Andrew Huxley in 1952, this model won them the Nobel prize for physiology in 1963. The model was developed by studying the axon of a giant squid neuron. The model itself tries to mimic the gates and ionic channels described above. Diagramatically we can represent the model as: . The voltage controlled ionic gates conductances are represented in the diagram by gn (only one represented in the diagram) and there is a leak conductance represented by gl. Cm represents the membrane capacitance and there is an external stimulus Ip which represents inputs from other neurons (or a test current applied by a probe in experiment). We can then represent the model as a set of 4 interacting PDEs: $ frac{dV_m}{dt} = frac{I_p}{C_m} - frac{ overline{g_K}n^4}{C_m}(V_m-V_K) - frac{ overline{g_{Na}}m^3h}{C_m}(V_m-V_{Na}) - frac{ overline{g_L}}{C_m}(V_m-V_L) $ $ frac{dn}{dt} = alpha_n(V_m)(1-n) - beta_n(V_m)n $ $ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m $ $ frac{dh}{dt} = alpha_h(V_m)(1-h) - beta_h(V_m)h $ . Where the functions $ alpha_x , beta_x$ as suggested by Hodgkin and Huxley are: $ alpha_n(V_m) = frac{0.01(10-V_m)}{e^{(1.0-0.1V_m)}-1} $ $ beta_n(V_m) = 0.125 e^{-V_m / 80} $ $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ $ alpha_h(V_m) = 0.07 e^{-V_m / 20} $ $ beta_h(V_m) = frac{1}{e^{(3.0-0.1V_m)}+1} $ . The following values are suggested for the model constants: $C_m = 1 mu F /cm^2$ - Capacitance per unit surface area of neuron membrane $ overline{g_{Na}} = 120 mu S / cm^2$ - Voltage controlled sodium conductance per unit surface area $ overline{g_{K}} = 36 mu S / cm^2$ - Voltage controlled potassium conductance per unit surface area $ overline{g_{L}} = 0.336 mu S / cm^2$ - Voltage controlled leak conductance per unit surface area $V_{Na} = 115mV$ - Sodium voltage gradient $V_{K} = -12mV$ - Potassium voltage gradient $V_{L} = 10.613mV$ - Leak current voltage gradient . This is a non-linear system of differential equations and as such it is not possible to study analytically. However we can simulate this numerically. For this example we will assume that the external stimulus follows a white noise (Brownian motion). Under certain conditions this can be a reasonable assumption. We will define this as follows: $I_p(t) = sigma W_t$ for some positive constant $ sigma$ with units $ mu A / cm^2$ . By introducing stochastic noise such as this we unfortunately are unable to use any of the Python pre-made ODE integrators (e.g. scipy.integrate.ode) instead we will rely on a forward Euler scheme which will perform well enough for our purposes assuming the discrete time steps are selected to be sufficiently small. An example implementation of this can be seen below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed np.random.seed(123) # Set time step dt (ms) and number of steps for simulation N dt = 0.001 N = 50000 # Set up time array T T = np.arange(N) * dt # Set model inputs, using nomenclature: K - Potassium, Na - Sodium, L - Leak # Set membrane capacitance per unit area (uF/cm^2) Cm = 1.0 # Set applied current density volatility (uA/cm^2) sigma = 2 # Set channel conductance per unit area (mS/cm^2) gK = 36.0 gNa = 120.0 gL = 0.3 # Set voltage gradients (mV) VK = -12.0 VNa = 115.0 VL = 10.613 # Define ion channel rate functions def alpha_n(Vm): return (0.01 * (10.0 - Vm)) / (np.exp(1.0 - (0.1 * Vm)) - 1.0) def beta_n(Vm): return 0.125 * np.exp(-Vm / 80.0) def alpha_m(Vm): return (0.1 * (25.0 - Vm)) / (np.exp(2.5 - (0.1 * Vm)) - 1.0) def beta_m(Vm): return 4.0 * np.exp(-Vm / 18.0) def alpha_h(Vm): return 0.07 * np.exp(-Vm / 20.0) def beta_h(Vm): return 1.0 / (np.exp(3.0 - (0.1 * Vm)) + 1.0) # Define applied signal function - can be replaced to investigate different signals def Ip(sig, t, V): return np.sqrt(dt) * sig * np.random.normal(0, 1, 1) # Set up arrays for dynamic results Vm = np.zeros(N) n = np.zeros(N) m = np.zeros(N) h = np.zeros(N) Signal = np.zeros(N) # Initialize the system V0 = 0 Vm[0] = V0 n[0] = alpha_n(V0) / (alpha_n(V0) + beta_n(V0)) m[0] = alpha_m(V0) / (alpha_m(V0) + beta_m(V0)) h[0] = alpha_h(V0) / (alpha_h(V0) + beta_h(V0)) # Loop through Euler-Forward scheme for i in range(1, N): Signal[i] = Ip(sigma, i*dt, Vm[i-1])/Cm Vm[i] = Vm[i-1] + Signal[i] - gK*np.power(n[i-1],4)*(Vm[i-1]-VK)*dt/Cm - gNa*np.power(m[i-1],3)*h[i-1]*(Vm[i-1]-VNa)*dt/Cm - gL*(Vm[i-1]-VL)*dt/Cm n[i] = n[i-1] + (alpha_n(Vm[i-1])*(1 - n[i-1]) - beta_n(Vm[i-1])*n[i-1])*dt m[i] = m[i-1] + (alpha_m(Vm[i-1])*(1 - m[i-1]) - beta_m(Vm[i-1])*m[i-1])*dt h[i] = h[i-1] + (alpha_h(Vm[i-1])*(1 - h[i-1]) - beta_h(Vm[i-1])*h[i-1])*dt # Plot sample path plt.plot(T, Vm) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Membrane voltage mV&quot;) plt.title(&quot;Hodgkin-Huxley Spiking&quot;) plt.show() # Plot gate opening over time plt.plot(T, n, label=&quot;n - K&quot;) plt.plot(T, m, label=&quot;m - Na&quot;) plt.plot(T, h, label=&quot;h - Na&quot;) plt.legend(loc=&quot;upper right&quot;) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Gate Proportion&quot;) plt.title(&quot;Gate Dynamics&quot;) plt.show() # Plot Limit trajectories plt.plot(n, Vm, label=&quot;n-Vm&quot;) plt.plot(m, Vm, label=&quot;m-Vm&quot;) plt.plot(h,Vm, label=&quot;h-Vm&quot;) plt.legend() plt.title(&quot;Cycle Trajectories&quot;) plt.xlabel(&quot;Gate Proportion&quot;) plt.ylabel(&quot;Membrane Voltage mV&quot;) plt.show() . Conclusion . In this blog post we have seen some of the basic biolgical and physical processes behind generating an action potential. Through the Hodgkin-Huxley model we have looked at dynamics of 3 voltage controlled ionic gates which control the membrane potential and neuron spiking. We can see that this model is fairly complicated, if we wanted to model many neurons it can become problematic. In a future blog post we will look at ways to simplify this model. . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/neuron-models-1",
            "relUrl": "/neuron-models-1",
            "date": " â€¢ Jan 7, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Insurance Aggregation Model",
            "content": "Note: Thoughts epressed within this workbook are my own and do not represent any prior, current nor future employers or affiliates . Background - An Overview of Modelling in Specialty Insurance . Within the specialty insurance space we are typically insuring economic interests of relatively rare events of high impact (for example: buildings damaged by hurricanes, aircraft crashes, impact of CEO wrong-doing, and so on.) These events are typically broken up into 2 broad classes: . Property | Casualty | Hence why the term &quot;P&amp;C&quot; insurer is sometimes used. Property risks are, as the name suggests, related to property - historically phyiscal property but now can include non-physical property (e.g. data). Owing to the relative simplicity of these risks there is an entire universe of quantitative models that exist for risk management purposes, in particular there are a handful of vendors that create &quot;natural catastrophe&quot; (nat-cat) models. These models are sophisticated and all essentially rely on GIS style modelling: a portfolio of insured risks are placed on a geographical map (using lat-long co-ordinates) then &quot;storm tracks&quot; representing possible hurricane paths are run through the portfolio resulting in a statistical distribution of loss estimates. For other threats such as earthquakes, typhoons and wild-fires similar methods are used. . These nat-cat models allow for fairly detailed risk management procedures. For example it allows insurers to look for &quot;hot spots&quot; of exposure and can then allow for a reduction in exposure growth in these areas. They allow for counter-factual analysis: what would happen if the hurricane from last year took a slightly different track? It allows insurers to consider marginal impacts of certain portfolios, for example: what if we take on a portfolio a competitor is giving up, with our current portfolio will it aggregate or diversify? As a result of this explanatory power natural catastrophe risks are now well understood and for all intents and purposes these risks are now commodified and have allowed insurance linked securities (ILS) to form. &lt;/br&gt; . Before this analytics boom specialty insurers made their money in natural catastrophe and property insurance, as such there has been a massive growth in recent years in the Casualty side of the business. Unfortunately the state of modelling on that side is, to put it politely, not quite at the same level. . As one would expect nat-cat model vendors have tried, and continue to try, to force the casualty business into their existing natural catastrophe models. This is a recipe for disaster as the network structure for something like the economy does not naturally lend itself to a geogprahic spatial representation. There is also a big problem of available data. Physical property risks give rise to data that is easy to cultivate. Casualty data is either hard to find or impossible - why would any corporation want to divulge all the details of their interactions? As such it does not appear that these approaches will become useful tools in this space. . To fill this void there has been an increasing movement of actuaries into casualty risk modelling roles. While this overcomes some of the problems that face the nat-cat models they also introduce a whole new set of issues. Traditional actuarial models relying on statistical curve fitting to macro-level data. Even assuming a suitable distribution function can be constructed it is of limited use for risk management as it only informs them of the &quot;what&quot; but not the &quot;why&quot;, making it hard to orient a portfolio for a specific result. More recently actuaries have slowly began to model individual deals at a micro-level and aggregate them to get a portfolio view. To do this a &quot;correlation matrix&quot; is typically employed, this aproach also has issues: . Methods don&#39;t scale well with size, adding new risks often require the entire model to be recalibrated taking time and effort. | They either require a lot of parameters or unable to capture multi-factor dependency (e.g. a double trigger policy where each trigger has its own sources of accumulation). | It is usually not possible to vary the nature of dependency (e.g. add tail dependence or non-central dependency) | Results are often meaningless in the real world, it is usually impossible to perform counter-factual analysis | To bridge this gap I have developed a modelling framework that allows for the following: . Modelling occurs at an individual insured interest level | Modelling is scalable in the sense that adding new insured interests requires relatively few new parameters and calibrations | Counter-factual analysis is possible and the model can be interpreted in terms of the real world | The framework itself is highly parallelizable, whereas nat-cat models require teams of analysts, large servers and IT infrastructure this framework lends itself to being run by multiple people on regular desktop computers with little additional workflow requirements | A First Step: A Simple Driver Method . We will now look at a very stylised model of aggregation that will form a foundation on which we can build the more sophisticated model framework. We call this method of applying dependence a &quot;driver method&quot;, it is standard practice for applying dependence in banking credit risk models where there can be many thousands of risks modelled within a portfolio. The interpretation is that there is a central &quot;driver&quot;, each individual risk is &quot;driven&quot; by this and since this is common to all risks there is an induced dependence relation between them. . The model relies on the generalised inverse transform method of generating random variates. Stated very simply: if you apply the inverse CDF of a random variable to a random number (U[0,1] variate) you will have samples distributed as that random variable. Therefore in order to apply dependence in a general form we only need to apply dependence between U[0,1] variates. We will also exploit the fact that normal distributions are closed under addition (that is the sum of normals is normal). . We can now express the model as follows: . We sample standard normal (N(0,1)) variates to represent the &quot;driver&quot; variable | For each risk sample an additional set of normal variates | Take a weighted sum of the &quot;driver&quot; and the additional normal variates to give a new (dependent) normal variate | Standardise the result from step 3) and convert to a U[0,1] variable using the standard gaussian CDF | Use an inverse transform to convert the result of step 4) to a variate as specified by the risk model | We can see that this method is completely general, it does not depend on any assumption about the stand-alone risk model distributions (it is a &quot;copula&quot; method). Another observation is that the normal variates here are in some sense &quot;synthetic&quot; and simply a tool for applying the dependence. . For clarity an example is presented below: . # Simple driver method example # We model a central driver Z # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates X1 and X2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate temporary synthetic variable X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Use normal CDF to convert X synthetic variables to uniforms U U1 = norm.cdf(X1) U2 = norm.cdf(X2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.4628059800990357 . The example above shows we have correlated gamma variates with around a 50% correlation coefficient (in this case we could calculate the correlation coefficient analytically but it is not necessary for our purposes, as we create more sophisticated models the analytic solutions become more difficult/impossible). . Even from this example we can see how models of this form provide superior scalability: for each additional variable we only need to specify 1 parameter: the weight given to the central driver. In contrast a &quot;matrix&quot; method requires each pair-wise combination to be specified (and then we require a procedure to convert the matrix to positive semi-definite form in order to apply it). Say our model requires something more sophisticated: say the sum of a correlated gamma and a weibull distribution - the number of parameters in a matrix representation grows very quickly. However it is worth noting we do lose some control, by reducing the number of parameters in this way we lose the ability to express every possible correlation network. However in most cases this is not a big problem as there is insufficient data to estimate the correlation matrix anyway. . It is worth pointing out that the type of dependency applied here is a &quot;rank normal&quot; dependency - this is the same dependency structure as in a multi-variate normal distribution, albeit generalised to any marginal distribution. . An Extension to the Simple Driver Method . We can extend the model above by noticing the following: there is nothing stopping the &quot;synthetic&quot; variables being considered drivers in their own right. Gaussians being closed under addition does not require that each variable needs to be independent, sums of rank correlated normals are still normal! We can thus extend the model to: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Use normal CDF to convert sX synthetic variables to uniforms U U1 = norm.cdf(sX1) U2 = norm.cdf(sX2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7851999480298125 . As before we have ended up with rank-normal correlated gamma variates. This time we have 3 potential &quot;driver&quot; variables Z, X1, X2 - all correlated with each other. It is not hard to see how this procedure can be iterated repeatedly to give arbitrarily many correlated driver variables. Further we can imagine these variables being oriented in a hierarchy, Z being at the bottom layer, X1 and X2 being a layer above, and so on. . What is a Driver? . We should now take a step back and think about the implications for the insurance aggregation problem. As stated previously this method allows us to define dependency with far fewer parameters than using a matrix approach. When you start getting into the realms of 100,000s of modelled variables this becomes increasingly important from a calibration perspective. . However there are other benefits: for example we can look at how the model variables relate to the driver variables. For example we can ask questions such as: &quot;What is the distribution of modelled variables when driver Z is above the 75th percentile&quot; and so on. This is a form of counter-factual analysis that can be performed using the model, with the matrix approaches you get no such ability. For counter-factual analysis to be useful however we require real-world interpretations of the drivers themselves. By limiting ourselves to counter-factual analysis based on driver percentiles (e.g. after the normal cdf is applied to Z, X1, X2 - leading to uniformly distributed driver variables) we make no assumption about the distribution about the driver itself, only its relationship with other drivers. . By not making a distributional assumption a driver can represent any stochastic process. This is an important but subtle point. For example we could create a driver for &quot;global economy&quot; (Z) and by taking weighted sums of these create new drivers &quot;US economy&quot; (X1) and &quot;european economy&quot; (X2). In this example there may be data driven calibrations for suitable weights to select (e.g. using GDP figures) however it is also relatively easy to use expert judgement. In my experience it is actually easier to elicit parameters in this style of model compared to &quot;correlation&quot; parameters given this natural interpretation. . Given this natural interpretation we can quite easily begin to answer questions such as: &quot;What might happen to the insurance portfolio in the case of a european economic downturn?&quot; and so on. Clearly the detail level of the driver structure controls what sort of questions can be answered. . As stated previously we can repeat the mechanics of creating drivers to create new &quot;levels&quot; of drivers (e.g. moving from &quot;european economy&quot; to &quot;French economy&quot;, &quot;UK economy&quot; and so on). We can also create multiple &quot;families&quot; of driver, for example in addition to looking at economies we may consider a family relating to &quot;political unrest&quot;, again this could be broken down into region then country and so on. Other driver families may not have a geographic interpretation - for example commodity prices. In some cases the families may be completely independent of each other, in other cases they can depend on each other (e.g. commodity prices will have some relationship with the economy). . In the examples so far we have presented a &quot;top down&quot; implementation in our examples: we start by modelling a global phenomena and then build &quot;smaller&quot; phenomena out of these. There is nothing special about this, we could have just as easily presented a &quot;bottom up&quot; implementation: take a number of &quot;Z&quot; variables to represent regions and combine these to form an &quot;X&quot; representing a global variable. Neither implementation is necessarily better than another and mathematically they lead to equivalent behaviours (through proper calibration). In practice however I have found the &quot;top down&quot; approach works better, typically you will start with a simple model and through time it can iterate and become more sophisticated. The top down approach makes it easier to create &quot;backward compatability&quot; which is a very useful feature for any modelling framework (e.g. suppose the first iteration of the framework only considers economic regions, next time a model is added which requires country splits - with top down adding new country variables keeps the economic regions identical without requiring any addtional thought.) . The need for more Sophistication: Tail Dependence . Unfortunately the model presented so far is still quite a way from being useful. We may have found a way of calibrating a joint distribution using relatively few (O(N)) parameters and can (in some sense) perform counter-factual analysis, but there is still a big issue. . So far the method only allows for rank-normal joint behaviour. From the analysis of complex systems we know that this is not necessarily a good assumption (please see other blog posts for details). We are particularly interested in &quot;tail dependence&quot;, in layman&#39;s terms: &quot;when things go bad, they go bad together&quot;. Tail dependence can arise for any number of reasons: . Structual changes in the system | Feedback | State space reduction | Multiplicative processes | Herd mentality/other human behaviours | And many others | . Given the framework we are working within we are not particularly interested in how these effects occur, we are just interested in replicating the behaviour. . To do this we will extend the framework to cover a multivariate-student-T dependence structure. To do this we note the following: $$ T_{ nu} sim frac{Z} { sqrt{ frac{ chi^2_{ nu}} { nu}}} $$ Where: $ T_{ nu} $ follows a student-t distribution with $ nu$ degrees of freedom $ Z $ follows a standard normal $N(0,1)$ $ chi^2_{ nu} $ follows Chi-Square with $ nu$ degrees of freedom . Therefore we can easily extend the model to allow for tail dependence. . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu = 3 Chi = chi2.rvs(nu, size=SIMS) sX1 /= np.sqrt(Chi / nu) sX2 /= np.sqrt(Chi / nu) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu) U2 = t.cdf(sX2, df=nu) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7907911109201866 . Adding Flexibility . We can further extend this model by allowing each model variate to have its own tail-dependence. Why is this important one might ask? In the case of this framework we are spanning many different models, selecting a single degree of tail dependence might not be suitable for all variables. We can do this via applying another inverse transform: $$ T_{ nu} sim frac{Z} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ As before but where: $U$ follows a uniform U[0,1] distribution $F^{-1}_{ chi^2_{ nu}}$ is the inverse cdf of $ chi^2_{ nu}$ . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 U = np.random.rand(SIMS) Chi1 = chi2.ppf(U,df=nu1) Chi2 = chi2.ppf(U, df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7703228652641819 . There is a small practical issue relating to multivariate student-t distributions: namely that we lose the ability to assume independence. This is a direct result of allowing for tail dependence. In many situations this is not an issue, however within this framework we have models covering very disperate processes some of which may genuinely exhibit independence. To illustrate this issue we will re-run the existing model with zero driver weights (&quot;attempt to model independence&quot;): . Estimated Pearson Correlation Coefficient: 0.08220534833363176 . As we can see there is a dependence between Y1 and Y2 0 clearly through the chi-square variates. We can overcome this issue by &quot;copying&quot; the driver process. The common uniform distribution is then replaced a number of correlated uniform distributions. We can then allow for independence. An implemntation of this can be seen in the code sample below: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7406745557389065 . To show this allows full independence we repeat the zero-weight example: . Estimated Pearson Correlation Coefficient: -0.01456173215652803 . We can see that this is a much better scatter plot if we are looking for independence! . Non-Centrality . We now extend this model yet further. So far we have allowed for tail dependence however it treats both tails equally. In some instances this can be problematic. For example if we rely on output from the framework to do any kind of risk-reward comparison the upisde and downside behaviour are both important. While it is easy to think of structural changes leading to a downside tail dependence an upside tail dependence is typically harder to justify. We can allow for this with a simple change to the model, namely: $$ T_{ nu, mu} sim frac{Z + mu} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ The addition of the $ mu$ parameter means that $T_{ nu, mu}$ follows non-central student-t distribution with $ nu$ degrees of freedom and non-centrality $ mu$. Details of this distribution can be found on wikipedia. By selecting large positive values of $ mu$ we can create tail dependence in the higher percentiles, large negative values can create tail dependence in the lower percentiles and a zero value leads to a symmetrical dependency. Adjusting the code futher we get: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers # We add non-centrality through an additive scalar import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Specify the non-centrality values nc1 = -2 nc2 = -2 # Use non-central t CDF to convert sX synthetic variables to uniforms U U1 = nct.cdf(sX1+nc1, nc=nc1, df=nu1) U2 = nct.cdf(sX2+nc2, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7100911602838634 . In the code example we have selected a non-centrality of -2 which is a fairly large negative value, we can see the dependency increasing in the lower percentiles (clustering around (0,0) on the plot). . Temporal Considerations . So far we have essentially considered a &quot;static&quot; model, we have modelled a number of drivers which represent values at a specific time period. For the majority of insurance contracts this is sufficient: we are only interested in losses occuring over the time period the contract is active. However in some instances the contracts relate to multiple time periods and it does not make sense to consider losses over the entire lifetime. Moreover it is not ideal to model time periods as independent from one another, to take the US economy example: if in 2020 the US enters recession it is (arguably) more likely that the US will also stay in recession in 2021. Clearly the dynamics of this are very complex and constructing a detailed temporal model is very difficult, however for the sake of creating the drivers we do not need to know the exact workings. Instead we are looking for a simple implementation that gives dynamics that are somewhat justifiable. . Fortunately it is relatively easy to add this functionality to the model framework we have described so far. Essentially we will adopt a Markovian assumption whereby a driver in time period t+1 is a weighted sum of its value at time t and an idiosyncratic component. Of course this is not a perfect description of the temporal behaviour of every possible driver but it shouldn&#39;t be completely unjustifiable in most instances and the trajectories shouldn&#39;t appear to be totally alien (e.g. US economy being in the top 1% one year immediately followed by a bottom 1% performance very frequently). . To illustrate this please see the code example below, for brevity I will change the model code above to a functional definition to avoid repeating blocks of code. . # Creating temporally dependent variables import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Define function to create correlated normal distributions def corr_driver(): # Create driver Z Z = np.random.normal(0, 1, SIMS) # Create drivers X1, X2 X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) return np.array([X1, X2]) # Create drivers variables for time periods t0 and t1 driver_t0 = corr_driver() driver_t1 = 0.5 * driver_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Create copy of drivers for tail process time periods t0 and t1 tail_t0 = corr_driver() tail_t1 = 0.5 * tail_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Define a standardise function def standardise(x): return (x - x.mean()) / x.std() # Create sythetic variables sX1 sX2 for variable 1 and 2 at times t0 and t1 # Note depending on the model idiosyncratic components may also be dependent sX1t0 = standardise(0.25*driver_t0[0] + 0.5*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1t1 = standardise(0.25*driver_t1[0] + 0.5*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t0 = standardise(0.5*driver_t0[0] + 0.25*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t1 = standardise(0.5*driver_t1[0] + 0.25*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Repeat synthetic variable construction for tail process sX1tailt0 = standardise(0.25*tail_t0[0] + 0.5*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1tailt1 = standardise(0.25*tail_t1[0] + 0.5*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt0 = standardise(0.5*tail_t0[0] + 0.25*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt1 = standardise(0.5*tail_t1[0] + 0.25*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Simulate Chi-Square for tail-dependence t0 and t1 nu1 = 2 nu2 = 4 Chi1t0 = chi2.ppf(norm.cdf(sX1tailt0),df=nu1) Chi2t0 = chi2.ppf(norm.cdf(sX2tailt0), df=nu2) sX1t0 /= np.sqrt(Chi1t0 / nu1) sX2t0 /= np.sqrt(Chi2t0 / nu2) Chi1t1 = chi2.ppf(norm.cdf(sX1tailt1),df=nu1) Chi2t1 = chi2.ppf(norm.cdf(sX2tailt1), df=nu2) sX1t1 /= np.sqrt(Chi1t1 / nu1) sX2t1 /= np.sqrt(Chi2t1 / nu2) # Specify the non-centrality values nc1 = 2 nc2 = 2 # Use non-central t CDF to convert sX synthetic variables to uniforms U for t0 and t1 U1t0 = nct.cdf(sX1t0+nc1, nc=nc1, df=nu1) U2t0 = nct.cdf(sX2t0+nc2, nc=nc2, df=nu2) U1t1 = nct.cdf(sX1t1+nc1, nc=nc1, df=nu1) U2t1 = nct.cdf(sX2t1+nc1, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 at t0 and t1 Y1t0 = gamma.ppf(U1t0, 2) Y2t0 = gamma.ppf(U2t0, 3) Y1t1 = gamma.ppf(U1t1, 2) Y2t1 = gamma.ppf(U2t1, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1t0, Y1t1) plt.xlabel(&#39;Y1(t=t0)&#39;) plt.ylabel(&#39;Y1(t=t1)&#39;) plt.show() correl = np.corrcoef(Y1t0, Y1t1) print(&quot;Estimated Pearson Auto-Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Auto-Correlation Coefficient: 0.37600307233845764 . In this code example we created to variables Y1 and Y2, each one taking a value from a Gamma distribution at times t0 and t1. Y1 and Y2 have a dependency between eachother but also temporally. . As with any temporal model the time period chosen is very important, typically for insurance contracts yearly time periods make sense. However in one particular model I developed there was a need for monthly simulations, rather than re-parameterising the entire central driver structure to work on a monthly basis (creating lots of extra data that will not be used by the vast majority of the models) I applied a &quot;Brownian Bridge&quot; type argument to interpolate driver simulations for each month. . Notes on Implementation . In this blog post I have not included the code exactly as it is implemented in production since this is my employer&#39;s IP. The implementation presented here is not very efficient and trying to run large portfolios in this way will be troublesome. In the full production implementation I used the following: . Strict memory management as the this is a memory hungry program | Certain aspects of the implementation are slow in pure python (and even Numpy) Cython and Numba are used for performance | The Scipy stats module is convenient but restrictive, it is better to either use the Cython address for Scipy special functions or implement functions from scratch. By implementing extended forms of some of the distribution functions one is also able to allow for non-integer degrees of freedom which is useful | The model naturally lends itself to arrays (vectors, matrices, tensors) however these tend to be sparse in nature, it is often better to construct &quot;sparse multiply&quot; type operations rather than inbuilt functions like np.dot | Conclusion . This blog posts represents the current iteration of the aggregation framework I have developed. It is considered a &quot;version 0.1&quot; implementation and is expected to develop as we use it more extensively and uncover further properties or issues. For example it is clear regardless of parameters selected the joint behaviour will always be (approximately) elliptical, as presented it is not possible to implement non-linearities (e.g. the price of some asset will only attain a maximum/minimum value dependent on some other driver indicator). It is not difficult to implement ideas like this when the need arises, the difficulty becomes more around how to implement the idea in a seamless way. . There are a couple of additional benefits to this framework which we have not mentioned, I will outline these here briefly: . It is possible to parallelise this process quite effectively as there are minimal bottlenecks/race conditions | The driver variables can be generated centrally and models can link to this central variable repository. From a work-flow perspective this means that individual underwriting teams can run models independently (quickly) leaving the risk teams to collate an analyse the results. (Sometimes called a federated workflow.) | The federated workflow means no specialist hardware is required, even very large portfolios can be run on standard desktops/laptops. | The current production version of this framework has around 5-10,000 driver variables (&quot;X1, X2&quot;) over 5 different hierarchical layers. These influence dependence between around 500,000 individual modelled variables (&quot;Y1, Y2&quot;) with 20 time periods (&quot;t0, t1&quot;). The quality of risk management analysis and reporting has increased dramatically as a result. . There are still some things left to do in relation to this framework and the work is on-going. These include: . Work relating to calibration and how to do this as efficiently as possible | Further work on increasing code efficiency | Further mathematical study of the framework&#39;s parameters | Study of the implied network behaviour: since we&#39;re placing risks on a network (driver structure) can we gain additional insight by considering contagion, critical nodes, etc.? | Further improvements to the workflow, how the model data is stored/collated etc. |",
            "url": "https://www.lewiscoleblog.com/insurance-aggregation-model",
            "relUrl": "/insurance-aggregation-model",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Welcome",
          "content": "Lewis Cole 2020 . Hello, and welcome to my new blog. I have never written a blog before and so this is likely to be a bit of a work in progress slowly evolving in time. . Within this blog I aim to talk about some ideas that are of interest to me. My interests are quite broad and so there will likely be a wide variety of topics discussed. I suppose most, if not all, of these interests could be placed under the broad umbrella of â€œmodelsâ€ or â€œsimulationâ€. Some particular areas or topics I wish to write about include: . Non-linear dynamics | Systems out of equilibrium | Fat-tail and extreme value statistics | Non-ergodicity | Path dependence | Individual vs Collective phenomena | Agent based modelling | Networks | Machine Learning / Data Analysis | Mathematics, Probability, Computer Science generally | Inter-disciplinary study | . Many of these areas could be considered â€œcomplexâ€ or â€œcomplex systemsâ€ although I am not a big fan of the term due to lack of a consistent definition. In many cases we must rely on computational methods since traditional analytic methods tend to fall down. As a result most blog posts will contain sample code, I will write this in Python (with a variety of libraries/packages) owing to ease of understanding and itâ€™s ubiquity. As such I will also write about more computational considerations such as: . Python packages | Other languages | Writing performant python | Optimization techniques | and so on | . From time to time I might also include more â€œthought piecesâ€ on news/recent research or book reviews or similar. . Where applicable I will try and assume no specific knowledge of a particular subject and try and build up to a somewhat sophisticated level of understanding. However I will be forced to assume a certain level of mathematical/statistical/programming understanding as I would rather not write articles on the fundamentals, where possible I will try and mention the name of techniques used so if they appear unfamiliar it will at least be possible to search for resources online. . To create this blog I used the fast.ai fastpages which has made the job much easier. I have a great deal of gratitude to Jeremy Howard, Hamel Husain and the folks at fast.ai for making this simple as possible so I can focus on creating content instead of worrying about the technicalities of creating the blog. . Thank you for your time, I hope you enjoy my blog posts. . If you would like to get in contact with me you can via my twitter account or via this blogâ€™s email: .",
          "url": "https://www.lewiscoleblog.com/welcome/",
          "relUrl": "/welcome/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

}