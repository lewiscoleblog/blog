{
  
    
        "post0": {
            "title": "Monte Carlo Methods 3: Vine Copula Methods",
            "content": ". This is the third blog post in a series - you can find the previous blog post here . . In this blog post we continue to look at the problem of sampling from multi-variate distributions. Recall from the previous blog post that a copula $C(.)$ is a multivariate function: . $$ C(U_1, , ... , , U_N) = mathbb{P}(X_1 leq F_1(U_1), , ... , , X_N leq F_N(U_N)) $$ . We note that this defines a multivariate CDF. We can find the corresponding density function (PDF) $c(.)$ in the usual way: . $$ c(u_1, , ... , , u_N) = frac{ partial^N}{ partial u_1 , ... , partial u_N} C(u_1, , ... , , u_N) $$ . Using this representation of the copula we can denote the joint distribution density $f(.)$ via a factorization of the copula density above and the marginal densities $f_i(.)$ (with corresponding CDF $F_i(x_i)$) via: . $$ underbrace{f(x_1, , ... , x_N)}_ text{Joint density} = underbrace{f_1(x_1) , ... , f_N(x_N)}_ text{Marginal density} , , , underbrace{c(F_1(x_1), , ... , ,F_N(x_N))}_ text{Copula density}$$ . This highlights the power of the copula method showing that in modelling we are able to separate the marginal and joint behaviours into independent parts. . Problems with the Copula Method . Things are not as rosy as they seem however. We can use data to inform the marginal density functions, this is well established and we have a wide number of techniques at our disposal. However the copula density remains problematic - in anymore than the bi-variate case we are unlikely to ever have enough data inform model selection and calibration. We thus have to rely quite heavily on &quot;trial and error&quot; to find the correct structure to use and furthermore calibrate the parameters of that structure to give the desired result. . We also have issues in higher dimension copula based modelling. Typically an archimedean copula will not be suitable in high dimensions. For an archimedean copula to be used it typically the generator needs to be well understood mathematically which results in only a handful being widely used. Of those used typically there will only be a handful of parameters and so they are not always flexible. For example the usual implemenation of the multi-variate Gumbel copula (used for modelling upper-tail dependence in high dimension) has a single parameter controlling the degree of tail dependence (and overall dependency) - this is not very useful where we think some subset of variables will be more strongly tied than others. . As a result high-dimension multi-variate modelling typically relies on the joint-elliptical methods (typically the Gaussian/rank-normal or the student-t). In their standard forms these themselves have a number of issues: . We need to specify an entire depedency matrix. This doesn&#39;t scale well with size and often adding a few new variables requires us to completely re-calibrate the matrix from scratch in order to ensure sensible behaviour, positive definite-ness and so on | The Gaussian shows no tail dependence and the standard student-t copula allows only for 1 parameter controlling the tail dependence of all variables | The tail dependence on the student-t is symmetrical which is not always desired | With the student-t copula we cannot enforce independence | . To overcome this we can apply various techniques, generalizations and extensions to (at least partially) overcome the issues above (e.g. see my insurance aggregation model). However these methods still lack the flexibility to model certain relations. For example suppose we have 3 variables $(X, Y, Z)$ we want to apply some dependence structure such that $X$ and $Y$ have a tie and $X$ and $Z$ have a tie - but given $Z$ then $X$ and $Y$ are independent. This can arise in various scenarios and yet it is very difficult to capture this through a joint-elliptical copula. We therefore require a more powerful method. . Vine Copula . This is where the vine copula comes in. As with many computational statistical methods we want to produce a general tool that will work in a wide variety of situations. In the case of multi-variate sampling we want something that scales nicely with dimension. In order to do this we will look ot specify the dependency structure using a probabilistic graphical model (PGM). . We will not get into all the details surrounding PGMs since that is a field of study all to itself. Instead we will just note that a graph such as that seen below: . . Can be used to specify a joint density by factorising over cliques in the undirected graph: . $$ f(X_A, X_B, X_C, X_D, X_E, X_F) = f_{ABC}(X_A, X_B, X_C) f_{CD}(X_C, X_D) f_{DEF}(X_D, X_E, X_F) $$ . We can note that this has more than a passing resemblence to our joint distribution decomposition using copulae above. The main crux of the vine copula is finding a decomposition of a copula using a graphical model, we can then specify individual copulae on each clique (e.g. in the example above clique $(A,B,C)$ could be defined with a rank-normal while clique $(C,D)$ via a gumbel, and so on). Of course this offers us tremendous flexibility, to the point of leading to option paralysis! Certain simplifications have been specified to aid this. . In vine copulae the graphical model used is, as the name suggests, a vine (or cantor tree). By enforcing this requirement we bypass the need for worrying about loops and all the other technical gubbins that make working with graphical models generally quite difficult. . . Regular Vines . We start by defining vines in a more precise way. A vine $ mathcal{V}$ defined on $N$ variables is a nested set of connected trees: $ mathcal{V} = {T_1 , , , ... , , , T_{N-1} }$. The nodes of $T_{j+1}$ being the edges of $T_{j}$. A regular vine is a vine where two edges in $T_j$ are joined by an edge in $T_{j+1}$ only if the two edges share a common node. It is much easier to work with regular vines than non-regular vines. We can define this more formally: . . Definition: Regular Vine . $ mathcal{V}$ is a regular vine on $N$ elements with edge set: $E( mathcal{V}) = E_1 cup E_2 cup , ... , cup E_{N-1}$ if: . $ mathcal{V} = {T_1 , , , ... , , , T_{N-1} }$ | $T_1$ is a connected tree with nodes $N_1 = {1 , , , ... , , , N-1 }$ and edges $E_1$. For $i=2, ... N-1$ : $T_i$ is a tree with nodes $N_i = E_{i-1}$ | For $i=2, ... N-1$ : for $ {a,b } in E_i$ we have: $|a triangle b| = 2$ for $ triangle$ the symmetric difference operator | . We can further delineate regular vines into C-vines (canonical vines) where for each tree $T_i$ there is a unique node with $n-i$ connecting edges (degree) - this is the maximum degree for any node at that level. A regular vine is called a D-vine if all nodes in $T_1$ have a degree no higher than 2. Of course we can also have a vine that is not a C-vine nor a D-vine while still satisfies the definition above - we call these R-vines. In some sense an R-vine is &quot;inbetween&quot; a C-vine and a D-vine - the C-vine and D-vine being &quot;boundary conditions&quot; in some sense. . It is often easier to see examples than battle with formal definitions (especially with PGMs) so lets look at an example in each class: . . Alternatively we can look at the sequence of trees separately: . . If we expressed this vine in the form from before it would look like: . . We can also make some further definitions: . . Defintion: Constraint Set, Conditioning Set, Conditioned Set . For $e in E_i$ ($i leq N-1$) the constraint set associated with $e$ is the complete union $U_e^*$ - the subset of $ {1, , ... , , N }$ reachable from $e$ by the membership relation | For $i leq N-1$ and $e in E_i$ : if $e= {j, k }$ then the conditioning set associated with $e$ is: $$ D_e = U_j^* cap U_k^*$$ The conditioned set associated with $e$ is: $$ { C_{e,j} , C_{e,k} } = { U_j^* backslash D_e, U_k^* backslash D_e }$$ | . In the example above the edge XXX has conditioning set XXX and conditioned set XXX - hence the choice of naming convention! . For a regular vine $ mathcal{V} = {T_1 , , , ... , , , T_{N-1} }$ we have the following properties: . The number of edges is $N(N-1)/2$ | Each pair of variables occurs exactly once as a conditioned set | If two edges have the same conditioning set then they are the same edge | The scaling of regular vines works as so: . For any regular vine on $(N-1)$ elements there are $2^{N-3}$ choices of $N$-dimensional regular vine that extend it | There are ${N choose 2} (N-2)!2^{(N-2)(N-3)/2}$ labelled regular vines | We can thus see that model selection for regular vine copula methods can quickly become unweildly. We often need to rely on sophisticated methods to do this (we will not discuss these here). We just note that we can set up &quot;equivalence classes&quot; of regular vines where two regular vines are in the same class if there is a permutation of the conditional distribution variables that maps one to the other. For example if $N=3$ then for a D-vine we have conditional distributions for: $ { (1,2), (2,3), (1,3)|2 }$ and for a C-vine we have: $ { (1,2), (1,3), (2,3)|1 }$ by taking permutation: $ pi(1,2,3)=(2,1,3)$ we have equivalence - that is C-vines and D-vines are equivalent for $N=3$ (of course this is not true in general). . The real power of the regular vine copula comes through the following theorem from Bedford and Cooke (shown without proof). . . Theorem: . Let $ mathcal{V} = {T_1 , , , ... , , , T_{N-1} }$ be a regular vine over $N$ elements. For any edge $e = {e_1, e_2 } in E( mathcal{V})$ with conditioning set $D_e$, let a bivariate conditional copula be denoted: $C_{e_1,e_2 | D_e}$ with density: $c_{e_1,e_2 | D_e}$. Let marginal distributions be denoted as $F_i$ with corresponding densities $f_i$. Then the vine-dependent multivariate distribution is uniquely determined and has density: $$f_{X_1...X_N}(x_1,...x_N) = left( prod_{i=1}^N f_i(x_i) right) left( prod_{e in E( mathcal{V})} c_{e_1,e_2 | D_e} ( F_{e_1 | D_e}(x_{e_1}) , F_{e_2| D_e}(x_{e_2})) right) $$ . . Just as Sklar defined a bijection between multi-variate uniform distributions and copulae, this theorem does the equivalent for regular vine copulae. From before we know that we have many options for bivariate relations (for example we looked at the Gaussian, student-t, Frank, Clayton and Gumbel copulae in the previous blog post) - whereas multi-variate relations are harder to specify (with typically only Gaussian and student-t used in practice). The regular vine copula method allows us more flexibility in modelling multi-variate distributions. Moreover we can make use of conditioning arguments to help specify fairly specialised relations. . Through construction we can notice the following properties: . If each bivariate copula specified is of Gaussian type then resulting multi-variate copula is also a Gaussian. This also has the property that conditional correlation does not depend on the conditioning varibles. We find similar results for the student-t | If edge $e = {e_1, e_2 } in E_l$ with $l &gt; 1$. Then if copula $C_e$ is more concordant than $C_e&#39;$ then the margin $F_e$ is more concordant than the margin $F_e&#39;$ | Of $C_e$ has upper/lower tail ependence for all $e in E_1$ and the remaining copulae have support in $[0,1]^2$ then all bivariate margins of the joint distribution have upper/lower tail dependence | For parametric copulae $C_e(.; theta_e)$ then each marginal copulae exists within the Frechet upper/lower bounds (see previous blog post for details) | . These points are a rather fancy way of saying that through the regular-vine construction we get a multi-variate distribution that behaves how we would like/expect. . We also now take a brief diversion into the definition of a partial correlation. . . Definition: Partial Correlation . Let $X_i$ be a variable with zero mean and unit variance. For $i=1,...,N$ define numbers: $b_{ij: {1,...,,N } backslash {i,j }}$ that minimise: . $$ mathbb{E} left[ left( X_i - sum_{j neq i} b_{ij: {1,...,,N } backslash {i,j }} X_j right)^2 right]$$ . We then define the parital correlation coefficient: $ rho_{ij: {1,...,,N } backslash {i,j }}$ as: . $$ rho_{ij: {1,...,,N } backslash {i,j }} = operatorname{sgn} left(b_{ij: {1,...,,N } backslash {i,j }} right) left(b_{ij: {1,...,,N } backslash {i,j }} b_{j,i: {1,...,,N } backslash {i,j }} right)^{1/2} $$ . We can think of this as the correlation between the projections of $X_i$ and $X_j$ onto a plane orthogonal to the space spanned by all other variables. We can equivalently write this as: . $$ rho_{ij: {1,...,,N } backslash {i,j }} = - frac{K_{ij}}{ sqrt{K_{ii}K_{jj}}} $$ . Where $K_{ij}$ is the $(i,j)$th co-factor of the correlation matrix. . . There is nothing special about this definition that is unique to the vine copula methods we are describing. We could equally define this for any Gaussian or student-t or other elliptical multivariate distribution. For those a little rusty on their linear algebra the $(i,j)$th co-factor of a matrix is equal to the determinant of the matrix after removing the $i$th row and $j$th column multiplied by the factor: $(-1)^{i+j}$. . We mention this here since we have the following theorem: . . Theorem: . The regular vine provides a bijective mapping from $(-1,1)^{N choose 2}$ to the set of positive definite matrices with $1$ diagonals (i.e. valid joint-elliptical correlation matrices) . . This means that we can use regular vines as a parameterization tool for joint elliptical (e.g. Gaussian) multi-variate distributions. We do this by specifying partial correlations on each edge in $E( mathcal{V})$. . Dependency Between Discrete Variables . So far we have not differentiated between sampling continuous and discrete variables (e.g. we generate our copula and then use a generalize inverse transform to return dependent variables of arbitrary distribution be they Gaussian or Poisson). Unfortunately things are not quite that simple if we are targetting a specific level of Pearson correlation (note that previously we showed why Pearson correlation is an imperfect measure, but in practical situations it is sometimes the only metric we have). Let&#39;s use the following definitions of variables: . $(Z_1,...,Z_N)$ are sample from a multivariate distribution with marginal distribution functions $G_i(.)$ | $(U_1,...,U_N) = (G_1(Z_1),...,G_N(Z_N))$ are rank dependent uniforms corresponding to $Z_i$ | $(X_1,...,X_N) = (F^{-1}_1(U_1),...,F^{-1}_2(U_2))$ are our discrete variable transforms | We then have that in general: . $$Corr_{Pearson}(Z_i, Z_j) neq Corr_{Pearson}(U_i, U_j) neq Corr_{Pearson}(X_i, X_j)$$ . This is not always rue for continuous variables e.g. when a Gaussian multi-variate and suitably chosen $F(.)$. To overcome this for discrete variables we often have to adjust our input parameters and sensitivity test until a desired output correlation is reached. In the case of large $N$ this quickly becomes prohibitive and we simply have to accept the biases our modelling procedure has, in some cases this can lead to large mis-statements in the model and thus we can draw incorrect conclusions. . It has been shown (in the Gaussian dependency, Poisson/eneralized Poisson/negative-binomial marginal cases) that regular vines have lower biases and do a better job at representing the input calibration. This is especially useful in cases of high dispersion where the typical approaches can lead to large inaccuracies. . Sampling from Regular Vines . Sampling the D-Vine . We will now move onto the problem of sampling from the D-Vine, we start with the D-Vine since it&#39;s structure makes it easier to sample from and provides a good foundation and further explanation of what we have seen so far. . Inverse Transform Method . Let $(u_1, u_2, u_3, u_4)$ be 4 iid samples from a uniform distribution. We use the notaion: $F_{i | j:x_j}$ to be the cumulative distribution function of $X_j$ given that $X_i = i$. Then we can produce a sample $(x_1, x_2, x_3, x_4)$ from the joint distribution $(X_1, X_2, X_3, X_4)$ specified by our D-Vine as: . begin{align} x_1 &amp;= u_1 x_2 &amp;= F^{-1}_{2|1:x_1}(u_2) x_3 &amp;= F^{-1}_{3|2:x_2}(F^{-1}_{3|12:F_{1|2}(x_1)}(u_3)) x_4 &amp;= F^{-1}_{4|3:x_3}(F^{-1}_{4|23:F_{2|3}(x_2)}(F^{-1}_{4|123:F_{1|23}(x_1)}(u_4))) end{align}We can simply repeat this to arbitrarily high dimension. A similar approach exists for regular vines but we need to be more careful about the labelling and ordering of variables. . Acceptance-Rejection Method . From before recall that: . $$c(x_1,...,x_N) = prod_{e in E( mathcal{V})} c_{e_1,e_2 | D_e} ( F_{e_1 | D_e}(x_{e_1}) , F_{e_2| D_e}(x_{e_2}))$$ . We can use this in an acceptance-rejection scheme in the usual way: we generate multi-variate uniform variates and then accept/reject samples based on this density above. This is no different from what we have seen before and the usual observations about acceptance rates, etc. still apply. . We can see that while the specification of the vine copulae are slightly different to the copulae we looked at previously we can see that we can sample using the same tools as before. . Generic Sampling Method . We now move onto looking at sampling from vine copulas with general dimensions. We will present these in pseudocode. In all examples the &quot;output&quot; will be the variables $(u_1,...,u_N)$ which are multi-variate uniform with the vine copula induced dependency. Other interim values will be calculated. . . Sampling from the C-vine . Generate $w_1,...,w_N$ iid $U[0,1]$ variates | Set: $u_1 = w_1$ | Set: $u_2 = C^{-1}_{2|1}(w_2 |w_1)$ | For $i &gt; 2$: Set $t=w_i$ | For $j = i - 1, i-2,...,1$: $t = C^{-1}_{i |j:1,...,j-1}(t|w_j)$ | | $u_i = t$ | | Return: $(u_1,...,u_N)$ | . . Sampling from the D-vine . Generate $w_1,...,w_N$ iid $U[0,1]$ variates. Initialize $NxN$ matrices $A=(a_{i,j})$ and $B(b_{i,j})$ | Set: $u_1 = a_{1,1} = b_{1,1} = w_1$ | For $i geq 2$: $a_{i,1} = w_i$ | For $j=2...i$: $a_{i,j} = C^{-1}_{i |j-1:j,...,i-1}(a_{i,j-1}|b{i-1,j-1})$ | | ui = b{i,i} = a_{i,i} | For $j=i-1,..,1$: $b_{i,j} =C_{j|i:j+1,...,i-1}(b_{i-1,j}|b{i,j+1})$ | | | Return $(u_1,...,u_N)$ | . Sampling from an R-vine requires a little more work in labelling the variables due to the lack of structure. We use indexing: . $$ 12, k_{3,1}3; k_{3,2}3|k_{3,1}; ... ; k_{i,1}i; k_{i,2}i | k_{i,1}; ... ; k_{i,i-1}i|k_{i1}; ... ; k_{n,1}n&#39; ...; k_{n,n-1}n|k_{n,1}; ... k_{n,n-2} $$ . With: $(k_{i,1}, ... , k_{i,i-1})$ some pemutation on $(1,...,i-1)$ (e.g. C-vine $k_{i,j} =j$ and D-vine $k_{i,j} = i-j$). With this in mind we can move on to general R-vine sampling. . . Sampling from the R-vine . Generate $w_1,...,w_N$ iid $U[0,1]$ variates. Initialize $NxN$ matrices $A=(a_{i,j})$ and $B(b_{i,j})$ | Set: $u_1 = a_{1,1} = b_{1,1} = w_1$ | For $i geq 2$: $M= { k_{i,1}, ... , k_{i,i-2} }$ | $v_{i,k_{i,i-1},M} = v_{i, { k_{i,i-1} cup M }} = w_i$ | $a_{i,1} = w_i$ | For $j geq 2$ $M= { k_{i,1}, ... , k_{i,i-j} }$ | $v_{i,M} = C^{-1}_{i |k_{i,i+1-j}:M}(a_{i,j-1}|v_{i,i+1-j,M})$ | $a_{i,j} = v_{i,M}$ | | $u_i = a_{i,i}$ | $b_{i,i} = a_{i,i}$ | For $j=i-1,...1$: $M= { k_{i,1}, ... , k_{i,i-1-j} }$ | $v_{k_{i,i-j},i,M} = C_{k_{i,i-j}|i:M}(v_{k_{i,i-j},M}|a_{i,j+1})$ | $b_{i,j} = v_{k_{i,i-j},i,M}$ | | | Return $(u_1,...,u_N)$ | . Vine-Copula packages . Thankfully there are pre-canned packages that we can use to sample from vines (as well as performing calibration procedures and other things we have not looked at in this blog). The R language perhaps has the most widely used (given it is used by statistiticians) in the &quot;VineCopula&quot; package. In python we have the following: pyvine, pyvinecopulalib and others. . Conclusion . In this blog post we have seen a gentle introduction to the concept of vine copulae, what they are, how to classify them and how to sample from them. In general vine copulae are useful in modelling high dimension multi-variate systems. They tend to perform best (in this authors experience) when the vine structure is sparse (i.e. lots of pairwise relations are independent) and on occasion very tail dependent behaviour can cause instability (although this is also true of other copula methods also). Although we have not covered the process of calibrating vine copulae there are a lot of powerful methods available to us in order to do this which is very useful. Vine copulae are increasingly used as a way of simulating &quot;synthetic data&quot; in order to feed the ever more data-hungry deep machine learning methodologies. . . References . The homepage for all things vine copula is: homepage . In preparing this blog post I relied heavily on the book: . Dependence Modelling: Vine Copula Handbook - Dorota Kurowicka and Harry Joe .",
            "url": "https://www.lewiscoleblog.com/monte-carlo-methods-3",
            "relUrl": "/monte-carlo-methods-3",
            "date": " â€¢ May 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Monte Carlo Methods 2",
            "content": ". This is the second blog post in a series - you can find the previous blog post here . . In this blog post we shall continue our exploration of Monte Carlo methods. To briefly recap in the previous blog post we looked at the general principle underlying Monte-Carlo methods, we looked at methods used by pseudo-random number generators to feed our models and we looked at a variety of methods to convert these uniform variates to general univariate distributions. . Multi-Variate Relations . We start by looking at the problem of sampling from general multi-variate distributions. In many instances simply sampling univariate distributions will be insufficient. For example if we are creating a model that looks of mortgage repayment defaults we do not want to model the default rate by one univariate distribution and the interest rate by another univariate distribution using the techniques described so far. In an ideal world we would know the exact relationship we are trying to model, in physics and the hard sciences the exact mechanism underlying the joint behaviour may be modelled exactly. However in many cases in finance (and the social sciences) this is very hard to elicit (if not impossible) so we want to create &quot;correlated&quot; samples that capture (at least qualitatively) some of the joint behaviour. . Measures of Correlation . We start by looking at some measures of dependance between variables. This helps us evaluate whether a model is working as we would expect or not. A lot of times joint-dependence is difficult to elicit from the data so often with this sort of model there is a lot of expert judgements, sensitivity testing, back testing and so on in order to calibrate a model. This is out of the scope of what I&#39;m looking to cover in this blog post series but is worth keeping in mind for this section on multi-variate methods. . By far the most prevelant measure of dependance is the Pearson correlation coefficient. This is the first (and in many cases only) measure of depedence we find in textbooks. We can express it algebraically as: $$ rho_{(X,Y)} = frac{ mathbb{E}[(X - mathbb{E}(X))(Y - mathbb{E}(Y))]}{ sqrt{ mathbb{E}[(X - mathbb{E}(X))^2] mathbb{E}[(Y - mathbb{E}(Y))^2]}} = frac{Cov(X,Y)}{SD(X)SD(Y)}$$ . That is the ratio of the covariance between two variables divide by the product of their standard deviations. This measure has a number of useful properties - it gives a metric between $[-1,1]$ (via the Cauchy-Schwarz inequality) which varies from &quot;completely anti-dependant&quot; to &quot;completely-dependant&quot;. It is also mathematically very tractable, it is very easy to calculate and it often crops up when performing an analytic investigation. However it&#39;s use in &quot;the real world&quot; is fairly limited, it is perhaps the most abused of all statistics. . One issue with this measure in practice is that it requires defined first and second moments for the definition to work. We can calculate this statistic on samples from any distribution, however if we are in the realms of fat-tailed distributions (such as a power-law) this sample estimate will be meaningless. In finance and real-world risk applications this is a big concern, it means that many of the early risk management models still being used that do not allow for the possiblity of fat-tails are at best not-useful and at worst highly dangerous for breeding false confidence. . Another issue with the measure is it is highly unintuitive. Given the way it is defined the difference between $ rho=0.05$ and $ rho = 0.1$ is negligible, yet the difference between $ rho = 0.94$ and $ rho = 0.99$ is hugely significant. In this authors experience even very &quot;technical&quot; practitioners fail to remember this and treat a &quot;5% point increase&quot; as having a consistent impact. This issue is compounded further when dealing with subject matter experts who do not necessarily have mathematical/probability training but have an &quot;intuitive&quot; grasp of &quot;correlation&quot;! . The biggest and most limiting factor for the Pearson coefficient however is that it only considers linear relationships between variables - any relation that is not linear will effectively be approximated linearly. In probability distribution terms this is equivalent to a joint-normal distribution. That is to say: Pearson correlation only exhibits good properties in relation to joint-normal behaviour! . Another option to use is the Spearmann correlation coefficient. This is strongly related to the Pearson correlation coefficient but with one important difference: instead of using the values themselves we work with the percentiles instead. For example suppose we have a standard normal distribution and we sample the point: $x = 1.64...$ we know that this is the 95th percentile of the standard normal CDF so we would use the value: $y=0.95$ in the definition of Pearson correlation above. This has the benefit that it doesn&#39;t matter what distributions we use we will still end up with a reasonable metric of dependance. In the case of joint-normality we will have that the Pearson and Spearmann coefficients are equal - this shows that Spearmann is in some sense a generalization of the Pearson. . However before we get too excited the issues around it being an unintuitive metric still stands. It also has the added complication that if we observe data &quot;in the wild&quot; we don&#39;t know what the percentile of our observation is! For example suppose we&#39;re looking at relationships between height and weight in a population: if we find a person of height: $1.75m$ what percentile does this correspond to? We will likely have to estimate this given our observations which adds a layer of approximation. In the case of heavy-tail distributions this is particularly problematic since everything may appear to be &quot;normal&quot; for many many observations but suddenly 1 extreme value will totally change our perception of the underlying distribution. . Another issue is that while the Spearmann doesn&#39;t rely only on linear relations - it does require at least monotone relations. Anything more complicated that that and the relationship will not be captured. Let&#39;s look at a few simple examples to highlight the differences between these metrics: . # Examples of Pearson and Spearman coefficients import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm, rankdata, pearsonr import seaborn as sns %matplotlib inline U = np.random.random(1000) X = norm.ppf(U) Y_lin = 4*X + 5 Y_exp = np.exp(X) Y_con = X**2 # Create function to return estimated percentile def pctile(x): return rankdata(x) / x.shape[0] # Calculate Pearson coefficients pea_lin = pearsonr(X, Y_lin)[0] pea_exp = pearsonr(X, Y_exp)[0] pea_con = pearsonr(X, Y_con)[0] # Calculate Spearman coefficients X_pct = pctile(X) Y_lin_pct = pctile(Y_lin) Y_exp_pct = pctile(Y_exp) Y_con_pct = pctile(Y_con) spe_lin = pearsonr(X_pct, Y_lin_pct)[0] spe_exp = pearsonr(X_pct, Y_exp_pct)[0] spe_con = pearsonr(X_pct, Y_con_pct)[0] # Create Plots fig, ax = plt.subplots(1, 3, figsize=(15, 5)) sns.scatterplot(X, Y_lin, ax=ax[0]) ax[0].set_ylabel(&quot;Y=4X+5&quot;) ax[0].set_xlabel(&quot;X n Pearson: %f n Spearman: %f&quot; %(pea_lin, spe_lin)) sns.scatterplot(X, Y_exp, ax=ax[1]) ax[1].set_ylabel(&quot;Y=exp(X)&quot;) ax[1].set_xlabel(&quot;X n Pearson: %f n Spearman: %f&quot; %(pea_exp, spe_exp)) sns.scatterplot(X, Y_con, ax=ax[2]) ax[2].set_ylabel(&quot;Y=X^2&quot;) ax[2].set_xlabel(&quot;X n Pearson: %f n Spearman: %f&quot; %(pea_con, spe_con)) fig.suptitle(&quot;Comparison of Pearson and Spearman Correlation&quot;, x=0.5, y=0.95, size=&#39;xx-large&#39;) plt.show() . We can see here that for a linear translation both metrics produce the same result. In the case of a non-linear monotone (e.g. exponential) relation the Spearman correctly identifies full dependancy. However neither metric is capable of capturing the dependency in the last non-monotone example - in fact both metrics suggest independence! From a basic visual inspection we would not describe these two variables as being independent. This shows we need to be careful using correlation metrics such as these. This is even before the introduction of noise! . Another somewhat popular choice of dependancy metric is the Kendall-Tau metric. This is a little different to the preceeding options. Essentially to calculate the Kendall-Tau we look at the joint pairs of samples, if both are concordant we add $1$ to a counter otherwise add $-1$ and move onto the next pair. We then take the average value of this (i.e. divide by the total number of pairs of joint samples). We can equally denote this by the formula: $$ tau ={ frac {2}{n(n-1)}} sum _{i&lt;j} operatorname{sgn}(x_{i}-x_{j}) operatorname{sgn}(y_{i}-y_{j})$$ . Where $ operatorname{sgn}(.)$ is the sign operator. $x_i$ and $y_i$ represent the rank or percentile of each sample within the set of all $x$ or $y$ samples. The benefit of the Kendall-tau is it is slightly more intuitive than the Spearman however it still suffers from some of the same issues (e.g. it will fail the $X^2$ example above). . We can show that Pearson, Spearman and Kendall are both particular cases of the generalized correlation coefficient: $$ Gamma ={ frac { sum _{{i,j=1}}^{n}a_{{ij}}b_{{ij}}}{{ sqrt { sum _{{i,j=1}}^{n}a_{{ij}}^{2} sum _{{i,j=1}}^{n}b_{{ij}}^{2}}}}}$$ . Where $a_{ij}$ is the x-score and $b_{ij}$ the y-score for pairs of samples $(x_i, y_i)$ and $(x_j, y_j)$. . For example for the Kendall Tau we set: $a_{ij} = operatorname{sgn}(x_i - x_j)$ and $b_{ij} = operatorname{sgn}(y_i - y_j)$. For Spearman we set: $a_{ij} = (x_i - x_j)$ and $b_{ij} =(y_i - y_j)$. Where again we are working with ranks (or percentiles) rather than sampled values themselves. . Can we improve on these metrics from a practical standpoint? The answer is: yes! Unfortunately it is difficult to do and there is as much art as there is science to implementing it. We instead consider the mutual-information: $$I(X, Y) = int int p_{X,Y}(x,y) operatorname{log} left( frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)} right) dx dy $$ . Or similar for discrete distributions. Here $p_{X,Y}(.)$ represents the joint pdf of $(X,Y)$ and $p_X$, $p_Y$ represent the marginal distributions of $X$ and $Y$ respectively. This has the interpretation that it represents the information gained in knowing about the joint distribution compared to assuming independance of the marginal distributions. This is essentially matches our intuition of what dependance &quot;is&quot;. However it is a bit of a pain to work with since we usually have to make distributional assumptions. . We can use generalizations of mutual information also. For example total correlation: $$C(X_{1},X_{2}, ldots ,X_{n})= left[ sum _{{i=1}}^{n}H(X_{i}) right]-H(X_{1},X_{2}, ldots ,X_{n}) $$ . Which is the sum of information for each marginal distribution less the joint information. This compares to mutual information that can be expressed: $$ operatorname{I} (X;Y)= mathrm {H} (Y)- mathrm {H} (Y|X) $$ . Another possible generalization to use is that of dual-total correlation which we can express as: $$D(X_{1}, ldots ,X_{n})=H left(X_{1}, ldots ,X_{n} right)- sum _{i=1}^{n}H left(X_{i} mid X_{1}, ldots ,X_{i-1},X_{i+1}, ldots ,X_{n} right) $$ . In the above we use the functional $H(.)$ to represent information: $$H=- sum _{i}p_{i} log _{2}(p_{i}) $$ . Or similar for continuous variables. . Despite the complications in working with these definitions these functions do pass the &quot;complicated&quot; examples such as $Y=X^2$ above and are a much better match for our intuition around dependence. I have long pushed for these metrics to be used more widely. . Other Measurements of Dependence . We now appreciate that there are many different ways in which random variables can relate to each other (linear, monotone, independent, and so on.) So far the metrics and measures presented are &quot;overall&quot; measures of dependence, they give us one number for how the variables relate to each other. However in many cases we know that relations are not defined by just one number - typically we may find stronger relations in the tails (in extreme cases) than in &quot;everyday&quot; events. In the extreme we may have independence most of the time and yet near complete depednence when things &quot;go bad&quot;. For an example of this consider we are looking at property damage and one factor we consider is wind-speed. At every day levels of wind there is unlikely to be much of a relationship between wind-speed and property damage - any damage that occurs is likely to be due to some other cause (vandalism, derelict/collapsing buildings, and so on). However as the wind-speed increases at a certain point there will be some property damage caused by the wind. At more extreme levels (say at the hurricane level) there will be almost complete dependence between wind level and property damage level. It is not difficult to think of other examples. In fact you&#39;ll likely find that most relations display this character to some degree often as a result of a structural shift in the system (e.g. in the wind example through bits of debris flying through the air!) Of course we would like to model these structural shifts exactly but this is often complicated if not impossible - through Monte-Carlo methods we are able to generate samples with behaviour that is at least qualitatively &quot;correct&quot;. . We call this quality &quot;tail dependence&quot;. This can affect upside and downside tails or just one tail depending on the phenomena. . Naively we would think that an option would be to use one of the correlation metrics above on censored data (that is filtering out all samples below/above a certain threshold). Unfortunately this does not work, the correlation metric is not additive - meaning you can&#39;t discretize the space into smaller units and combine them to give the overall correlation measure. This is another reason correlation metrics don&#39;t fit well with our intuition. However the information theoretic quantities can satisfy this condition but as before are a little more difficult to work with. We have some specialist metrics that are easier to use for this purpose. . The simplest metric to look at to study tail-dependence is the &quot;joint exceedance probability&quot;. As the name suggests we look at the probability of time that the joint distribution exists beyond a certain level (or below a certain level). Suppose we have joint observations $(X_i, Y_i)_{i=1}^N$ and we have: $ mathbb{P}(X &gt; tilde{x}) = p$ and $ mathbb{P}(Y &gt; tilde{y}) = p$ then the joint exceedance probability of $(X,Y)$ at percentile $p$ is: $$JEP_{(X,Y)}(p) = frac{ sum_i mathbb{1}_{(X_i &gt; tilde{x})} mathbb{1}_{(Y_i &gt; tilde{y}})}{N} $$ . Where: $ mathbb{1}_{(.)}$ is the indicator variable/Heaviside function. By construction we have that for $X$ and $Y$ fully rank-dependent then $JEP_{(X,Y)}(p) = p$ if $X$ and $Y$ fully rank-independent then $JEP_{(X,Y)}(p) = p^2$. It often helps to standardise the metric to vary between $[0,1]$ through the transform (we wouldn&#39;t use this metric if we are in the case of negative-dependence so can limit ourselves to indpendence as the lower-bound): $$JEP_{(X,Y)}(p) = frac{ sum_i mathbb{1}_{(X_i &gt; tilde{x})} mathbb{1}_{(Y_i &gt; tilde{y}})}{pN} - p$$ . As with other rank-methods this metric is only useful with monotone relations, it cannot cope with non-monotone relations such as the $Y=X^2$ example. . We can also view tail dependence through the prism of probability theory. We can define the upper tail dependence ($ lambda _{u}$) between variables $X_1$ and $X_2$ with CDFs $F_1(.)$ and $F_2(.)$ respectively as: $$ lambda _{u}= lim _{q rightarrow 1} mathbb{P} (X_{2}&gt;F_{2}^{-1}(q) mid X_{1}&gt;F_{1}^{-1}(q))$$ . The lower tail dependence ($ lambda _{l}$) can be defined similarly through: $$ lambda _{l}= lim _{q rightarrow 0} mathbb{P} (X_{2} leq F_{2}^{-1}(q) mid X_{1} leq F_{1}^{-1}(q))$$ . The JEP metric above is in some sense a point estimate of these quantities, if we had infinite data and could make $p$ arbitrarily small in $JEP_{(X,Y)}(p)$ we would tend to $ lambda _{u}$. We will revisit the tail-dependence metric once we have built up some of the theoretical background. . Multi-Variate Generation . We now move onto the issue of generating multi-variate samples for use within our Monte-Carlo models. It is not always to encode exact mechanisms for how variables relate (e.g. if we randomly sample from a distribution to give a country&#39;s GDP how do we write down a function to turn this into samples of the country&#39;s imports/exports? It is unlikely we&#39;ll be able to do this with any accuracy, and if we could create functions like this we would better spend our time speculating on the markets than developing Monte-Carlo models!) So we have to rely on broad-brush methods that are at least qualitatively correct. . Multi-Variate Normal . We start with perhaps the simplest and most common joint-distribution (rightly or wrongly) the multi-variate normal. As the name suggests this is a multi-variate distribution with normal distributions as marginals. The joint behaviour is specified by a covariance matrix ($ mathbf{ Sigma}$) representing pairwise covariances between the marginal distributions. We can then specify the pdf as: $$f_{ mathbf {X} }(x_{1}, ldots ,x_{k})={ frac { exp left(-{ frac {1}{2}}({ mathbf {x} }-{ boldsymbol { mu }})^{ mathrm {T} }{ boldsymbol { Sigma }}^{-1}({ mathbf {x} }-{ boldsymbol { mu }}) right)}{ sqrt {(2 pi )^{k}|{ boldsymbol { Sigma }}|}}}$$ . Which is essentially just the pdf of a univariate normal distribution just with vectors replacing the single argument. To avoid degeneracy we require that $ mathbf{ Sigma}$ is a positive definite matrix. That is for any vector $ mathbf{x} in mathbb{R}^N_{/0}$ we have: $ mathbf{x}^T mathbf{ Sigma} mathbf{x} &gt; 0$. . How can we sample from this joint distribution? One of the most common was is through the use of a matrix $ mathbf{A}$ such that: $ mathbf{A} mathbf{A}^T = mathbf{ Sigma}$. If we have a vector of independent standard normal variates: $ mathbf{Z} = (Z_1, Z_2, ... , Z_N)$ then the vector: $ mathbf{X} = mathbf{ mu} + mathbf{A} mathbf{Z}$ follows a multivariate normal distribution with means $ mathbf{ mu}$ and covariances $ mathbf{ Sigma}$. The Cholesky-decomposition is typically used to find a matrix $ mathbf{A}$ of the correct form, as a result I have heard it called the &quot;Cholesky-method&quot;. We will not cover the Cholesky decomposition here since it is a little out of scope, you can read more about it here if you would like. . Let&#39;s look at an example of generating a 4d multivariate normal using this method: . # Simulating from a multivariate normal # Using Cholesky Decomposition import numpy as np from scipy.stats import norm import matplotlib.pyplot as plt import seaborn as sns import pandas as pd %matplotlib inline # Fix number of variables and number simulations N = 4 sims = 1000 # Fix means of normal variates mu = np.ones(N)*0.5 # Initialize covariance matrix cov = np.zeros((N, N)) # Create a covariance matrix # Randomly sampled for i in range(N): for j in range(N): if i==j: cov[i, j] = 1 else: cov[i, j] = 0.5 cov[j, i] = cov[i, j] # Calculate cholesky decomposition A = np.linalg.cholesky(cov) # Sample independent normal variates Z = norm.rvs(0,1, size=(sims, N)) # Convert to correlated normal variables X = Z @ A + mu # Convert X to dataframe for plotting dfx = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;, &quot;X4&quot;]) # Create variable Plots def hide_current_axis(*args, **kwds): plt.gca().set_visible(False) pp = sns.pairplot(dfx, diag_kind=&quot;kde&quot;) pp.map_upper(hide_current_axis) plt.show() . Here we can see we have geerated correlated normal variates. . We see here that this is a quick and efficient way of generating correlated normal variates. However it is not without its problems. Most notably is specifying a covariance/correlation matrix quickly becomes a pain as we increase the number of variates. For example if we had $100$ variables we would need to specify: $4950$ coefficients (all the lower off diagonals). If we have to worry about the matrix being positive definite this quickly becomes a pain and we can even begin to run into memory headaches when matrix multiplying with huge matrices. . An alternative highly pragmatric approach is to use a &quot;driver&quot; approach. Here we keep a number of standard normal variables that &quot;drive&quot; our target variables - each target variable has an associated weight to each of the driver variables and a &quot;residual&quot; component for its idiosyncratic stochasticity. In large scale models this can drastically reduce the number of variables to calibrate. Of course we lose some of the explanatory power in the model, but in most cases we can still capture the behaviour we would like. This procedure in some sense &quot;naturally&quot; meets the positive definite criteria and we do not need to think about it. Let&#39;s look at this in action: . # Simulating from a multivariate normal # Using a driver method import numpy as np from scipy.stats import norm import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline # Fix number of variables and number simulations N = 4 sims = 1000 # Target means and standard deviations mu = np.ones(N)*0.5 sig = np.ones(N)*0.5 # Fix number of driver variables M = 2 # Specify driver weight matrix w_mat = np.zeros((M, N)) for i in range(M): for j in range(N): w_mat[i, j] = 0.25 # Residual weights w_res = 1 - w_mat.sum(axis=0) # Simulate driver variables drv = norm.rvs(0,1, size=(sims, M)) # Simulate residual variables res = norm.rvs(0,1, size=(sims, N)) # Calculate correlated variables X = drv @ w_mat + w_res * res # Standardise variables X = (X - X.mean(axis=0)) / (X.std(axis=0)) # Apply transforms X = X*sig + mu # Convert X to dataframe for plotting dfx = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;, &quot;X4&quot;]) # Create variable Plots def hide_current_axis(*args, **kwds): plt.gca().set_visible(False) pp = sns.pairplot(dfx, diag_kind=&quot;kde&quot;) pp.map_upper(hide_current_axis) plt.show() . We can see that this &quot;driver&quot; method can significanlty reduce the number of parameters needed for our models. They can also be used to help &quot;interpret&quot; the model - for example we could interpret one of our &quot;driver&quot; variables as being a country&#39;s GDP we could then use this to study what happens to our model when GDP falls (e.g. is below the 25th percentile). The nature of the drivers is such that we do not need to know how to model them exactly, this is useful for modelling things such as &quot;consumer sentiment&quot; where a sophisticated model does not necessarily exist. This is part of the &quot;art&quot; side of Monte-Carlo modelling rather than a hard science. . The tail dependence parameters from the multi-variate normal satisfy: $$ lambda_u = lambda_l = 0$$ . That is there is no tail-dependence. This can be a problem for our modelling. . Other Multivariate Distributions . With the multivariate normal under our belts we may now want to move onto other joint distributions. Unfortunately things are not that simple, using the methods above we are essentially limited to multivariate distributions that can be easily built off of the multivariate normal distribution. One such popular example is the multivariate student-t distribution. We can sample from this using the transform: $$ mathbf{X} = frac{ mathbf{Z}}{ sqrt{ frac{ chi_{ nu}}{ nu}}}$$ . Where: $ mathbf{Z}$ is a multivariate normal and $ chi_{ nu}$ is a univariate chi-square with $ nu$ degrees of freedom. The resulting multivariate student-t has covariance matrix $ mathbf{ Sigma}$ and $ nu$ degrees of freedom. Here the degrees of freedom is a parameter that controls the degree of tail-dependence with lower values representing more tail dependence. In the limit $ nu to infty$ we have $ mathbf{X} to mathbf{Z}$. . We can add a couple of extra lines to our Gausssian examples above to convert them to a multi-variate student t distribution: . # Simulating from a multivariate student t # Using Cholesky Decomposition and a chi2 transform import numpy as np from scipy.stats import norm, chi2 import matplotlib.pyplot as plt import seaborn as sns import pandas as pd %matplotlib inline # Fix number of variables and number simulations N = 4 sims = 1000 # Fix means of normal variates mu = np.ones(N)*0.5 # Initialize covariance matrix cov = np.zeros((N, N)) # Set degrees of freedom nu nu = 5 # Create a covariance matrix # Randomly sampled for i in range(N): for j in range(N): if i==j: cov[i, j] = 1 else: cov[i, j] = 0.5 cov[j, i] = cov[i, j] # Calculate cholesky decomposition A = np.linalg.cholesky(cov) # Sample independent normal variates Z = norm.rvs(0,1, size=(sims, N)) # Convert to correlated normal variables X = Z @ A + mu # Standardize the normal variates Z = (X - X.mean(axis=0)) / X.std(axis=0) # Sample from Chi2 distribution with nu degrees of freedom chis = chi2.rvs(nu, size=sims) # Convert standard normals to student-t variables T = (Z.T / np.sqrt(chis / nu)).T # Convert T to dataframe for plotting dft = pd.DataFrame(T, columns=[&quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;, &quot;T4&quot;]) # Create variable Plots def hide_current_axis(*args, **kwds): plt.gca().set_visible(False) pp = sns.pairplot(dft, diag_kind=&quot;kde&quot;) pp.map_upper(hide_current_axis) plt.show() . The tail dependence parameter fro the student-t copula for 2 variates with correlation parameter $ rho$ and $ nu$ degrees of freedom is: . $$ lambda_u = lambda_l = 2 t_{ nu+1} left(- frac{ sqrt{ nu+1} sqrt{1- rho}}{ sqrt{1+ rho}} right) &gt; 0$$ . We can see that even if we specify $ rho=0$ there will be some level of tail-dependence between the variates. In fact it is impossible to enforce independence between variables using the student-t copula. This can be an issue in some instances but there are extensions we can make to overcome - for example see my insurance aggregation model. . Another interesting property of the student-t copula is that it also has &quot;opposing&quot; tail dependency. Namely: . $$ lim _{q rightarrow 0} mathbb{P} (X_{2} leq F_{2}^{-1}(q) mid X_{1} &gt; F_{1}^{-1}(1-q)) = lim _{q rightarrow 0} mathbb{P} (X_{2}&gt;F_{2}^{-1}(1-q) mid X_{1} leq F_{1}^{-1}(q)) = 2 t_{ nu+1} left(- frac{ sqrt{ nu+1} sqrt{1+ rho}}{ sqrt{1- rho}} right) &gt; 0$$ . This gives rise to the familiar &quot;X&quot; shape of a joint student-t scatter plot. . The Need for More Sophistication . So far we are limited to modelling joint behaviour with either normal or student-t marginal distributions. This is very limiting, in practice we will want to sample from joint distributions with arbitrary marginal distributions. Typically we will find it easier to fit marginal distributions to data and we will want our models to produce reasonable marginal results. We therefore need a better method of modelling joint behaviour. . Copula Methods . One way to do this is through a copula method. A copula is nothing more than a multi-variate distribution with uniform $[0,1]$ marginals. This is particularly useful for modelling purposes as it allows us to fully delineate the univariate distributions from the joint behaviour. For example we can select a copula with the joint behaviour we would like and then use (for example) a generalized inverse transform on the marginal distributions to get the results we desire. We can do this simply by adapting the multi-variate Gaussian above, we can use the CDF function on the generated normal variates to create joint-uniform variates (i.e. a copula) and then inverse transform these to give Gamma marginals. This will result in Gamma marginals that display a rank-normal correlation structure: . # Simulating from Gamma variates with rank-normal dependency # Using a copula type method import numpy as np from scipy.stats import norm, gamma import matplotlib.pyplot as plt import seaborn as sns import pandas as pd %matplotlib inline # Fix number of variables and number simulations N = 4 sims = 1000 # Fix means of normal variates mu = np.ones(N)*0.5 # Initialize covariance matrix cov = np.zeros((N, N)) # Select Gamma parameters a and b a = 5 b = 3 # Create a covariance matrix # Randomly sampled for i in range(N): for j in range(N): if i==j: cov[i, j] = 1 else: cov[i, j] = 0.5 cov[j, i] = cov[i, j] # Calculate cholesky decomposition A = np.linalg.cholesky(cov) # Sample independent normal variates Z = norm.rvs(0,1, size=(sims, N)) # Convert to correlated normal variables X = Z @ A + mu # Standardize the normal variates Z = (X - X.mean(axis=0)) / X.std(axis=0) # Apply the normal CDF to create copula C C = norm.cdf(Z) # Use inverse transform to create Gamma variates G = gamma.ppf(C, a=a, scale=1/b) # Convert T to dataframe for plotting dfg = pd.DataFrame(G, columns=[&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;, &quot;G4&quot;]) # Create variable Plots def hide_current_axis(*args, **kwds): plt.gca().set_visible(False) pp = sns.pairplot(dfg, diag_kind=&quot;kde&quot;) pp.map_upper(hide_current_axis) plt.show() . This is really quite neat! It makes our modelling that much easier as we do not need to consider the joint-distribution in its entirety, we can break it down into smaller pieces that are easier to handle. The example above is quite a simple one however, we would like to have more freedom than this as we are still bound by sampling from a joint-distribution. . Let&#39;s look at some notation. Suppose we have variates: $(X_1, ... ,X_N)$ where each follows a cdf $F_i(x) = mathbb{P}(X_i leq x)$. We then have: $(U_1, ... , U_N) = (F_1(X_1), ..., F_N(X_N)$ with uniform random variates by definition. The copula defined by variables $(X_1, ..., X_N)$ is: $C(U_1, ... , U_N) = mathbb{P}(X_1 leq F_1(U_1), ... , X_N leq F_N(U_N))$. From this definition $C: [0,1]^N to [0,1]$ is an N-dimensional copula if it is a cumulative distribution function (CDF) on the unit cube with uniform marginals. . Going the other way any function $C: [0,1]^N to [0,1]$ defines a copula if: . $C(u_{1}, dots ,u_{i-1},0,u_{i+1}, dots ,u_{N})=0$, the copula is zero if any one of the arguments is zero, | $C(1, dots ,1,u,1, dots ,1)=u$, the copula is equal to u if one argument is u and all others 1, | C is d-non-decreasing, i.e., for each hyperrectangle $B= prod _{i=1}^{N}[x_{i},y_{i}] subseteq [0,1]^{N}$ the C-volume of B is non-negative: $$ int _{B} mathrm {d} C(u)= sum _{ mathbf {z} in times _{i=1}^{N} {x_{i},y_{i} }}(-1)^{N( mathbf {z} )}C( mathbf {z} ) geq 0$$ where the $N( mathbf {z} )= # {k:z_{k}=x_{k} }$. | . We now move onto the fundamental theorem of copulae: Sklar&#39;s theorem. The statement of the theorem exists in two parts: . Let $H(X_1, ..., X_N)$ represent a joint distribution function with marginal distributions $X_i sim F_i$. Then there exists a copula $C(.)$ such that: $H(X_1, ..., X_N) = C(F_1(X_1), ... , F_N(X_N))$. Moreover if $F_i$ are continuous then the copula $C(.)$ is unique | Given a copula function $C(.)$ and univariate distribution functions $F_i$ then $C(F_1(X_1), ... , F_N(X_N))$ defines a joint distribution with marginal distributions $F_i$ | Sklar&#39;s theorem shows there is a bijection relation between the space of joint distributions and the space of copulae (for continuous variables). This shows that this is indeed a powerful method to use in our modelling. . There are a couple of things to note here: the copula is itself a rank-order method of applying dependency. It is invariant under monotonic transforms (due to the use of the generalized inverse method). We can also derive the FrÃ©chetâ€“Hoeffding bounds: . $$ operatorname{max} left { 1 - N + sum_{i=1}^{N} u_i, 0 right } leq C(u_1, ..., u_N) leq operatorname{min} {u_1, ... , u_N }$$ . We can see this quite easily by noting that for the lower-bound: begin{align} C(u_1, ... , u_N) &amp;= mathbb{P} left( bigcap_{i=1}^N { U_i leq u_i } right) &amp;= 1 - mathbb{P} left( bigcup_{i=1}^N { U_i geq u_i } right) &amp; geq 1 - sum_{i=1}^N mathbb{P}( U_i geq u_i ) &amp;= 1 - N + sum_{i=1}^{N} u_i end{align} . For the upper bound we have: . $$ bigcap_{i=1}^N { U_i leq u_i } subseteq { U_i leq u_i } qquad implies qquad mathbb{P} left( bigcap_{i=1}^N { U_i leq u_i } right) leq mathbb{P} left( U_i leq u_i right) = u_i $$ . For all possible indices $i$. We have that the upper-bound function $ operatorname{min} {u_1, ... , u_N }$ itself defines a copula corresponding to complete (rank) dependence. It is often called the &quot;co-monotonic copula&quot;. The lower bound exists as a copula only in the case $N=2$ whereby it represents complete negative (rank) dependence. . We can also calculate the Spearman ($ rho$) and Kendall-Tau ($ tau$) dependency coefficients using the copula construction: . $$ rho = 12 int_0^1 int_0^1 C(u,v) du dv - 3 $$ . And: . $$ tau = 4 int_0^1 int_0^1 C(u,v) dC(u,v) - 1$$ . We can view the Gaussian example above in a copula frame via: . $$C^{Gauss}(u_1, ... , u_N) = Phi_{ mathbf{ mu, Sigma}}( Phi^{-1}(u_1), ..., Phi^{-1}(u_N))$$ . Where: $ Phi_{ mathbf{ mu, Sigma}}$ is the CDF of a multivariate Gaussian with means $ mathbf{ mu}$ and correlation matrix $ mathbf{ Sigma}$ The function: $ Phi^{-1}(.)$ is the inverse CDF of the univariate standard normal distribution. We call this the Gaussian copula or the rank-normal copula. . We can define the student-t copula in a similar way. I have developed a model based around a hierarchical driver structure with a generalization of a student-t copula for the purposes of modelling insurance aggregation. You can read my blog-post dedicated to this specifically here! . But what are some other options? The copula methods are very flexible so lets look at some other examples. Starting off with the Archimedean family of copulae. A copula is Archimedian if it admits a representation: . $$ C_{Arch}(u_{1}, dots ,u_{N}; theta )= psi ^{[-1]} left( psi (u_{1}; theta )+ cdots + psi (u_{N}; theta ); theta right) $$ . Here the function $ psi$ is called the generator function. We also have parameter $ theta$ taking values in some arbitrary parameter space - the role that $ theta$ takes depends on the form of the generator. $ psi ^{-1}$ is the pseudo-inverse of $ psi$: . begin{equation} psi ^{[-1]}(t; theta ) = begin{cases} psi ^{-1}(t; theta )&amp;{ mbox{if }}0 leq t leq psi (0; theta ) 0&amp;{ mbox{if }} psi (0; theta ) leq t leq infty end{cases} end{equation}We note that this functional form has some very useful properties. For example we note that we can express the upper and lower tail dependence parameters (defined above). If we place further assumptions on the generator function being a Laplace transform of strictly positive random variables we can write down neat forms of the upper and lower tail depdence metrics: . $$ lambda _{u} = 2 - 2 lim_{s downarrow 0} frac{ psi&#39;^{[-1]} (2s)}{ psi&#39;^{[-1]} (s)} $$ . and: $$ lambda _{l} = 2 lim_{s to infty} frac{ psi&#39;^{[-1]} (2s)}{ psi&#39;^{[-1]} (s)} $$ . Similarly we can conveniently calculate the Kendall-tau measure of dependence via: . $$ tau = 1 + 4 int_0^1 frac{ psi(t)}{ psi&#39;(t)} dt $$ . This is very convenient for modelling purposes since we can easily control the joint properties in a predictable way. Unforunately the calculation of Spearman correlation coefficients is not as simple and in many cases a closed form analytic solution does not exist. . We can summarise some of the common Archimedean copulae in the table below: . Copula Name $ psi(t)$ $ psi^{[-1]}(t)$ $ theta$-defined-range Lower-Tail-$ lambda_l( theta)$ Upper-Tail-$ lambda_u( theta)$ $ tau$ . Frank | $- log left({ frac { exp(- theta t)-1}{ exp(- theta )-1}} right)$ | $-{ frac {1}{ theta }} , log(1+ exp(-t)( exp(- theta )-1))$ | $ mathbb {R} backslash {0 }$ | 0 | 0 | $1 + 4(D_1( theta)-1)/ theta$ | . Clayton | $ frac {1}{ theta }(t^{- theta }-1)$ | $ left(1+ theta t right)^{-1/ theta }$ | $[-1, infty) backslash {0 }$ | $2^{-1/ theta}$ | 0 | $ frac{ theta}{ theta + 2}$ | . Gumbel | $ left(- log(t) right)^{ theta }$ | $ exp left(-t^{1/ theta } right)$ | $[1, infty)$ | 0 | $2-2^{1/ theta}$ | $1 - theta^{-1}$ | . Where: $D_k( alpha) = frac{k}{ alpha^k} int_0^{ alpha} frac{t^k}{exp(t)-1} dt$ is the Debye function. . We can see that the Frank copula applies dependence without any tail-dependence, whereas the Clayton and Gumbel are options for lower or upper-tail dependence modelling. This is in contrast to the student-t we observed before which applies symmetrically to both lower and upper tails. . To finish we will look at the bi-variate Frank, Clayton and Gumbel copulae. We will use a variety of methods to do this. For the Frank copula we will look at the &quot;conditional&quot; copula (that is the probability of one variate conditional on a specific value of the other): we will sample the overall percentile for the copula ($z$) and one of the variates ($u_1$) and then back &quot;out&quot; the remaining variate ($u_2$). The pair $ { u_1, u_2 }$ is then distributed as the copula: . $$ u_2 = - frac{1}{ theta} log left[1 + z frac{1-e^{- theta}}{z(e^{- theta u}-1) - e^{- theta u}} right]$$ . We can do the same for the Clayton: . $$ u_2 = left(1 + u^{- frac{1}{ theta}} left( z^{- frac{1}{1+ theta}}-1 right) right)^{- theta}$$ . Unforunately the conditinal copula of the Gumbel is not invertible and so we are unable to follow the same approach. Instead we follow the approach shown by Embrechts where we sample a uniform variate $v$ and then find $0&lt;s&lt;1$ such that: $sln(s) = theta(s-v)$. We then sample another uniform variate $u$ and the pair: $ { exp(u^{1/ theta}ln(s)), exp((1-u)^{1/ theta}ln(s)) }$ is a sample from the Gumbel copula with parameter $ theta$. . We can implement this as: . # Sampling from the Frank, Clayton and Gumbel archimedean Copulae # A very basic bi-variate implementation import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.optimize import fsolve %matplotlib inline # Set number of simulations and theta parameter sims = 1000 theta = 2 # Simulate u1 - first component of copula U1 = np.random.random(sims) # Simulate z - the joint probaibility Z = np.random.random(sims) # Define conversion functions for each copula def frank(theta, z, u): return -1/theta*np.log(1+ z*(1-np.exp(-theta))/(z*(np.exp(-theta*u)-1) - (np.exp(-theta*u)))) def clayton(theta, z, u): return np.power((1+ np.power(u, -1/theta)*(np.power(z, -1/(1+theta))-1)), -theta) # Define function to find S such that sln(s) = theta(s-u) def gumfunc(s): return s*np.log(s) - theta*(s-U1) # Use fsolve to find roots S = fsolve(gumfunc, U1) U1_gumbel = np.exp(np.log(S)*np.power(Z, 1/theta)) U2_gumbel = np.exp(np.log(S)*np.power(1-Z, 1/theta)) U1_frank = U1 U2_frank = frank(theta, Z, U1) U1_clayton = U1 U2_clayton = clayton(theta, Z, U1) # Create Plots fig, ax = plt.subplots(1, 3, figsize=(15, 5)) sns.scatterplot(U1_frank, U2_frank, ax=ax[0]) ax[0].set_title(&quot;Frank&quot;) ax[0].set_xlim([0,1]) ax[0].set_ylim([0,1]) sns.scatterplot(U1_clayton, U2_clayton, ax=ax[1]) ax[1].set_title(&quot;Clayton&quot;) ax[1].set_xlim([0,1]) ax[1].set_ylim([0,1]) sns.scatterplot(U1_gumbel, U2_gumbel, ax=ax[2]) ax[2].set_title(&quot;Gumbel&quot;) ax[2].set_xlim([0,1]) ax[2].set_ylim([0,1]) fig.suptitle(r&quot;Comparison of Archemedean Copulae with $ theta =$%i&quot; %theta, x=0.5, y=1, size=&#39;xx-large&#39;) plt.show() . As expected we can see little dependence in the tails of the Frank Copula, some dependence in the lower tail (small percentiles) of the Clayton and some dependence in the upper tail (larger percentiles) of the Gumbel copulae. . It was a bit of work to sample from these copulae. Thankfully packages exist to make our lives easier - for example: copulas. But is important to understand at least the basics behind how some of these packages work before we jump in and use them. . Conclusion . We have now extended our abilities from being able to sample from univariate distributions in our previous blog post to how to sample from multi-variate distributions. We saw how we could use a Cholesky decomposition to sample from a mulit-variate normal and further how we could implement a driver approach in order to reduce the number of model parameters. We then saw how we could extend this to sample from a multi-variate student-t distribution. We then looked at copula methods in order to separate the marginal and joint behaviour of a system. We ended by looking at the powerful class of Archimedean copulae andthe Frank, Clayton and Gumbel copulae specifically. .",
            "url": "https://www.lewiscoleblog.com/monte-carlo-methods-2",
            "relUrl": "/monte-carlo-methods-2",
            "date": " â€¢ May 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Monte Carlo Methods",
            "content": "In this blog post we shall be looking at Monte-Carlo methods in general. We will start from first principles and try and build up a somewhat complete picture of what the methods are, why they work and how to use them. With current programming languages and packages the way they are set up it is incredibly easy to throw together a Monte Carlo model. Unfortunately, in the authors experience, most do not have a clear understanding of how any of it works and there are a multitude of sins commited that go completely undetected. Hopefully this blog post will help clarify a few things. . What is a Monte-Carlo Method? . Going right back to basics, what is a Monte-Carlo method? If you read around (probabilstic/statistical) for any length of time you will see the term bandied around - often to many seemingly unrelated models. The concept of Monte-Carlo can be described as: &quot;Using random simulations to estimate a quantity&quot;. This is opposed to a generic &quot;computational&quot; approach where we would rely on deterministic approaches. Often we can think of problems solved with Monte-Carlo being in one of three classes: integration, optimization and generating samples from some probability distribution. . To make things a bit clearer let&#39;s look at an example (albeit a very silly one nobody would use in practice!) Let&#39;s try and create an estimate of $ pi$ - using a Monte-Carlo approach to solve this is a &quot;classic&quot; interview question for entry level modelling positions. First let&#39;s consider the &quot;classic&quot; Riemann integral approach: we can imagine drawing a unit circle on a piece of paper. We know that this will have area: $ pi times 1^2 = pi$. We can now imagine creating a square lattice covering this, we can calculate the area of each square easily - by counting the number of squares that are totally inclosed by the circle we can get an estimate of the area. As we decrease the size of the squares we should get a better estimate of $ pi$ and in the limit of the squares tending to zero we get exactly $ pi$. This is exactly the theoretical underpinning of integration. We can create a python code of this : . # Riemann estimate of pi import numpy as np # Set number of squares dividing the length of the [-1,1]x[-1,1] square N = 1000 # Calculate square side length and square area dx = 2 / N da = dx**2 # Initiate estimate pi_est = 0 # Iterate over all possible squares for x in range(1,N+1): for y in range(1,N+1): #top left corner distance from origin tl = np.sqrt(((x-1)*dx-1)**2+((y-1)*dx-1)**2) #top right corner distance from origin tr = np.sqrt(((x)*dx-1)**2+((y-1)*dx-1)**2) #bottom right corner distance from origin br = np.sqrt(((x)*dx-1)**2+((y)*dx-1)**2) #bottom right corner distance from origin bl = np.sqrt(((x-1)*dx-1)**2+((y)*dx-1)**2) # If all corners lie within circle the entire # square is circumscribed so add area to pi estimate if min(tl, tr, br, bl) &lt;= 1: pi_est += da print(&quot;Estimate of pi:&quot;, pi_est) . Estimate of pi: 3.1493760000072126 . This is not the prettiest code and there are ways to make it more efficient, etc. However it gives us an idea of how we can create an estimate of pi. (Of course nobody would do this in practice, there are much better ways of calculating pi!) In contrast a Monte-Carlo approach to creating an estimate of $ pi$ would be as follows: imagine we take the same unit circle located within a square $[-1,1]x[-1,1]$. Instead of thinking about areas we can think about points. If we generate random points within this square we can count the number falling within the circle compared to the total number of points (when we simulate a point and it falls inside the circle we add &quot;4&quot; to the count via the symmetry of the simulation - we could flip the signs of the simulated point and it would still fall within the circle!). We can code this up as follows: . # Monte-Carlo estimate of pi import numpy as np # Set number of simulated points N = 100000 # Initiate counter count = 0 # Generate random points within [-1,1]x[-1,1] square and test for i in range(N): pt = 2* np.random.random(2) - 1 if np.sqrt(pt[0]**2 + pt[1]**2) &lt; 1: count += 4 print(&quot;Estimate of pi:&quot;, count/N) . Estimate of pi: 3.1432 . Again this is a very dumb algorithm that (outside of an interview) nobody would ever use. We can see that we have used &quot;random numbers&quot; as provided by numpy. As it is coded every time we run this code we would end up with a different estimate (for a given number of simulated points) - in some cases this may be useful and in others a problem. . We notice that the procedure in the Monte-Carlo estimate of $ pi$ is fundamentally different to the Riemann approach. Since both algorithms are so dumb we won&#39;t comment on which is more efficient. However we can notice that the Monte-Carlo approach is slightly &quot;simpler&quot; and the determistic approach requires slightly more finesse and understanding of the area (i.e. we use the concavity of the circle to say that if all corners of the square are within the circle then the entire square is within the circle) - while this is simple in this scenario it doesn&#39;t take much of a leap to realise that in more complicated scenarios we may be in trouble. This is true in general and is one of the selling points of Monte-Carlo methods; it is generally (relatively) easy to implement a method even when there are no analytic solutions to the problem! . Why is it called Monte-Carlo? . Monte-Carlo techniques were first used in designing the atomic bomb. The physicists settled on the name &quot;Monte-Carlo&quot; (I believe) due to the number of casinos in Monte Carlo, Monaco. The earliest mathematical-probabilists were all gamblers, they were studying games of chance (most likely to look for easy money!) - Monte-Carlo seems to be a fitting tribute to those early practitioners! . We can see now that Monte-Carlo methods rely heavily on random numbers. We now take a brief trip into the world of simulating random numbers. . Generating Pseudo Random Number Generators . In order for our Monte-Carlo methods to be viable we require an efficient way of generating &quot;random&quot; numbers. The quotes around &quot;random&quot; are intentional for there are great philosophical issues relating to &quot;what constitutes randomness?&quot; For the sake of our purposes we will not go down that dark road, instead we will take the approach: if it looks random then it is random! But what do we mean by looks random? . Desired Properties . There are a few properties we desire from generated random numbers: . The samples produced are independent and identically distributed (iid) | If there is a repeating pattern in our samples it is beyond the scope of our model so we will not notice them (this is called the periodicity) | There are no correlations between samples | The samples are not biased | Ideally we would like the samples to be &quot;repeatable&quot; so that if I run a method multiple times I can get the exact same results (this is more a practical consideration than a theoretical one) | We would like the random samples to be from a uniform distribution (for reasons that will become clear later) | It is possible to think of more criteria | . To do this we use pseudo-random number generators with the preface reminding us that we are not dealing with &quot;true&quot; randomness. For clarity we will define a pseudo-random number generator (herein PRNG) to be a mapping $D$ such that for a given initial condition (&quot;seed&quot;) $u_0$ the sequence $u_i = D^i (u_0)$ is such that it reproduces the behaviour of $(V_1,...,V_n)$ a collection of samples from an iid uniform distribution. Where &quot;reproducing&quot; is justified through the use of statistical tests. We can see that the use of a &quot;seed&quot; as an initial value satisfies the &quot;repetable&quot; criteria we noted above. . We will not delve too deeply into the range of statistical tests one can use here as there are many options and to some extent the quality of a PRNG depends on the application. For example cryptographic uses of PRNGs will likely have additional constraints we do not particularly care about for Monte-Carlo. One basic example of a test is the Kolmogorv-Smirnov Test which essentially quantifies the &quot;distance&quot; of the empirical distribution of the PRNG samples and the analytic uniform distribution. Of course this test does not tell us anything about auto-correlation between successive samples, periodicity and so on. A more comprehensive set of tests is the Diehard tests Another common comprehensive collection of tests can be found in the C library TestU01. We will not go into details of these tests but we should be aware that formal testing exists, and a PRNG need not pass every test in order to be useful. . So what are some possibilities for PRNGs? Since we will need to call the mapping multiple times in a Monte-Carlo simulation (e.g. in the $ pi$ example above we used $200,000$ random numbers!) We require something lightweight and quick to call or our runtime will be seriously slow to the point of being unusable. This will always be a trade-off with PRNGs, adding complexity may increase the statistical properties but at the same time it may cost us in computational time. The trick is to find &quot;the sweet spot&quot;. . In many textbooks I have seen phrases along the lines of &quot;We can use the computers internal clock to generate random numbers&quot; - but this is a bit misleading. For one generating random numbers this way is by its nature not repeatable. Further it has been shown that the random numbers are not strictly uniform and it becomes very difficult to ensure independence of draws. If we are after only a small number of samples for our program (say we just want to &quot;pause&quot; for a random amount of time) - there is no problem with this approach. However for larger scale Monte-Carlo models where we generate millions of pseudo random numbers these issues mount up. . Chaotic Systems . Some readers from a mathematics background may be aware of &quot;chaotic systems&quot; whereby a simple determistic map can result in unpredictable and &quot;wild&quot; behaviour. This is certainly a candidate for a PRNG. The issue however is that the random numbers generated tend not to be &quot;uniform&quot; but follow some other distribution, to convert to a uniform will thus cost an addtional transformation (typically expensive). For example the logistic map $X_{n+1} = 4 X_n(X_n - 1)$ is distributed as an arcsine distribution. To convert this to a uniform distribution we need to apply the transform: $Y_n = frac{1}{2} + frac{arcsin(2X_n)-1}{ pi}$ Which is expensive. To make matters worse successive samples $(Y_{n+1} Y_n)$ do not appear jointly uniform: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Specify number of random numbers N = 2500 # Specify starting condition X0 = 2 # Create array of random numbers X = np.zeros(N+1) Y = np.zeros(N+1) # Specify starting condition X[0] = 0.254123 Y[0] = 0.5 + np.arcsin(2*X[0]-1) / np.pi # Loop mapping for i in range(N): X[i+1] = 4*X[i]*(1-X[i]) Y[i+1] = 0.5+ np.arcsin(2*X[i+1]-1) / np.pi # Create Plots fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].hist(Y, density=True, color=&#39;grey&#39;) ax[0].set_title(&quot;Emperical Distribution Yn&quot;) ax[0].set_xlim(0,1) ax[0].set_ylim(0,1.2) ax[1].scatter(Y[0:N], Y[1:N+1], color=&#39;grey&#39;) ax[1].set_title(&quot;Joint Distribution (Yn, Yn+1)&quot;) ax[1].set_xlim(0,1) ax[1].set_ylim(0,1) plt.show() . We see that the marginal distribution of $Y_n$ does follow a uniform distribution. However we also see that in fact there is a strong degree of correlation between successive samples which we do not want from a PRNG. We would want the successive points to &quot;fill&quot; the unit square, triples to &quot;fill&quot; the unit cube and so on. Otherwise if we use our Monte-Carlo method to optimise some quantity we may be optimizing to the PRNG not to the problem itself! . Instead we can look at values &quot;further apart&quot; to get better joint behaviour: . We can see that to ensure a good coverage of the unit square for the joint distribution we need to &quot;throw away&quot; a lot of simulations. This is wasted computation time that will make our programs run very slowly! As such this is not really a viable option (plus regardless of the gap it would fail other statistical uniformity tests.) . Linear Congruential Generators . So what options do we have? One popular option is a linear congruential generator. The concept behind this is very simple. We use the update scheme: . begin{align} x_{n+1} &amp;= (a x_n + c) mod m u_{n+1} &amp;= frac{x_{n+1}}{m} end{align}Through careful selection of parameters we can get a sequence of random numbers with period $(m-1)$ these conditions are: . $c$ and $m$ are co-prime | Every prime number dividing $m$ also divides $(a-1)$ | $(a-1)$ is divisible by 4 only if $m$ is | . It is often convenient to simply take $c=0$ in which case the conditions become: . $(a^m - 1)$ is a multiple of $m$ | $(a^j - 1)$ is not a multiple of $m$ for any $j=1,2,...,(m-2)$ | . We can implement this easily as: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Specify parameters (from L&#39;Ecuyer) m = 2147483563 a = 40014 # Specify number of random numbers N = 2500 X = np.zeros(N+1) U = np.zeros(N+1) # Specify initial condition (seed) X[0] = 123 U[0] = X[0] / m for i in range(N): X[i+1] = (a*X[i]) % m U[i+1] = X[i+1] / m # Create Plots fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].hist(U, density=True, color=&#39;grey&#39;) ax[0].set_title(&quot;Emperical Distribution Un&quot;) ax[0].set_xlim(0,1) ax[0].set_ylim(0,1.2) ax[1].scatter(U[0:N], U[1:N+1], color=&#39;grey&#39;) ax[1].set_title(&quot;Joint Distribution (Un, Un+1)&quot;) ax[1].set_xlim(0,1) ax[1].set_ylim(0,1) plt.show() . With just a basic &quot;eyeball&quot; test we can see that the joint distribution is better than our last attempt (and runs much more quickly). We also have the added benefit of being able to &quot;look ahead&quot; via: $ x_{n+k} = a^k x_n mod m$ . Typically we would not use an algorithm like this as presented, instead we would take multiple &quot;copies&quot; of this code using different parameters and combine them into a &quot;Multiple Linear Congruential Generator&quot; (MLCG) which has better properties than any one in isolation. It is difficult to see in the example above but a single congruential generator exhibits a &quot;lattice&quot; like structure, if we re-run the code above for $a=6$ and $m=11$ we can see this more clearly: . We can see the generated joint &quot;random&quot; numbers exist on &quot;lines&quot; - this is not ideal! With larger periodicity like the previous example this is hard to see. The effect of these &quot;lines&quot; becomes less prominent when we combine multiple distinct generators, we do need to be somewhat careful for particularly large models however. . Mersenne-Twister . Another popular option for a PRNG is the Mersenne Twister (MT). This is used as the default PRNG in many programming environments (including python/numpy - when we call &quot;np.random.random()&quot; or any other random element behind the scenes it is using a Mersenne twister! But before using any pre-canned method it is recommended to try and code a basic version by hand to understand it fully.) The performance is generally more than acceptable for most Monte-Carlo methods. However there is a small &quot;issue&quot; whereby taking multiple sequences each with a different seed (while keeping other parameters the same) will not necessarily lead to independent sequences. However there are methods we can use to overcome this (however this is well out of scope of this blog post, it is just an issue to be aware of). There are many variants on the method that aim to overcome some of the shortfalls but we will restrict ourselves to the &quot;vanilla&quot; version. . We won&#39;t get totally bogged down with the details of the MT as it is a bit of a distraction for our purposes. It is a little more involved theoretically than the previous algorithms. It uses linear recurrence and a twisted generalised feedback shift (TGFSR(R)). We create a sequence $x_i$ through a recurrence relation and then apply a matrix mapping $T$ (tempering matrix). The method requires the following parameters (with their definitions): . w: word size (in number of bits) | n: degree of recurrence | m: middle word, an offset used in the recurrence relation defining the series x, 1 â‰¤ m &lt; n | r: separation point of one word, or the number of bits of the lower bitmask, 0 â‰¤ r â‰¤ w - 1 | a: coefficients of the rational normal form twist matrix | b, c: TGFSR(R) tempering bitmasks | s, t: TGFSR(R) tempering bit shifts | u, d, l: additional Mersenne Twister tempering bit shifts/masks | . With parameters satisfying that $2^{nw-r} -1$ is Mersenne prime (hence the name). The method produces a sequence of integers in the range $[0, 2^w -1]$. The recurrence relation for the sequence $x_i$ is: . $$ x_{k+n} = x_{k+m} oplus (( x_k^u mathbin{ |} x_{k+1}^l )A) $$ . Where $ oplus$ represents a bit-wise xor, $x_k^u$ represents the upper $w-r$ bits of $x_k$ and $x_{k+1}^l$ the lower $r$ bits of $x_{k+1}$. The Twist mapping $A$ is such that: . $$xA={ begin{cases}{x} gg 1&amp;x_{0}=0 ({x} gg 1) oplus { boldsymbol {a}}&amp;x_{0}=1 end{cases}}$$ . Where $x_0$ is the lowest order bit of $x$. we now apply the tempering transform through the creation of temporary intermediate value $y$ and define the output of the algorithm $z$ via: begin{align} y &amp;= x oplus ((x gg u) &amp; d) y &amp;= y oplus ((y ll s) &amp; b) y &amp;= y oplus ((y ll t) &amp; c) z &amp;= y oplus (y gg l) end{align} We have denoted bitwise logical and as $ &amp;$ and bitwise left and right shifts as $ ll$ and $ gg$ respectively. . We now implement a basic version of this in python and plot the results: . # Basic Implementation of the Mersenne Twister MT19937 algorithm import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Specify parameters for MT19937 (w, n, m, r) = (32, 624, 397, 31) a = 0x9908B0DF (u, d) = (11, 0xFFFFFFFF) (s, b) = (7, 0x9D2C5680) (t, c) = (15, 0xEFC60000) l = 18 f = 1812433253 # Store state of the generator in array MT MT = [0 for i in range(n)] index = n+1 lower_mask = 0xFFFFFFFF upper_mask = 0x00000000 # Initialize the generator via the seed # Default seed matches C++ implementation def mt_seed(seed=5489): MT[0] = seed for i in range(1, n): temp = f * (MT[i-1] ^ (MT[i-1] &gt;&gt; (w-2))) + i MT[i] = temp &amp; 0xffffffff # Extract a tempered value based on MT[index] # calling twist() every n numbers def generate_number(): global index if index &gt;= n: twist() index = 0 y = MT[index] y = y ^ ((y &gt;&gt; u) &amp; d) y = y ^ ((y &lt;&lt; t) &amp; c) y = y ^ ((y &lt;&lt; s) &amp; b) y = y ^ (y &gt;&gt; l) index += 1 return y &amp; 0xffffffff # Generate the next n values from the series x_i def twist(): for i in range(0, n): x = (MT[i] &amp; upper_mask) + (MT[(i+1) % n] &amp; lower_mask) xA = x &gt;&gt; 1 if (x % 2) != 0: xA = xA ^ a MT[i] = MT[(i + m) % n] ^ xA # Generate random numbers N = 2500 U = np.zeros(N) # Set seed mt_seed() for i in range(N): U[i] = generate_number() / 2**w # Create Plots fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].hist(U, density=True, color=&#39;grey&#39;) ax[0].set_title(&quot;Emperical Distribution Un&quot;) ax[0].set_xlim(0,1) ax[0].set_ylim(0,1.2) ax[1].scatter(U[0:N-1], U[1:N], color=&#39;grey&#39;) ax[1].set_title(&quot;Joint Distribution (Un, Un+1)&quot;) ax[1].set_xlim(0,1) ax[1].set_ylim(0,1) plt.show() . Which we can see passes our (very crude) eyeball test for random number generation. . It is also worth mentioning that it is possible to combine the Mersenne twister with the MLCG to again try and get the &quot;best of both worlds&quot;. The KISS algorithm) combines an MLCG with a bit-shifting PRNG (somewhat similar to the Mersenne twister). . There are other options that we have not covered here. For example the potential use of cellular automata to create randomness (you can read my blog post on cellular automata here!) There are also PRNGs being used for cryptography which we have not covered. There has also been research into leveraging quantum computing to generate random numbers but the field is still relatively fresh and (as far as I know) not particularly close to being widely adopted yet. . The bright side to all of this is most programming languages have modules with PRNGs ready to go! Most of the time these will be more than sufficient for Monte-Carlo based modelling, so we do not require an intimate knowledge of how to create them from scratch. There are some exceptions worth remembering however: if running the PRNG multiple times &quot;independently&quot; with differeing seeds keep in mind the sequences might not be random, if you&#39;re making a truly goliath model be aware on periodicity! And if we want a model to run super-quickly it might be worth coding up a very simple PRNG with less than ideal statistical performance, in some cases run-time is the most important thing. . Generating Samples Statistical Distributions . In our (rather contrived) Monte-Carlo $ pi$ estimator we relied on uniform random variates. In general we will want to simulate from more general distributions. For example suppose we wanted to generate representative heights of people within a population, we know that the normal/Gaussian distribution would be a good choice - how can we do this? We could come up with a whole new random number generator that spits out normally distributed samples instead, however this isn&#39;t a good general approach to the problem - what if instead we required samples from a Beta distribution instead? In some instances the &quot;specialist random number generator&quot; approach can be useful where speed is of the utmost concern, however for at least 99% of Monte-Carlo methods we encounter this is not required. . Generalized Inverse Method . Instead what we tend to do is look for ways of &quot;converting&quot; the uniform variates (herein we will denote these as $(u_1, u_2, ... )$ with $u_i sim U[0,1]$ iid) into the distribution of our desire. By far the most common way of doing this is to use the &quot;Generalized Inverse Method&quot;. Suppose we want to simulate variable $X$ with some distribution. We denote it&#39;s cumulative function as: $$F(x) = mathbb{P}( X leq x) $$ . We denote the generalized inverse of $F$ as: $$ F^{-1}(u) = inf { x : F(x) geq u } $$ . This looks like a rather complicated definition but it is infact rather simple. In the case of $X$ being a continuous variable the generalized inverse function is identical to our normal notion of an &quot;inverse function&quot;. The complication with infima arises when we consider discrete variables, we want to define the generalized inverse on the entire line $[0,1]$ not just the discrete points - we can just imagine drawing &quot;straight lines&quot; between the discrete points on the graph. We then have the probability integral transform lemma: . For $U sim U[0,1]$ and $X sim F$ then the random variable $F^{-1}(U) sim F$ . This means if we can define the cumulative function for a distribution and invert it we can simulate from the distribution. This is really powerful since it allows us to simulate from essentially any distribution. Say we have a bunch of data and we want to sample from its empirical distribution we can even do this by defining the cumulative function. It is simple fo prove the probability integral transform lemma:&gt;For all $u in [0,1]$ and $x in F^{-1}([0,1])$ we have:&gt; &gt; $F(F^{-1}(u)) geq u$ and $F^{-1}(F(x)) leq x$ &gt; . Therefore:&gt;&gt; $ {(u,x) :F^{-1}(u) leq x } = {(u,x) : F(x) geq u }$ &gt; &gt; And so:&gt;&gt; $ mathbb{P}(F^{-1}(U) leq x) = mathbb{P}(U leq F(x)) = F(x)$ $$ square$$ . Let&#39;s see how we can use this method with a simple example. If we have $X sim Exp( lambda)$ (an exponential distribution with parameter $ lambda$) we have that the cumulative function is:$$F(x) = 1 - e^{- lambda x}$$ By inverting this function with some basic algebra we get: $$F^{-1}(u) = frac{-1}{ lambda} ln(1-u) $$ . We can note that $U sim (1-U)$ and so we have: $$ frac{-1}{ lambda} ln(U) sim F $$ . Let&#39;s look at this in action: . # Sampling from an Exponential(lambda) distribution import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set number of simultations (N) and lambda (lam) N = 1000 lam = 3 # Sample uniform random variates U = np.random.random(N) # Apply probability integral transform X = - np.log(U) / lam # Create Array for analytic result x_a = np.linspace(0, 2.5, num=50) fx_a = lam * np.exp(-lam * x_a) # Plot histogram of samples plt.hist(X, density=True, bins=50, label=&#39;Empirical&#39;, color=&#39;grey&#39;) plt.ylabel(&quot;f(x)&quot;) plt.xlabel(&quot;x&quot;) plt.xlim(0,2.5) plt.ylim(0,3) plt.plot(x_a, fx_a, label=&#39;Analytic&#39;, color=&#39;red&#39;, linewidth=2.5) plt.legend() plt.title(&quot;Empirical Simulated Exponential Distribution Compared to Analytic Result&quot;) plt.show() . We can see that this sampling method produced empirical results very close to the true analytic solution! While we may be able to find more efficient sampling schemes in some instances we now have a completely general purpose tool for simulating from arbitrary distributions &quot;exactly&quot; (in the sense that we are limited only by computational runtime not by the method itself). . This aren&#39;t all as rosy as they seem however! In some instances the cumulative function may be rather &quot;messy&quot; - even for something as &quot;simple&quot; as a Gaussian(0,1) distribution we have: $$F(x) = frac{1}{ sqrt{2 pi}} int_{- infty}^x e^{ frac{-z^2}{2}} dz$$ . Good luck trying to invert that function analytically! We can however relax the need for &quot;exact&quot; sampling and apply approximation procedures. For example from Abramowitz and Stegun we have: $$ F^{-1}(x) = sqrt{ln(x^{-2})} - frac{2.30753 + 0.27061 sqrt{ln(x^{-2})}}{1 + 0.99229 sqrt{ln(x^{-2})} + 0.044841 ln(x^{-2})} $$ . Where we have an absolute error of the order $10^{-8}$ - which may or may not be acceptable. We can find approximations to arbitrary levels of accuracy should we wish. . Thankfully a lot of the time the hard work has been done for us however and &quot;inverse CDF&quot; functions exist in most stats packages and modules (e.g. scipy.stats) and these are by far the most common way of simulating from generic distributions. It was useful for us to do this ourselves by hand however for the cases where this is not possible (e.g. fit an empirical curve to some data and we don&#39;t want to use a parameteric distribution and lose any detail - or we want some particularly obscure parameteric distribution that has not been included in the package!). We also need to keep in mind although these inverse CDFs exist, and they&#39;ve likely be coded optimally by very smart folks, they still might be slow in some situations. Part of the skill that seperates a great Monte-Carlo practitioner from a merely good one is to know how/when to use alternate methods to achieve better performance for a given objective (typically the tradeoff between statistical accuracy and run-time). . For completeness let&#39;s apply the generalized inverse transform using &quot;scipy.stats&quot; (here the &quot;.ppf&quot; functions are the inverse CDFs). Repeating the process above with pre-canned functions: . # Sampling from an Exponential(lambda) distribution # Using Scipy stats import numpy as np import matplotlib.pyplot as plt from scipy.stats import expon %matplotlib inline # Set number of simultations (N) and lambda (lam) N = 1000 lam = 3 # Sample uniform random variates U = np.random.random(N) # Apply probability integral transform X = expon.ppf(U, scale=1/lam) # Create Array for analytic result x_a = np.linspace(expon.ppf(0.01, scale=1/lam), expon.ppf(0.99, scale=1/lam), 100) fx_a = expon.pdf(x_a, scale=1/lam) # Plot histogram of samples plt.hist(X, density=True, bins=50, label=&#39;Empirical&#39;, color=&#39;grey&#39;) plt.ylabel(&quot;f(x)&quot;) plt.xlabel(&quot;x&quot;) plt.xlim(0,2.5) plt.ylim(0,3) plt.plot(x_a, fx_a, label=&#39;Analytic&#39;, color=&#39;red&#39;, linewidth=2.5) plt.legend() plt.title(&quot;Empirical Simulated Exponential Distribution Compared to Analytic Result&quot;) plt.show() . We can see upto noise the results are the same as our &quot;hand-coded&quot; example. Stats packages also sometimes include a &quot;simulate from&quot; function that replaces the entire need to inverse transform, however we&#39;ll see later that this is rarely convenient for larger scale more complicated models. For completeness we could simulate directly using scipy.stats with: . # Sampling from an Exponential(lambda) distribution # Using Scipy stats import numpy as np from scipy.stats import expon # Set number of simultations (N) and lambda (lam) N = 1000 lam = 3 X = expon.rvs(scale=1/lam, size=N) . We will not reproduce the plot again, but this approach is equivalent to that above. . Relationships Between Distributions . However in a lot of cases we can rely on relationships between distributions to help us (think back to stats class where you thought you&#39;d never use this stuff!) For example if $X_i sim Exp(1)$ iid then we can define $Y sim Gamma( alpha, beta)$ via: $$ Y = beta sum_{i=1}^{ alpha} X_i$$ . Along with many other relations. We can then avoid having to invert the CDF: $$F(x) = frac {1}{ Gamma ( alpha )} gamma ( alpha , beta x) $$ . Which is not particuarly easy to do. There is a skill in knowing when/how to use these sorts of transformations. Lots of times the difference in execution time is negligble so it is better to use the standard inversion method on the CDF as it makes the code more transparent. In others timing matters and with the right transformation there can be serious time savings available. . One transform method for generating standard Gaussian variates is the Box-Muller technique. It relies on a polar co-ordinate representation $(r, theta)$. If $(X_1, X_2)$ are Gaussian(0,1) then we can write: $$r^2 = X_1^2 + X_2^2 sim chi_2^2 = Exp( frac{1}{2})$$ . We can then use $U_1, U_2 sim U[0,1]$ iid to represent $(X_1, X_2)$ as: begin{align} X_1 &amp;= sqrt{-2lnU_1}cos(2 pi U_2) X_2 &amp;= sqrt{-2lnU_1}sin(2 pi U_2) end{align} . This algorithm isn&#39;t the fastest performing algorithm one can find however, the $sin(.)$ and $cos(.)$ function calls are computationally quite expensive. . Relying on the relationships between distributions can be problematic. For one thing it might not be possible to construct a nice neat relationship - or if we can construct a relationship it may have strict conditions that means any conversion becomes inefficient. . Acceptance-Rejection Methods . Another class of method we can use are the acceptance-rejection methods. Again this relies on taking (pseudo-random) uniform variates as a starting point (you can now see why we would want a PRNG to be efficient at generating uniform variates by now!) The general concept is to &quot;look&quot; at each uniform pseudo-random number and decide whether we believe this could have been generated by the distribution we wish to sample from (and accept it) or not (and reject it). . The main theoretical backbone behind these methods is: . The Fundatmental Theorem of Simulation: . Simulating variates with density $f(x)$: $$ X sim f(x) $$ . Is equivalent to simulating: $$ (X, U) sim U { (x,u) : 0 &lt; u &lt; f(x) } $$ . It is not immediately clear why this is useful at this stage, but it is quite prophetic and it underlies many simulation algorithms. To see why lets note: begin{align} f(x) &amp;= int_0^{f(x)} du mathbb{P}(X leq x) &amp;= int_{ infty}^{x} f(y) dy end{align} . These are essentially just definitions, but they help us enormously in out simulation problem. For example let&#39;s suppose we have some arbitrary density $f(.)$ such that: begin{align} int_a^b f(x) dx &amp;= 1 sup_x { f(x) } &amp; leq M end{align} . Then we can simulate from $f(.)$ by generating: $(Y, U)$ such that $Y sim U[a,b]$ and $U|Y=y sim U[0,M]$ we accept simulations $u$ if $(0 &lt; u &lt; f(y))$ otherwise we reject the simulations. This works because: begin{align} mathbb{P}(X leq x) &amp;= mathbb{P}(Y leq x | U &lt; f(Y)) &amp;= frac{ int_a^x int_0^{f(y)} du dy}{ int_a^b int_0^{f(y)} du dy} &amp;= int_a^x f(y) dy end{align} . This is all a bit notation heavy so let&#39;s look at a specific example now. We shall generate from a Beta distribution. This has density function: $$ f(x) = frac{x^{ alpha-1}(1-x)^{ beta-1}} {B( alpha, beta)} $$ . Where $B$ is the Beta function defined: $$ B( alpha , beta )={ frac { Gamma ( alpha ) Gamma ( beta )}{ Gamma ( alpha + beta )}} $$ . The Gamma function $ Gamma$ being: $$ Gamma (z)= int _{0}^{ infty }x^{z-1}e^{-x} ,dx $$ . Through differentiation we find the maximum value attained by this function is: $$ max_x (f(x)) = frac{ left( frac{ alpha-1}{ alpha+ beta-2} right)^{ alpha-1} left(1- frac{ alpha-1}{ alpha+ beta-2} right)^{ beta-1}} {B( alpha, beta)} = M $$ . Assuming $ alpha + beta &gt; 2$ . We can use this to sample from the Beta distribution using acceptance/rejection: . # Sampling from a beta distribution using acceptance/rejection import numpy as np import matplotlib.pyplot as plt from scipy.special import beta %matplotlib inline # Define the beta distribution parameters ap = 3 bt = 2 # Define the beta pdf def beta_pdf(x, a=ap, b=bt): return x**(a-1)*(1-x)**(b-1) / beta(a,b) # Define Maximum attained value x_max = (ap - 1) / (ap+bt - 2) M = beta_pdf(x_max) # Define number of samples N = 1000 # Create samples Y = np.random.random(N) U = np.random.random(N)*M mask_acc = U &lt; beta_pdf(Y) mask_rej = U &gt; beta_pdf(Y) # Accepted Samples Y_acc = Y[mask_acc] U_acc = U[mask_acc] # Rejected Samples Y_rej = Y[mask_rej] U_rej = U[mask_rej] # Create analytic pdf plots X = np.linspace(0,1,100) PDF = beta_pdf(X) # Create Plots fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].scatter(Y_acc, U_acc, color=&#39;pink&#39;, label=&#39;Accepted&#39;) ax[0].scatter(Y_rej, U_rej, color=&#39;grey&#39;, label=&#39;Rejected&#39;) ax[0].plot(X, PDF, color=&#39;red&#39;, linewidth=2.5, label=&#39;PDF&#39;) ax[0].set_title(&#39;Acceptance-Rejection Sampling&#39;) ax[0].set_ylabel(&#39;f(x)&#39;) ax[0].set_xlabel(&#39;x&#39;) ax[0].set_xlim(0,1) ax[0].set_ylim(0,M+0.05) ax[0].legend() ax[1].hist(Y_acc, density=True, color=&#39;grey&#39;) ax[1].plot(X, PDF, linewidth=2.5, color=&#39;red&#39;) ax[1].set_title(&quot;Empirical Distribution&quot;) ax[1].set_ylabel(&#39;f(x)&#39;) ax[1].set_xlabel(&#39;x&#39;) ax[1].set_xlim(0,1) ax[1].set_ylim(0,M+0.05) plt.show() . In the left-hand image above we get a graphical representation of what is actually happening with the acceptance/rejection procedure. On the right we see that the resulting samples are indeed distributed as we would like. By construction we know that the acceptance rate should be $ frac{1}{M}$ - we can check this easily: . print(&quot;Theoretical Acceptance Rate:&quot;, 1/M) print(&quot;Empirical Acceptance Rate:&quot;, Y_acc.shape[0] / N ) . Theoretical Acceptance Rate: 0.5624999999999999 Empirical Acceptance Rate: 0.568 . So far this method is fairly cute but not entirely useful. The main issue is we have restricted ourselves to a finite support (in the beta example $x in [0,1]$) For many applications we want an infinite support. Another issue is we have assumed that there is an achieved maximum in the distribution function, in some distributions we want to sample from this is not the case. Thankfully we can modify this method slightly to allow for this. . If we modify the fundamental theorem of simulation to: $$ (Y, U) sim U { (y,u) : 0 &lt; u &lt; m(y) } $$ . Now instead of censoring on the distribution function itself we censor according to some other function with: $m(x) &gt; f(x)$ everywhere. We then express $m(.)$ in the form: $m(x) = M g(x)$, such that $ int g(x) = 1$ and $g(.)$ is itself a distribution function. If we simulate $Y sim g$ and $U | Y=y sim U(0, Mg(y))$ then by the constraint $m(x) &gt; f(x)$ the procedure works, why? begin{align} mathbb{P}(X in A) &amp;= mathbb{P}(Y in A | U &lt; f(Y)) &amp;= frac{ int_A int_0^{f(y)} frac{du}{Mg(y)} g(y) dy} { int_D int_0^{f(y)} frac{du}{Mg(y)} g(y) dy} &amp;= int_A f(y) dy end{align} . Where $A$ is any subset of the entire domain of $f$ ($D$). . There are two big consequences to this adapted method: firstly we do not actually need to know $f(.)$ exactly, we only need to know it up to a constant multiplier (i.e. we could replace $f(.)$ with $ hat{f} propto f$) - this is particularly useful for Bayesian modelling where we have the mantra: $$ Posterior , propto Prior times Likelihood$$ And so we don&#39;t always want to integrate out the posterior. Secondly it does not matter what the distribution function $g(.)$ we choose, the method will still work. The only constraint is that it needs to have a domain that is a super-set of the domain of $f(.)$. . However as before we do have that the acceptance probability is bounded by $ frac{1}{M}$ - therefore the &quot;better match&quot; the function $g(.)$ is to $f(.)$ the better the algorithm will perform. The &quot;wasted&quot; samples will exist between the two plots generated by $g(.)$ and $f(.)$ respectively. . Let&#39;s consider an example, suppose we find that we want to sample from: $$f(x) propto cos^{2}(x) exp(-x^2) $$ . Where we ignore the normalizing constant (which will not have the prettiest form) and x can take any value on the real line. . We know that $cos^2(x)$ takes values in $[0,1]$ and so we know that $f(.)$ is bounded by: $$g(x) propto exp(-x^2)$$ . We should now find $M$ which is the maximum of the ratio of $g(.)$ to $f(.)$ which is: $$M = max_x frac{f(x)}{g(x)} = max_x cos^2(x) = 1 $$ . So we can use the standard Gaussian as our proposal distribution in our acceptance rejection algorithm. . We can plot these functions using matplotlib: . # Plotting the target function and proposal function import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm %matplotlib inline def f(x): return np.cos(x)**2 * np.exp(-x**2) def g(x): return np.exp(-x**2) # Generate N samples N = 10000 U = np.random.random(N) V = np.random.random(N) Y = norm.ppf(V) M = 1 mask_acc = U &lt; f(Y) / (M *g(Y)) mask_rej = U &gt; f(Y) / (M *g(Y)) # Accepted Samples Y_acc = Y[mask_acc] U_acc = U[mask_acc] # Rejected Samples Y_rej = Y[mask_rej] U_rej = U[mask_rej] # Create plots for analytic functions X = np.linspace(-4, 4, 1000) PDF = f(X) g_x = g(X) # Estimate Normalizing constant dx = X[1] - X[0] const = PDF * dx const = const.sum() # Create Plots fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].plot(X, PDF, color=&#39;red&#39;, linewidth=2.5, label=&#39;Target&#39;) ax[0].plot(X, g_x, color=&#39;black&#39;, linewidth=2.5, label=&#39;Proposal&#39;) ax[0].set_title(&#39;Target and Proposal Distributions&#39;) ax[0].set_ylabel(&#39;f(x)&#39;) ax[0].set_xlabel(&#39;x&#39;) ax[0].set_xlim(-4,4) ax[0].set_ylim(0,1.2) ax[0].legend() ax[1].hist(Y_acc, density=True, color=&#39;grey&#39;, bins=50, label=&#39;Sample&#39;) ax[1].plot(X, PDF/const, linewidth=2.5, color=&#39;red&#39;, label=&#39;Target&#39;) ax[1].set_title(&quot;Empirical Distribution from Acceptance-Rejection Sampling&quot;) ax[1].set_ylabel(&#39;f(x)&#39;) ax[1].set_xlabel(&#39;x&#39;) ax[1].set_xlim(-4,4) ax[1].set_ylim(0,1) ax[1].legend() plt.show() . Which we can see performs similarly to the previous case with finite domain. . We can often improve the pure acceptance-rejection method by adding a lower bound function ($g_L$) such that: $$ g_L(x) leq f(x) leq Mg_M(x) $$ . We then modify our algorithm by adding an addition step. In pseudo-code we have: . Sample $X sim g_M$ and $U sim U[0,1]$ | Accept $X$ if $U leq frac{g_L(X)}{Mg_M(X)}$ | Else except $X$ if $U leq frac{f(X)}{Mg_M(X)}$ | This can offer improvements when the function $f(.)$ is computationally expensive to call. This procedure can reduce the number of calls potentially by a factor: $ frac{1}{M} int g_L(x) dx$. This is sometimes called the squeeze method. (Note: $g_L$ need not be a density function). We can also improve the efficiency further by taking piece-wise linear functions as our upper and lower bound functions, these are easy to specify and quick to call. In some instances it is easy to create such bounds (but in others this might not be possible). . Adaptive-Rejection-Sampling (ARS) . One extension to the acceptance-rejection sampling is adaptive rejection sampling (ARS) from Gilks and Wild (1992) - like many methods in computational statistics it is often possible to craft more efficient schemes for a specific purpose, however ARS does provide reasonable efficiency for a wide variety of sampling problems. . For ARS to work we require that the distribution function $f(.)$ is log-concave. That is: $$ frac{ partial^2}{ partial x^2} ln(f(x)) &lt; 0$$ . This may seem like a highly restrictive constraint but it actually turns out that very often a distribution function will have this property, even in seemingly complicated distributions. So this method is in fact fairly flexible and tends to offer some improvement over &quot;vanilla&quot; acceptance-rejection. . The main idea of ARS is to use the &quot;piecewise linear&quot; bounding functions and the &quot;squeeze method&quot; in acceptance-rejection above but applied to the log transform of the pdf rather than the pdf itself. We see that we require log-concavity since we can say with certainty that any chord (straight line connecting 2 points on the curve) will always be below the curve, thus it is easy to construct bounding functions. We will present how to do this construction below: . We take the notation: $h(x) = ln(f(x))$ for convenience. We specify $n$ points along the domain of $f(x)$. We denote these by $S_n= {x_1, x_2, ..., x_n }$. We define a line segment $L_{i,i+1}$ as being the straight line between points $(x_i, h(x_i))$ and $(x_{i+1}, h(x_{i+1}))$. By log concavity for all $x in [x_i, x_{i+1}]$ we have: $L_{i,i+1}(x) &lt; h(x)$. We denote this $ underline{h_n}(x) = L_{i,i+1}(x)$ for $x in [x_i, x_{i+1}]$ - piecewise linear. Through construction we also have that on the same interval: $min(L_{i-1,i}(x),L_{i+1,i+2}(x)) &gt; h(x)$ - so we have an upperbounding function: $ overline{h_n}(x) = min(L_{i-1,i}(x),L_{i+1,i+2}(x))$. With these functions we can apply the squeeze method, by taking: $ underline{f_n}(x) = exp( underline{h_n}(x) )$ and $ overline{f_n}(x) = exp( overline{h_n}(x))$. Since we will have: $ underline{f_n}(x) &lt; f(x) &lt; overline{f_n}(x) $ for the entire domain of $f(x)$. All that remains is to normalize $ overline{f_n}(x)$ such that: $ overline{f_n}(x) = omega_n g_n(x)$ with $ int g_n(x) dx = 1$ (i.e. $g(.)$ is a true pdf). . Since calls to $f(.)$ may be (very) expensive we want to limit the number of times we evaluate it, therefore whenever we create a new sample we add it to the set $S_n$ (so it becomes $S_{n+1}$). As the sampler runs the piecewise linear functions created become increasingly close to the target denity, this can lead to large improvements in efficiency when generating large numbers of samples from a complicated desnity function. Of course for large $n$ storing $S_n$ and related linear functions may become an issue in terms of memory, the rest of our model may also require memory storage and we will always aim to keep as much in cache/RAM as possible to ensure efficient running of the code. As such we may not want to store every sample we create, perhaps we would apply a rule that only if the new sample is a certain distance away from the other points does it get stored in $S_n$ or we could keep the number of nodes fixed and simply &quot;replace&quot; the &quot;less useful&quot; points as we find &quot;more useful&quot; points by sampling - again this is the part of Monte-Carlo that is as much art as it is science. . In pseudo-code we can write down the ARS procedure as: . Initialize $n$ and $S_n = { x_1, x_2, ... , x_n } $ | Sample $X sim g_n$ and $U sim U[0,1]$ | If: $U leq frac{ underline{f_n}(X)}{ omega_n g_n(X)}$ then accept sample $X$ | Else if: $U leq frac{f(X)}{ omega_n g_n(X)}$ then accept sample $X$ | If: $X$ is accepted append: $S_n to S_n cup {X } = S_{n+1}$ | Repeat steps $1 to 4$ for desired number of samples | We will not code this up as an example here since it gets a bit fiddly and will take up a reasonable amount of space. (I may come back and edit this later if I can create a very simple easy to read code). ARS is built into many stats packages (e.g. R has a pre-canned ARS sampler). In Python we have the (unfortunately named) package ARSpy which we can use. In doing a quick GitHub search I also found a clean implementation from Alberto Lumbreras: here! which the interested reader can investigate further should they wish. . Although log-concavity is not a &quot;rare&quot; property for a distribution function, neither is it completely general. As such there have been attempts to modify the method to drop this requiirement, however details of these is beyond the scope of this blog post. The ARS method is particularly useful in Markov-Chain Monte-Carlo which is something we have not yet touched on but will later. . This concludes our section on sampling from generic univariate distributions. We have seen a few different options for converting our PRNG samples into arbitrary distributions. The methods presented here are in essence the &quot;building blocks&quot; and represent some of the key concepts, they are blunt tools that are fairly universal in their application. Many more methods exist that offer improvements in computational cost but they are often very specific in nature (for example to simulate exact normal variates very quickly). . Conclusion . I this blog post we have looked at the theoretical underpinnings of some basic Monte-Carlo methods that can be used to sample from arbitrary probability distributions. We first started by looking at a motivating example to compare a &quot;traditional&quot; mathematical approach to computational approximation and then compared this to a stochastic Monte-Carlo option. . Knowing we would need to sample uniform variates we then looked at various options for pseudo-random number generation and found two main &quot;types&quot;: the multiple-linear-congruential-generator and the Mersenne-twister. We saw that for the purposes of the vast majority of Monte-Carlo simulations these offer reasonable statistical properties and aren&#39;t too inefficient. . Following on we saw various methods that we could use in order to &quot;convert&quot; the pseudo-random uniform variates generated into samples from a generic distribution. One main class being the &quot;generalized-inverse&quot; method (by far the most popular), which works however in some more complicated cases specifying the CDF is a challenge, let alone inverting it. Moreover the cost of calling an inverse CDF may be prohbitive. To overcome this we can rely of relationships between distributions, we can use inverse transform to generate &quot;easy&quot; distributions (e.g. exponential) and then apply functions to these samples to create the more complicated distributions we require. As an alternative we also saw &quot;acceptance-rejection&quot; schemes. These methods are useful and fairly general, however efficiency is highly dependent on the rejection rate and finding good proposal distributions (or creating them piecewise linearly) is often challenging. . We also saw some of the considerations at play when designing a Monte-Carlo model relating to the tradeoffs between code efficiency, statistical accuracy and (often overlooked) code transparency. . . References . In preparing this blog post I consulted the following texts: . Monte-Carlo Statistical Methods - CP Robert &amp; G Casella | Monte-Carlo Methods in Financial Engineering - P Glasserman | Non-Uniform Random Variate Generation - L Devroye | . And relied on lecture notes from Warwick University course &quot;Monte Carlo Methods&quot; (c.2010) .",
            "url": "https://www.lewiscoleblog.com/monte-carlo-methods",
            "relUrl": "/monte-carlo-methods",
            "date": " â€¢ May 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Entropy and Complex Systems",
            "content": "In this blog post we shall be looking at the concept of &quot;entropy&quot; which appears under a number of guises. We will look at a number of areas in which entropy arises and look at similiarities and differences. In the interests of brevity this will be a whistle-stop tour of each area and we will not cover all the details. Each area has enough results/theorems/etc. to cover a series of blog posts by themselves. In this blog post we are simply concerned with the concept of entropy. . (Note: This blog post follows the research by Rudolf Hanel and Stefan Thurner and in particular the first half of the 6th chapter of the book: &quot;Inroduction to the Theory of Complex Systems&quot; by Stefan Thurner, Rudolf Hanel and Peter Klimek) . Entropy Thermodynamics and Statistical Mechanics . When we mention &quot;entropy&quot; what many people first think of is a measure of &quot;disorder&quot; or &quot;uncertainty&quot; of a system. We imagine a system whereby an individual configuration is called a microstate. A microstate can be associated with a global behaviour called a macrostate. For each macrostate if we take (the logarithm of) the number of microstates that lead to the macrostate then we have the &quot;entropy&quot; quantity (Boltzmann entropy). A macrostate that has more generating microstates is considered to be more &quot;disordered&quot;. To take a simple example taking water ($H_2 O$) in solid (ice) form the individual molecules are (somewhat) forced into position in the crystaline structure, as such there are relatively few different orientations of molecules and hence a low entropy. As we move into the liquid phase the molecules move around more freely and so there are many more positions which the molecules can take, leading to a higher entropy. In the gaseous phase the molecules are whizzing around with higher energies and even more orientations are possible and so the entropy increases yet further. . The first definition of entropy comes from Rudolf Clausius in the mid 1800s through work in thermodynamics. He asserts that internal energy ($U$), temperature ($T$), pressure ($P$) and volume ($V$) are related via entropy ($S$) as: $$ dU = T dS - P dV $$ Through this definition we find that entropy is a way of relating the macroscopic behaviours of a system. This holds for reversible processes that are quasi-stationary (essentially: there can be an absorbing state that we reach almost-surely but initial conditions are such that the system will take a long time to reach it). We sometimes re-arrange this formulation slightly to give: $$ frac{dQ}{T} = dS$$ Where $Q$ is the heat added/removed to/from the system. There are a couple of extra properties we require of entropy, firstly it must be additive. If for example we take 2 identical systems and combine them the entropy must double, in other words: $$ S_{A+B} = S_A + S_B $$ For systems $A$ and $B$. Also we have that the second law of thermodynamics must hold - that is: for a closed system entropy must never be decreasing: $$dS geq 0$$ Which means if we know the macrostate at a given time then the state at a later time must be of equal or higher entropy. Moreover if the closed system is in equilibrium: $dS = 0$. . Another definition of entropy via statistical mechanics is: $$ S_B = k_B log Omega $$ Where $k_B$ is a unit-correcting constant and $ Omega$ is the number of microstates consistent with a macrostate. If the microstates are not equally probable we can use the Gibbs-Boltzmann form of entropy: $$S_B = -k_b sum_i p_i ln p_i $$ This can be shown to be equivalent to Clausius entropy by invoking the Boltzmann distribution, that is: $$ p_i = frac{1}{Z} exp left( frac{-E_i}{k_B T} right) $$ Where $E_i$ is the energy corresponding to the microstate, $T$ is the temperature and $Z$ is the partition function (normalizing constant). . To show the equivalency between Gibbs-Boltzmann and Clausius entropy formulations: begin{align} dS_B &amp;= -k_b sum_i dp_i (ln p_i) &amp;= -k_b sum_i dp_i left( frac{-E_i}{k_B T} - lnZ right) &amp;= sum_i frac{E_i dp_i}{T} &amp;= sum_i frac{d(E_i p_i) - (dE_i)p_i}{T} &amp;= frac{dE - dW}{T} &amp;= frac{dQ}{T} end{align} Which is equivalent to above. In the last line we used the first law of thermodynamics: $dE = dW + dQ$ - the change in energy is the work done plus the change in heat. We also used $ sum d(E_i p_i) = dE$ (average energy) and $ sum d(E_i)p_i = dW$ (work done). For this to hold we require that the system is in a thermal equilibrium. . This equivalence is quite an impressive and powerful result, we now have a mechanism by which we can relate the macrostates of a system to the microstates - that is we habe a link between the mircoscopic behaviour (particle behaviour) to macroscopic behaviour (phase of matter). The key assumption however is that the system is in equilibrium. . Entropy in Information Theory . Another area where entropy appears is in information theory. Put very simply information theory is concerned with the problem of encoding messages and how much information is required to encode a message so that it can be decoded again with a certian level of accuracy. If we know what messages we want to send and a noise-rate then we can work out what capacity of channel we would require. . In the context of information theory entropy is the term used to quantify &quot;uncertainty&quot; in a message/signal. High entropy represents a large uncertainty and vice versa. Typically this is taken to be the Shannon entropy: $$ S_S = - sum_i p_i ln p_i $$ Which looks somewhat familiar now we have looked at Boltzmann entropy ($S_B$) however we should note these represent fundamentally different things and any resemblance is merely coincidence. . Why is Shannon entropy a good measure in this setting? Let&#39;s consider an example where we have $N$ signals with associated probabilites: $(p_1, p_2, ... , P_N)$. If we have a determinstic case: $(p_1=1, p_2=0,...,p_N=0)$ then the Shannon entropy is zero. Noting: $$ lim_{x to 0} xlnx = 0$$ and $$1ln1=0$$ But also we can see that Shannon entropy is maximised for $p_i = frac{1}{N}$ - uniform distribution. This fits with our notion of &quot;maximum uncertainty&quot;. We also have for two systems $(A=(p_1, p_2, ... , P_N), B=(q_1, q_2, ..., Q_M)$ then: $$S_S(AB) = S_S(A) + S_S(B | A) $$ Where $(B|A)$ represents the distribution of $B$ conditional on $A$ - if $A$ and $B$ are fully independent then $(B|A)=B$ (for example flipping two coins, the probability of heads on one coin will not impact the probability of heads on the other - unless you taped the coins together or something equally silly). . By defining entropy in this way we can derive Shannon&#39;s noisy channel theorem (we will not prove this here) in layman&#39;s terms this states that: &quot;we can create a coding scheme for a message that allows it to be transmitted (essentially) error free if the information capacity of the channel is greater than the source entropy.&quot; We have been a little sloppy in not defining all the terms here but we only need the gist for our purposes. We can see that the entropy is an important concept. . However, we have not applied any restriction to our &quot;source&quot; of the signals/messages. Since we are dealing with uncertainty we typically rely on a probabilistic description. Further for the statement above to hold for $S_S$ we require the information source process to be: . Markov | Ergodic The first criteria states that the next signal to be sent depends only on the current signal, not the entire history of sent signals. The second criteria ensures that the process exhibits stationarity (rates for individual signals do not vary in time) and moreover there is no state we can start a trajectory in that makes it impossible to reach any other state. This assumption underlies most of the classic information theory results. | However Shannon also proposed 3 axioms (SK1, SK2, SK4) representing fundamental requirements that an entropy measure $S(p)$ should have for dealing with Markov-Ergodic (&quot;simple&quot;) information sources. Later Khinchin added a 4th (SK3). . SK1. - Entropy $S(p)$ must be continuous and depend only on arguments $p_i$ - probability of individual states and nothing else | SK2. - $S(p)$ must take it&#39;s maximum for $p_i = frac{1}{N}$ | SK3. - Adding an additional state with zero probability does not change the entropy value | SK4. - The composition of 2 systems should result in: $S(AB) = S(A) + S(B|A)$ - if the systems are fully independent this becomes $S(AB) = S(A) + S(B)$ | . Using these axioms we find that any functional $S(p)$ satisfying these axioms must be of the form: $$ S(p) = - k sum_i p_i ln p_i $$ Which is nothing more than Shannon entropy upto a multiplicative constant. . Statistical Inference . In statistical inference we are trying to answer the question: what is the best model that captures some data? Or given some model what is the most likely outcome? There are a number of methods we can use to do this, one of which is the &quot;maximum entropy principle&quot; which loosely states that the distribution with the highest entropy given our current data is our &quot;best guess&quot; for the distribution that generated it. This was originally popularized by Jaynes in the late 1950s. . To illustrate the method let&#39;s imagine that we have $N$ possible states of a system. We have observations $k=(k_1, k_2, ... , k_N)$ for how often each has occured in our data. The process generating the data has &quot;true&quot; distribution $q=(q_1, q_2, ... , q_N)$ with $ sum_i q_i = 1$. We then have the probability of our observation (assuming independence) following a multinomial distribution: $$P(k|q) = frac{N!}{k_1! k_2!...k_N!} q_1^{k_1} q_2^{k_2} ... q_N^{k_N}$$ Which we notice we can factorize as: $$ P(k|q) = M(k) G(k|q) $$ With: begin{align} M(k) &amp;= frac{N!}{k_1! k_2!...k_N!} G(q|k) &amp;= q_1^{k_1} q_2^{k_2} ... q_N^{k_N} end{align} We call the function $M(k)$ the multiplicity, and $G(q|k)$ the probability. If we take logarithms and divide by N this formula becomes: $$ frac{1}{N} ln P(k|q) = frac{1}{N} ln M(k) + frac{1}{N} ln G(q|k) $$ Where the left hand side of the equation is called the relative entropy (or Kullback-Leibler divergence). The right hand term containing $M(k)$ is called the entropy and the final term the cross entropy. Via Stirling&#39;s approximation we have: $$M(k) approx frac{N^N}{k_1^{k_1}k_2^{k_2}...k_N^{k_N}}$$ If we denote the sample average: $$p_i = frac{k_i}{N} $$ Then via Stirling: $$ S_J = frac{1}{N} ln M(k) = - sum_i p_i ln p_i $$ Which is the (now familiar) entropy funcational. We can also substitute $p_i$ into the formula above, in a similar way we can relate the relative entropy, entropy and cross entropy via: $$ - sum_i p_i ln frac{p_i}{q_i} = - sum_i p_i ln p_i + sum_i p_i ln q_i $$ We can make the assumption $q_i = exp^{- alpha - beta epsilon_i}$. And then: $$ frac{1}{N} ln P(k|q) = - sum_i p_i ln p_i - alpha sum_i p_i - beta sum_i p_i epsilon_i $$ By differentiating and setting to zero we end up with a set of equations we can solve for $p^*_i$ that is the maximum entropy distribution: $$ 0 = frac{ partial}{ partial p_i} ln P(k|q) = frac{ partial}{ partial p_i} left(- sum_i p_i ln p_i - alpha sum_i p_i - beta sum_i p_i epsilon_i right) $$ The constants $ alpha$ and $ beta$ represent Lagrangian multipliers here, $ alpha$ ensures normalization of $p_i$ and $ beta$ ensures the first moment (mean) is correct. If we have additional information (e.g. we know higher order moments) we can build this in via additional Lagrangian multipliers. . We can see that entropy in this sense has the same functional as before. The key assumption however is that the process is multinomial. . Equivalence of Entropy . We have now seen 3 classic examples of where the term entropy is used - the Boltzmann, Shannon and Jaynes entropy functionals. Since the functionals are identical (upto a multiplicative constant) it is natural for us to consider them equivalent. This is a mistake, although they are all called &quot;entropy&quot; and have the same form they represent different things. . It is worth noting that for each energy functional to make sense we make some pretty cavalier assumptions about the underlying systems: . Boltzmann entropy assumes a process is in equilibrium | Shannon entropy assumes an ergodic process | Jaynes entropy assumes a multinomial process | But what happens if we remove these assumptions? . Entropy and Complexity . Complex systems by their nature do not follow the assumptions above. For example most complex systems are non-ergodic. In particular they may be &quot;evolutionary&quot; systems that show path dependent behaviour which violates ergodicity. They may also display long term &quot;memory&quot; of previous states, which again is non-ergodic. So what can we say in these cases? . Using the Shannon-Khinchin axioms above we can see the most restrictive is $SK4$ - this is a requirement built off the properties of an ergodic system. What if we simply drop this axiom? Thurner-Hanel-et. al. have shown we can do a lot with the following axioms: . SK1. - Entropy $S(p)$ must be continuous and depend only on arguments $p_i$ - probability of individual states and nothing else | SK2. - $S(p)$ must take it&#39;s maximum for $p_i = frac{1}{N}$ | SK3. - Adding an additional state with zero probability does not change the entropy value | $S(p)$ takes a trace form: $S(p) = sum_i g(p_i)$ | . We can re-write these axioms further by taking the form: $S(p) = sum_i g(p_i)$ and noting that: . $g(.)$ must be continuous (by SK1) | $g(.)$ must be concave (by SK2) | $g(0) = 0$ (by SK3) | . If we define: $$ S_g(N) = sum_i g left( frac{1}{N} right) = N g left( frac{1}{N} right) $$ . We can then elicit scaling behaviour: begin{align} lim_{N to infty} frac{S_g(N lambda)}{S_g(N)} &amp;= lambda^{1-c} lim_{N to infty} frac{S_g(N^{1+a})}{S_g(N)N^{a(1-c)}} &amp;= (1+a)^d end{align} We can then see there are 2 constants that define scaling laws for entropy following this trace form. We have $0 leq c leq 1$ and $d$ real valued. If $c&gt;1$ then SK2 is violated and if $c&lt;0$ SK3 is violated. Moreover it has been shown that an entropy funcitonal can be of the form: begin{align} S_{c,d}(p) &amp;= frac{r}{c} A^{-d} e^A left( sum_i Gamma(1+d, A - c ln p_i) - rp_i right) A &amp;= frac{cdr}{1 - (1-c)r} Gamma(a,b) &amp;= int_b^{ infty} t^{a-1} e^{-t} dt end{align} Where $c$ and $d$ are the same as in the scaling properties above. There is an additional constant $r$ taking a range of values dependant on $c$ and $d$ to ensure the expression makes sense, it does not affect the scaling behaviour. We can thus use constants $c$ and $d$ to create equivalence classes of complex systems. . (Note: the expression of $S_{c,d}$ is not unique and simpler expressions exist, however this is a general form that allows for the widest range of $c$ and $d$ values available). . What happens if we take $c=1$ and $d=1$ as an example? Then $A=r$ and via some work (and the fact $ Gamma(2,x) = e^{-x}(x+1)$) we get: $$ S_{1,1}(p) = (r+1) - re^r - sum_i p_i ln p_i $$ Which we see as equivalent to the Boltzmann/Shannon/Jaynes entropy above (the additive constant non-withstanding which is a consequence of allowing SK4 to be relaxed - careful selection of $r$ can remove this constant). . So far we have limited ourselves to a trace form function: $S_{c,d}(p) = sum_i g_{c,d}(p_i)$ - however this is not a requirement. We can use entropy functionals that are not of trace form and still use the same $(c,d)$ criteria for equivalence classes. The table below shows some entropy functionals proposed and their equivalence class: . Name- Entropy-Functional-Form-- c --d-- . Boltzmann | $- sum_i p_i ln p_i $ | 1 | 1 | . Reyni | $ frac{1}{1- alpha}ln sum_i p_i^{ alpha} $ | 1 | 1 | . Tsallis | $ frac{1 - sum_i p_i^q}{q-1} $ | $q$ | 0 | . Abe | $- frac{ sum_i p_i^q - p_i^{1/q}}{q - 1/q}$ | $q$ | 0 | . Sharma-Mittal | $ - sum_i p_i^r (p_i^{ kappa} - p_i^{- kappa}) / 2 kappa $ | $r- kappa$ | 0 | . Landsber-Vedral | $ left( left( sum_i p_i^q right)^{-1} - 1 right) / (q-1)$ | $2-q$ | 0 | . Exponential | $ sum_i p_i left( 1 - e^{ frac{p_i-1}{p_i}} right) $ | 1 | 0 | . Anteonodo-Plastino | $ sum_i left( Gamma left( frac{ eta+1}{ eta}, -lnp_i right) -p_i Gamma left( frac{ eta+1}{ eta} right) right)$ | 1 | $ frac{1}{ eta}$ | . Shafee | $- sum_i p_i^{ beta} lnp_i$ | $ beta$ | 1 | . Hanel-Thurner | $ frac{r}{c} A^{-d} e^A left( sum_i Gamma(1+d, A - c ln p_i) - rp_i right)$ | c | d | . From this we can see that equivalence classes can contain functionals that appear very different from each-other. For example with the right choice of parameter Landsber-Vedral displays the same scaling as the Sharma-Mittal. . For any trace form of entropy we can denote the generalized logarithm of that entropy as: $$ Lambda(p_i) = - frac{d}{dp_i} g(p_i) $$ . We can apply scaling to $g$ to ensure $ Lambda(1) = 0$ and $ Lambda &#39; (1) = 1$ (this does not change the equivalence class of the entropy functional). This determines the probability distribution function of the system via: $$p(x) = Lambda^{-1}(-x) $$ . Given: $$S_{c,d}(p) = frac{r}{c} A^{-d} e^A left( sum_i Gamma(1+d, A - c ln p_i) - rp_i right)$$ Then: $$ Lambda_{c,d,r}(x) = r left( 1 - x^{c-1} left(1 - frac{1-(1-c)r}{dr} ln(x) right)^d right) $$ Which we can invert to give: $$ p_{c,d,r}(x) sim exp left( - frac{d}{1-c} W_k left( frac{(1-c)r}{1-(1-c)r} exp left( frac{(1-c)r}{1-(1-c)r} right) left(1 + frac{x}{r} right)^{ frac{1}{d}} right) right) $$ Where $W_k$ is the Lambert-W function, which only has real solutions for $k=0$ ($d geq 0$) or $k=-1$ ($d &lt; 0$). . By taking $c=d=r=1$ we regain the Boltzmann distribution: $$p_{1,1,1}(x) = e^{-x}$$ . Similarly for $c=1$ and we have a stretched exponential distribution: $$p_{1,d,r}(x) = e^{-dr left( left(1+ frac{x}{r} right)^{ frac{1}{d}}-1 right)}$$ . For $d=0$ and $r= frac{1}{1-c}$ we hae a power-law: $$p_{c,0,r} = (1+(1-c)x)^{ frac{-1}{1-c}}$$ . In general we find that the distribution functions will look very similar to the power-law case. This demonstrates the uniquity of power-law relations in non-ergodic systems. . Conclusion . In this blog post we have seen that the 3 &quot;classic&quot; uses of entropy (Boltzmann, Shannon and Jaynes) are in fact distinct despite having the same functional form. This leads to much confusion. Moreover we have seen that the degeneracy in functional form arises out of some pretty strong assumptions on the underlying process namely: equilibrium, Markov-Ergodicity and multinomial distributions. For some systems these assumptions are either true or reasonable approximations of reality, however in some cases these aren&#39;t good assumptions and using a Boltzmann entropy functional is asking for trouble. . We also looked at the Shannon-Khinchin axioms for defining a &quot;good&quot; entropy function. We saw that by dropping $SK4$ we were no longer bound to Ergodicity. As such we considered the scaling behaviour of an entropy functional and found 2 key forms of scaling $c$ and $d$, which we could use to create equivlance classes of entropy functionals and non-Ergodic systems. . By further assuming a trace form ($S(p) = sum_i g(p_i)$) of entropy functional we found a general form for the entropy functional. By taking the generalized logarithm of this function we find that the distribution function for the system must follow a Lambert-W exponential form. For all intents and purposes this leads to power-law behaviour which acts as justification for the ubiquity of power-laws in non-Ergodic systems. . In this blog we talk about &quot;modelling&quot; a lot. The result that non-Ergodic systems follow a distribution that is (at least somewhat like) a power-law suggests that our default modelling assumption for statistical systems should be a power-law type distribution unless we can specifically prove otherwise. . . References . Most of the work in this area (that I know about) is from Stefan Thurner and Rudolf Hanel. Some suggested reading is: . Introduction to the Theory of Complex Systems - Thurner, Hamel, Klimek (This blog post follows the first half of chapter 6 of this book) | https://arxiv.org/abs/1310.5959 - Generalized (c,d)-entropy and aging random walks - Hanel, Thurner | https://arxiv.org/abs/1211.2257 - Generalized entropies and logarithms and their duality relations - Hanel, Thurner, Gell-Mann | https://arxiv.org/abs/1005.0138 - A comprehensive classification of complex statistical systems - Hanel, Thurner | https://arxiv.org/pdf/1705.07714.pdf - The three faces of entropy for complex systems - Thurner, Corominas-Murtra, Hanel |",
            "url": "https://www.lewiscoleblog.com/entropy",
            "relUrl": "/entropy",
            "date": " â€¢ May 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Percolation",
            "content": "In this blog post we shall take a first look at percolation and how we can study it mathematically. . Introduction . Percolation theory and related studies is concerned with a simple problem: how does a substance pass through a random medium? Speaking in generalities such as this can obscure a concept so lets consider a real world example: we have a pool of oil sitting below a porous rock. We would like to know if we can &quot;push&quot; the oil through this rock or whether the rock is not &quot;porous enough&quot; for this to happen. (We could also ask a follow up question: if oil can pass can it do so in a timely matter?) Another real world example is a coffee percolator: Here we have a mass of ground coffee that we wish to pass (near) boiling water through in order to extract all its caffeine related goodness. From experience we know that if we grind very finely and pack very tightly the water has a hard time passing and we may need a lot of heat and pressure to get any coffee out (in some cases water may never pass). . We can also consider percolation in a slightly different way. Imagine we have a sheet of paper and a hole punch, we randomly start punching holes in the paper - at some point the paper will break into 2 (or more) smaller seperate pieces of paper. What can we say about the number of holes required for this to happen? . In all the examples above we are observing criticality or a phase transition. At some point of porosity the rock can allow oil to pass, if the rock is any less porous no oil can pass. Similarly grind the coffee too fine and the water will not pass through, but coarsen ever so slightly we can make (perhaps a small) cup of coffee. In the paper example the phase transition is even more obvious as the paper will physically break into multiple parts when we punch the last hole. Perhaps the most common example of a phase transition however are states of matter - in particular water changing to ice at 0 celcius or water vapour at 100 celcius (this is unrelated to percolation however). The point at which the phase of these systems change is called the critical point and it is a very common behaviour in both physical and social systems. Studying these can be quite illuminating as behaviour often &quot;gets weird&quot; when a system nears a critical point. . Mathematical Formulation . Now we have some real world intuition of what percolation theory is all about let&#39;s start to phrase it in a more mathematical setting so that we are better able to study it. . In percolation theory we are usually concerned with lattices. Lattices are a form of connected graph that can extend to infinity. A classic example would be a 2d square lattice where each site in the lattice has an &quot;up&quot;, &quot;down&quot;, &quot;left&quot; and &quot;right&quot; neighbour, we can imagine this as a sheet of squared paper. Other types of lattice can exist however; higher dimensions, different geometries and so on. . With our lattice we can then consider &quot;populating&quot; it, for each site in the lattice we randomly select whether it is populated or not. Typically we would consider this independently uniformly at random. We denote this probability $p$. One question we might ask is at what population probability is it possible to move from one side of the lattice to the other where each step we can move to a nearest neighbour only? Or we could ask the question what is the distribution of cluster size? What is the average cluster size? And so on. . In general we do this in relation to an &quot;infinite&quot; sized lattice since these are often easier to work with mathematically. When dealing with finite sizes we typicaly want to assume the lattices are sufficiently large that any &quot;edge effects&quot; (weirdness relating to edge sites having fewer neighbours than interior sites) can be ignored. Although investigations on smaller scale percolations are also an active field of research due to their practical application. . So far we have phrased the percolation problem in terms of site percolation - whereby the sites themselves are occupied or vacant. We could equally consider bond percolation whereby in the lattice we consider the &quot;edges&quot; between sites. With probability $p$ we select whether an edge exists. Now we can can consider &quot;paths&quot; travelling along the edges - can we move from the top of the lattice to the bottom? and so on. In our examples above the porous oil and coffee problems are examples of bond percolation, the paper problem is a site percolation. . One might expect these to be equivalent to one another however that is not always the case depending on the geometry of a lattice. . Given the formulation here we can see that percolation problems lend themselves to being studied computationally. We can randomly generate finite lattices and analyse them to get intiution about how these things behave. However using this approach will not allow us to ever have a definitive &quot;result&quot; without a corresponding mathematical proof. . Some extensions we may wish to consider include: . What if the sites/bonds are not independent (or uniformly) distributed? | What about the edge effects? | What about directed networks? | What if connectivity is not static over time? | How do we find the &quot;path of least resistance&quot; to pass through a realisation of a lattice? | . Why Should We Care? . We have seen one potential application of percolation theory: predicting whether we can extract oil through a porous rock. In fact when extracting oil they do take a small sample of the rock to analyse to see whether percolation is possible. The coffee and paper examples, while providing intuition, are not applications in the sense we would not use percolation theory to solve them! . What are some other examples we might care about? . Forest Fires - we could imagine a forest being made up of sites on a 2d lattice, some occupied with trees and some empty. We could investigate how forest density leads to fire spreading, we would not want to run real-world experiments on this for obvious reasons! We can build in more sophistication through modelling damp/dry patches, wind, air temperature and so on. The results from models like these can influence policy around back-burning, controlled cut-downs etc. to reduce the likelihood of larger more devastating wild fires. (This is a site percolation problem) | Social Networks - We may be interested in how information spreads through a social network. We can study how network connectivity influences this. If we were running a viral marketing campaign we might wish to understand the penetration required of the social network for an idea to take off. (This is a bond percolation problem) | Communication Networks - Suppose we are a telecom provider interested in keeping a service running 24/7. We have a network that allows this to happen with a number of connections between the individual nodes. We may wish to know how many connections (or nodes) need to &quot;go down&quot; for the entire system to fall over. We can then grow the network in such a way as to ensure robustness. (This is a bond percolation problem) | Electrical Engineering - If we imagine impregnating an insulating material with (for example) carbon nano-tubes, we could ask how much carbon we would need to add to turn the insulator into a conductor? Essentially we are asking what is the proportion of carbon required to make a &quot;bridge&quot; across the material to allow for electron flow. (This is a site percolation problem). | Fault Lines - Another application is the study of fault lines for use in predicting earthquakes. We can study the degree of &quot;cracking&quot; required for a &quot;split&quot; using tools based on percolation. | Biology/Medicine - Percolation theory has been used to understand virus fragmentation as well as modelling disease spread and fragmentation of ecological systems. | Percolation in 1 dimension . As with many physical and mathematical systems it makes sense to start with 1 dimension since this is often easy to visualize, the mathematics tends to be simple and we can generally use intuition to &quot;solve&quot; the system. . Our lattice in 1 dimension is essentially a line segment made up of a number of sites joined together. (We can equivalently consider a bond percolation in the exact same way, in this 1d example the 2 formulations are equivalent - not necessarily true in more complex scenarios!) . We assume that each site is occupied (independently) with fixed probability $p$. If we imagine taking this line segment and adding extra sites to the end one-by-one (to the limit of infinity), it does not take much to realise that if $p&lt;1$ then eventually we will come across an un-occupied site and so there will not exist a path from one end of the lattice to the other (i.e. there is no percolation!) Clearly if $p=1$ then every site is occupied and it is possible to create a path. We denote this probability $p_c$ (critical probability), it is one of the quantities we wish to study. In more complicated lattices we will not be able to use intuition quite so easily! . We will now step back and solve the system mathematically. In this case it is not strictly required, however it will provide a blueprint for how to approach more complicated lattices. It also allows us to introduce some of the notation. . We will fix a lattice size $L$, we will assume this is suitably large so that we can ignore any edge effect complications. The probability that there exists a cluster of size $s&lt;L$ (that is a group of adjacent sites that are all occupied) is: $$n_s = p^s(1-p)^2 $$ This is since we need $s$ occupied sites (probability $p^s$) surrounded by 2 empty sites (probability $(1-p)^2$) - since we are assuming independence this works as desired. We can look at the expected number of such clusters via: $Ln_s$ since we assume any edge effects are minimal (we could work this out exactly but it is not necessary here). Typically however we want to scale $L to infty$ so we will work with probabilities instead. Quite clearly if $p&lt;1$ then $n_s to 0$ as $s to infty$ - which is just a way of expressing our intuition above through mathematics. . We can further note that the probability that a site belongs to a cluster of size $s$ is $s n_s$ (it can be positioned in any 1 of the $s$ sites in the cluster). We then have (for an infinite size lattice) the relation: $$ sum_s s n_s = p $$ To see this we can note: begin{align} sum_s s n_s &amp;= sum_s s p^s (1-p)^2 &amp;= (1-p)^2 p sum_s frac{d}{dp} p^s &amp;= (1-p)^2 p frac{d}{dp} sum_s p^s &amp;= (1-p)^2 p frac{d}{dp} left( frac{1-p^{L&#39;}}{1-p} right) &amp;= (1-p)^2 p left((1-P^{L&#39;})(1-p)^{-2} - L&#39;p^{L&#39;-1}(1-p^{L&#39;-1})(1-p)^{-1} right) &amp; xrightarrow{L&#39; to infty} (1-p)^2 p (1-p)^{-2} &amp;= p end{align} Note: here we imagine a smaller &quot;window&quot; within our lattice such that $L&#39; leq L-2$ to avoid edge effects. This is a common &quot;trick&quot; to deal with asymptotic behaviours. There are other ways to deal with this such as using periodic boundary conditions too. In this example we could actually compute all the options if we wanted to, the extra terms would &quot;disappear&quot; when taking the limit. . This formula containing $p$ is actually fairly intuitive despite looking &quot;complicated&quot;, it essentially states: &quot;every occupied site has to belong to some cluster&quot; - so the sum over all cluster sizes must equal the probability that the site is occupied! In some cases the seemingly intuitive explanations can turn out to be wrong, so it is a good habit to prove these results rather take them for granted and only later realise they are incorrect. This seems especially true in probability theory. . We can now define a new variable: $$ w_s = frac{s n_s}{ sum_{s&#39;} s&#39; n_{s&#39;}}$$ Which is in some sense a &quot;weighting&quot; of relative cluster sizes. We can then calculate the average cluster size using: $$ S_L = sum_s s w_s $$ By taking the limit we can observe the asymptotic behaviour: $$ S_{ infty} = lim_{L to infty} sum_s s w_s $$ . Can we find a formula for this value? In the case of the 1d lattice we can, in other more complicated geometries we may not be able to. In the derivation below I exchange limits/sums/derivatives quite freely since this is valid in this case. begin{align} S_{ infty} &amp;= lim_{L to infty} sum_s frac{s^2 n_s}{ sum_{s&#39;} s&#39; n_{s&#39;}} &amp;= lim_{L to infty} sum_s frac{s^2 n_s}{p} &amp;= frac{(1-p)^2}{p} left( p frac{d}{dp} right)^2 lim_{L to infty} sum_s p^s &amp;= frac{(1-p)^2}{p} left( p frac{d}{dp} right)^2 frac{1}{1-p} &amp;= frac{(1-p)^2}{p} left( p frac{d}{dp} right) frac{p}{(1-p)^2} &amp;= frac{(1-p)^2}{p} left( frac{p}{(1-p)^2} + frac{2p^2}{(1-p)^3} right) &amp;= 1 + frac{2p}{1-p} &amp;= frac{1+p}{1-p} &amp;= frac{p_c+p}{p_c-p} end{align} For values of $p$ close to $p_c = 1$ we have: $$ S_{ infty} approx frac{2p_c}{p_c - p} propto |p_c - p|^{-1}$$ In general percolation problems the behaviour of cluster size around the critical probability is defined by the &quot;critical exponent&quot; which we denote $ gamma$: $$ S_{ infty} propto |p_c - p|^{- gamma} $$ In the case of the 1d lattice percolation this is $ gamma = 1$. . Another concept we have is the &quot;correlation function&quot; $g(r)$ which represents the probability that 2 sites distance $r$ apart belong to the same cluster. In the 1d case we can represent this as: $$g(r) = p^r$$ Which we can re-express in the form: $$ g(r) = exp left( frac{-r}{ xi} right)$$ Where $ xi = frac{-1}{ln(p)}$ is called the correlation length. For $p$ close to $p_c$ we have $ xi approx (p_c - p)^{-1} approx S_{ infty}$. Which is another critical exponent: $ xi approx (p_c - p)^{- nu}$. In this case $ nu = gamma$ but this is not necessarily true in general. . We can also express the cluster size via correlation functions: $$ S_L = sum_r g(r) $$ . This completely solves the percolation problem in the 1d lattice case. Unfortunately things are not always this simple. . Bethe Lattice . We now move onto another lattice that can be solved exactly: the Bethe lattice. A Bethe lattice is an infinite connected cycle-free graph where each node has exactly $z$ neighbours. It is also sometimes called a Caley tree. We can think of the 1d lattice case above as being a Bethe lattice with $z=2$. . . If the sites are arranged in &quot;shells&quot; as in the diagram above the number of sites in the kth shell is: $$N_k = z(z-1)^{k-1}$$ . We can think of the Bethe lattice as an infinite dimensional lattice. Why is this? If we consider a 3d space, we note that: $$Area propto L^2$$ For some length $L$ for a generic shape. similarly for volume we have: $$Vol propto L^3 $$ For example the volume of a sphere is $ frac{4}{3} pi r^3$ and the surface area of the sphere is $4 pi r^2$, and similar formulae exist for cubes, etc. We notice that: $$ frac{Area}{Vol}_{3d} propto frac{L^2}{L^3} = L^{1 - frac{1}{3}}$$ If we repeat the same procedure with a 4d space we get: $$ frac{Area}{Vol}_{4d} propto L^{1 - frac{1}{4}}$$ And in general for a D dimensional space: $$ frac{Area}{Vol}_{Dd} propto L^{1 - frac{1}{D}}$$ . What about the Bethe lattice? From above we know the number of sites in the kth shell is: $z(z-1)^{k-1}$ we can then image these shells &quot;encompassing&quot; all preceeding shells to get a &quot;volume&quot;. So in this case: $$Vol = 1 + sum_{i=1}^k z(z-1)^{i-1} = 1 + frac{z((z-1)^k - 1)}{z-2} = frac{z(z-1)^k - 2}{z-2}$$ And so: $$ frac{Area}{Vol}_{Bethe} = frac{z(z-1)^{k-1}}{ frac{z(z-1)^k - 2}{z-2}} xrightarrow{k to infty} frac{z-2}{z-1} $$ And so the ratio tends to a constant, in other words: $$ frac{Area}{Vol}_{Bethe} propto L^{0}$$ Which is in some sense equivalent to an &quot;infinite dimensional&quot; space (by taking $D= infty$ into the formula for $ frac{Area}{Vol}_{Dd}$). . We can find the percolation threshold probability through a relatively simple argument. We start at the origin (centre site); if a site exists in the next shell it has $(z-1)$ possible &quot;new&quot; neighbours. Given this site is occupied with probability $p$, the number of (expected) occupied neigbours is: $p(z-1)$. For an infinite cluster to exist we require this quantity to be greater than $1$ and so we have: $$p_c = frac{1}{z-1}$$ This ties in with our finding for the 1d lattice ($z=2$) as we would expect. Also notice that again in this case we could exchange &quot;bond&quot; for &quot;site&quot; and the argument still holds and so we have the critical probability is the same for both site and bond percolation problems. This does not hold true in general lattices. . We now introduce a new concept: In the 1d case we could only approach $p_c$ from below (at least I know of no way to consider probabilities in excess of 1!) Whereas now we can consider what happens above the percolation threshold. From common sense intuition we know that if $p=1$ the infinite cluster will in some sense be &quot;stronger&quot; than if $p$ is only slightly above $p_c$. We would like a metric to use to illustrate this concept. The infinite cluster strength represents the probability that a given site is part of the infinite cluster (that is we can follow a path from this site to infinity). Cleary the strength is zero below the percolation threshold and $1$ for $p=1$. We denote the cluster strength by $P(p)$ - this is an example of an order parameter. . To derive the cluster strength we introduce a quantity $Q(p)$ which is the probabilty that a site is not connected to the infinite cluster. We can then write: $$P(p) = p(1-Q^z(p))$$ Which is the probability that a site is occupied and at least one path to infinity exists for that site. But we can further write: $$Q(p) = (1-p) + pQ^{z-1}(p)$$ The probability the node is not occupied or it is occupied but the neighbours do not have a path to infinity. . We find that there is a trivial solution $Q(p)=1$ with non-trivial solution: $$Q(p) = 1 - frac{2p(z-1)-2}{p(z-1)(z-2)} $$ And so: $$P(p) = p left(1 - left( 1 - frac{2p(z-1)-2}{p(z-1)(z-2)} right)^z right) $$ Via a rather messy Taylor expansion we can show: $$ P(p) propto |p_c - p|^{-1} $$ This is another critical exponent for the Bethe lattice. In general lattices we shall find: $$ P(p) propto |p_c - p|^{- beta} $$ For some $ beta$. . We now move onto looking at the average cluster size $S(p)$. To do this we will introduce an intermediate varible $T$ which represents contribution from an individual bond in the lattice. The average cluster size containing the origin is then: $$S(p) = 1 + zT$$ Since there are $z$ bonds. For each branch we note that either the neighbouring site is un-occupied (probability $(1-p)$ but contribution 0 to the average) or it is occupied (probability $p$ with $(z-1)$ neighbours) so we can express $T$ as: $$ T = p(1 + (z-1)T) iff T = frac{p}{1-(z-1)p}$$ And so: $$S(p) = 1 + frac{zp}{1-(z-1)p} = frac{p_c(1+p)}{p_c - p}$$ Where $p_c = frac{1}{z-1}$. Thus the critical exponent is $ gamma = 1$ as in the 1d case. We can also expand on our previous identity from the 1d case to give: $$P(p) + sum_{s=1}^{ infty} s n_s(p) = p $$ This equation holds for all lattices not just the Bethe lattice. We may naturally ask what form does $n_s(p)$ take in the Bethe lattice? Clearly the formula from the 1d lattice is not correct. We can show that $n_s$ is of the form: $$ n_s(p) = g_{s,z} p^s (1-p)^{2+s(z-2)} $$ Where $g_{s,z}$ is some constant fixed for each cluster size ($s$) and Bethe lattice number ($z$). This constant can be hard to calculate for large $s$ since there are many possible configurations of sites. We can however look at ratios so remove the pesky constant: begin{align} frac{n_s(p)}{n_s(p_c)} &amp;= frac{p^s (1-p)^{2+s(z-2)}}{p_c^s (1-p_c)^{2+s(z-2)}} &amp;= left( frac{1-p}{1-p_c} right)^2 exp left(s ln left[ frac{p(1-p)^{z-2}}{p_c(1-p_c)^{z-2}} right] right) &amp;= left( frac{1-p}{1-p_c} right)^2 exp left( frac{-s}{s_ xi} right) end{align} With: $$ s_ xi = frac{-1}{ln left( frac{p(1-p)^{z-2}}{p_c(1-p_c)^{z-2}} right)} $$ Through a Taylor expansion we can show: $$ s_ xi propto |p_c - p|^{ frac{-1}{ sigma}} $$ With another critical exponent parameter $ sigma$. In the case of the Bethe lattice $ sigma = frac{1}{2}$. . We can now use $s_ xi$ to investigate $n_s(p)$. Through some more work we can show: $$n_s(p) propto s^{- tau} exp left( frac{-s}{s_{ xi}} right) $$ For another critical exponent $ tau$. In the case of the Bethe lattice we have $ tau = frac{5}{2}$. The quantity $s_{ xi}$ Is often called the &quot;cutoff cluster size&quot; since: for for clusters very much below this we have $n_s(p) propto s^{- tau}$ while above this clusters decay very quickly. . Percolation in d-dimenison with 1 &lt; d &lt; $ infty$ . We have seen percolation in 1 dimension and in infinite dimensions (Bethe lattice) - what about other dimensions? Unfortunately things get (much) more complicated. To see why this is we first saw that we could &quot;count&quot; the clusters of a given size in the 1d lattice fairly easily, in the Bethe lattice we could also rely on the &quot;cycle free&quot; structure to help us count the arrangement of sites. This is not so easy in other lattices. For example consider the 2d square lattice, if we just look at the different ways a site (in red) can contribute to a cluster (in black) we have: . And so (by counting the empty neigbouring sites): $$n_3(p) = 6 p^3 (1-p)^8 + 12 p^3 (1-p)^7 $$ These formulae only get more complicated as the cluster size increases and as the dimension increases. We cannot easily find a general solution to this problem. Even if we could it is unlikely we&#39;ll be able to make much traction using the techniques we have discussed so far. . So far we have seen that site percolation is equivalent to bond percolation. In general this is not the case, it is not hard to see this by considering bond and site percolations on the 2d space. We note however that any bond percolation problem can be reformulated as a site percolation problem (in most cases in a different lattice geometry). As such most study is focussed on site percolation. . In many problems an exact anaytic solution to the percolation problem is not possible/very difficult so many results are uncovered using computer simulations. . The table below summarises some of the critical probabilites for various lattice dimension and geometries: . pc Site Percolation pc Bond Percolation . 1d | 1 | 1 | . 2d Hex | 0.696 | 1-2sin(ðœ‹/18) | . 2d Square | 0.593 | 1/2 | . 2d Triangle | 1/2 | 2sin(ðœ‹/18) | . 3d Diamond | 0.430 | 0.388 | . 3d Cubic | 0.312 | 0.249 | . 4d Cubic | 0.197 | 0.160 | . 5d Cubic | 0.141 | 0.118 | . 6d Cubic | 0.107 | 0.094 | . Bethe (z) | 1/(z-1) | 1/(z-1) | . When not expressed as a fraction or trigonometric function probabilities are expressed to 3 significant figures. . Summary of Critical Exponents . Due to its simplicity for the 1d case we only looked at 1 critical exponent. In more complicated lattices we can describe 5 different critical behaviours $( gamma, beta, nu, sigma, tau)$: begin{align} S(p) &amp; propto |p_c - p|^{- gamma} P(p) &amp; propto |p_c - p|^{- beta} xi(p) &amp; propto |p_c - p|^{- nu} s_ xi(p) &amp; propto |p_c - p|^{ frac{-1}{ sigma}} n_s(p) &amp; propto s^{- tau} exp left( frac{-s}{s_{ xi}} right) end{align} Where these proportionalities only hold when we are close to the critical percolation probability. We can see that these will vary depending on the geometry of the lattices involved. . We can see that the behaviour of these quantities around the critical point behave as a power-law. This is a universal behaviour of &quot;criticality&quot; and what makes studying behaviour around phase transitions so interesting. The behaviours can become highly unusual, the critical exponents provide a way of classifying these behaviours. . Another exponent of interest is $ alpha$. We define the probability that a site (e.g. the origin) is connected to another site distance $r$ away as: $$q(r) propto r^{ alpha} $$ Where the probability is evaluated at the critical percolation probability. . We can summarise these exponents int he table below: . 1d 2d 3d 4d 5d 6d Bethe . ð›¼ | 0 | -2/3 | -0.62 | -0.72 | -0.86 | -1 | -1 | . ð›½ | 0 | 5/36 | 0.41 | 0.64 | 0.84 | 1 | 1 | . ð›¾ | 1 | 43/18 | 1.80 | 1.44 | 1.18 | 1 | 1 | . ðœˆ | 1 | 4/3 | 0.88 | 0.68 | 0.57 | 1/2 | 1/2 | . ðœŽ | 1 | 36/91 | 0.45 | 0.48 | 0.49 | 1/2 | 1/2 | . ðœ | 2 | 187/91 | 2.18 | 2.31 | 2.41 | 5/2 | 5/2 | . We notice that the critical exponent behaviour depends on the dimension only and not any specific geometry, unlike the critical probability. . Conclusion . In this blog post we have looked at the very basics of Percolation theory. We should have an idea of why we should study this phenomena through the use of some motivating examples. We gained some intuition by analysing 2 &quot;simple&quot; examples in 1d and infinite dimensions. We have also seen how the percolation problem gives rise to criticality and how behaviour around a critical point gives rise to power-law behaviours. . This is just a brief introduction and there are many areas we did not touch upon here. Since percolation theory can provide some truly interesting mathematics and insight I&#39;m sure I will visit some of these topics in a later blog post. .",
            "url": "https://www.lewiscoleblog.com/percolation",
            "relUrl": "/percolation",
            "date": " â€¢ Apr 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Cellular Automata and Iterative Algorithms",
            "content": "Cellular automata, despite the gradiose name, are very simple rule-based programs. Essentially we have a number of cells that can take various states, through time the cell&#39;s state will update according to some simple rule. We will begin with the simplest case. . 1D Cellular Automata . We start by looking at 1-dimensional cellular automata (elementary cellular automata). We imagine this consisting of a &quot;line&quot; of cells lined up 1 next to the other. Each cell can take values $0$ or $1$ (at least initially). The cells update their value in time based on a set of rules based on the previous value the cell has taken and the previous values taken by its two adjacent neighbours (left and right). At each step all cells update in unison (synchronous updates - to avoid any &quot;cascading&quot; type effects). . If we colour code the cells such that $0$ is equivalent to white and $1$ is equivalent to black we could describe update rules in tabular form: . . For example the top entry in the table gives the sub-rule that if the left neighbour is white, the cell is white and the right neighbour is white then the cells value in the next timestep is white. The second entry states that if the left neighbour and previous cell is white while the right neighbour is black then the cell updates to black. And so on for all the other entries in the table. There are 8 sub-rules total to describe this automata since each cell can take only 2 values and $2^3 = 8$. The collection of 8 sub-rules constitutes a rule. For convenience I refer to the &quot;input&quot; to the cell by a binary value (left most cell representing $4$, above $2$ and right $1$ in a binary representation). So a binary input of $001$ means the output of the update step will be $1$ (black) - the second row in the table. To classify the rules we also use binary, the right most column in the table shows how to do this, in this case the rule shown is rule $30$ ($2+4+8+16=30$). . That is all there is to it. On the face of it this does not seem very interesting but it turns out that cellular automata are deceptively complex even in this simple 1d form. To see this let&#39;s look at how rule 30 behaves, starting with a single black cell. . We will make a 1d cellular automata function to help us plot the automata. The state of the cells will evolve &quot;downwards&quot; on the plot, so the first row will represent the initial condition and the second will represent the state after one update, and so on. We will then call the function to generate images of the rules we wish to examine. For convenience we wil assume any &quot;cells&quot; outside of the frame are set to a $0$ state (white) - we could just as easily take a periodic boundary condition so that the left most cell is connected to the right most cell. In taking the $0$ assumption we can introduce &quot;edge effects&quot; for some rules, however this is fine for our purpose. . The function can be seen below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix Random seed np.random.seed(123) def CA_1D(N, rules, initial): # Initialize array image = np.zeros((N+1, 2*N+1)) image[0, :] = initial # Loop over total time steps for i in range(1,N+1): # Loop over cells per time step for j in range(2*N + 1): # if cell 0 set left neighbour to 0 if j == 0: left = 0 else: left = image[i-1, j-1] above = image[i-1,j] # If cell 2N+1 set right neigbour to 0 if j == 2*N: right = 0 else: right = image[i-1, j+1] # Convert to binary binary = right + 2*above + 4*left # Update cell image[i,j] = rules[7 - int(binary)] # Plot image of automata plt.figure(figsize=(10,15), frameon=False) plt.imshow(image, cmap=&#39;binary&#39;) plt.xticks([]) plt.yticks([]) plt.axis(&#39;off&#39;) plt.show() . We can now use this to plot rule 30, we will start with a single black cell in a row and evolve from there: . # Rule 30 N = 250 rules = np.array([0,0,0,1,1,1,1,0]) initial = np.zeros(2*N+1) initial[N] = 1 CA_1D(N, rules, initial) . This is somewhat counter to our intuition, we would not expect a simple set of rules to be able to generate such complicated, seemingly random behaviour. Instead we would expect something much more &quot;regular&quot; and &quot;predictable&quot;. . From our construction we know that there are &quot;only&quot; $256$ rules ($2^8$) so in this case it is possible to iterate through them all. Stephen Wolfram did this in the 1980s and observed 4 distinct classes of behaviour: . Class 1 - The automata quickly converges to a stable state and does not change | Class 2 - The automata quickly converges to a regular repetitive pattern | Class 3 - The automata quickly converges to a seemingly random or chaotic pattern (rule 30 is one such example) | Class 4 - The automata quickly converges to a number of distinct patterns that &quot;interact&quot; with each other in unpredictable ways | . We would have likely expected to observe class 1 and class 2 behaviour for all rules, the 3rd and 4th classes are interesting - they are not merely &quot;anamolies&quot; there are reasonably large numbers of rules in each class. These classes are some of the simplest examples of emergent behaviour - from seemingly simple rules comes complex global behaviours that look like they are specifically engineered. Stephen Wolfram landed on the phrase &quot;A New Kind of Science&quot; (and wrote a book under the same name) to represent this approach. Whereas in the past to engineer a solution we would have looked at large scale properties and try to &quot;construct&quot; a solution. The &quot;emergence&quot; approach suggests instead that we can find a &quot;universe&quot; of possible global behaviours generated by simple rules and then select the best (or at least a suitable) one. This very much mimics the evolutionary process. . As of 2020 the team at Wolfram have embarked on framing the physics problem of a &quot;theory of everything&quot; in terms of elementary computation (in some sense a generalization of the cellular automata presented here). They suggest exploring the space of fundamental computations in hope of finding rules that are able to generate all our physical laws (e.g. the strong and weak nuclear forces, general relativity, quantum theory etc). You can read more about this at: https://www.wolframphysics.org/ . Despite the apparent simplicity there are still many unanswered theoretical questions about 1d automata. Wolfram have offered a $30k prize for solving any one of the following open problems in relation to rule 30 as shown above: . Does the centre column remain non-periodic? (i.e. no repeating patterns) | Does the centre column (on average) contain equally many black and white cells? | Does computation of the Nth cell in the centre column require at least $O(N)$ effort? | Point 3 above alludes to the concept of computational irreducibility. If the computation requires at least $O(N)$ effort then (essentially) the cellular automata rule-based procedure is the &quot;most efficient&quot; way to compute these sequences and it is not possible to find a &quot;shortcut&quot; that allows you to predict what the result of some rule set more quickly. . If the centre column (or other rules) are capable of producing non-periodic random numbers this has quite a profound impact on the nature of randomness. What we percieve as randomness in the real world could infact be a deterministic process, moreover the rules generating it could be remarkably simple! From a more practical standpoint it could pave the way for better pseudo-random number generators for our stochastic models, cryptography and so on. . We will now look at examples from each of the 4 rule classes. For each one we will start with a random initial condition. . Class 1 (e.g. rule 160): . # Rule 160 # Class 1 Behaviour # Converges to stable state N = 50 rules = np.array([1,1,0,0,0,0,0,0]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules,initial) . Class 2 (e.g. rule 3): . # Rule 3 # Class 2 Behaviour # Periodic behaviour N = 50 rules = np.array([0,0,0,0,0,1,0,1]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules, initial) . Class 3 (e.g. rule 126): . # Rule 126 # Class 3 Behaviour # Seemingly random behaviour N = 250 rules = np.array([0,1,1,1,1,1,1,0]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules, initial) . Class 4 (e.g. rule 110): . # Rule 110 # Class 4 Behaviour # Localised repetitive structures that interact with each other N = 250 rules = np.array([0,1,1,0,1,1,1,0]) initial = np.random.randint(low=0, high=2, size=2*N+1) CA_1D(N, rules, initial) . 2d Cellular Automata . While the 1d cellular automata is interesting and easy to investigate and study we may want to extend these ideas. The first natural extension is to consider what happens to automata in 2 spatial dimensions. Unlike before we cannot (easily) plot a 2d automata evolving in time on a single figure moving down the page. We can still visualise however through either the use of animations or simply plotting static figures at certain time steps. . By expanding to 2 spatial dimensions it also opens up new geometries. In 1 dimension we were limited to the &quot;left, above and right&quot; neighbours. In 2 dimensions there are options; we could imagine a square lattice with 4 neighbours (up, down, left, right) or 8 neighbours (plus the diagonals) or we could imagine a triangular lattice with 6 neighbours, and so on. This opens up a lot of flexibility. . Many/all of the behaviours we observe in 1d also carry over to 2d. For a simple illustration of a 2d automata we will plot a simple &quot;class 2&quot; rule on a triangular lattice. Plotting a triangular lattice requires a little attention since computer arrays appear to &quot;naturally&quot; be square lattices. However it only requires a renaming of the indices to give 6 neighbours: . . To plot this we will use a scatter plot to more accurately capture the triangular lattice geometry. . We will implement a simple rule, if a cell is &quot;off&quot; and only any 1 neighbour is &quot;on&quot; then it shall flip its state to &quot;on&quot;, otherwise it remains unchanged. . The code for this can be seen below: . # 2d Cellular Automata on a Triangular Lattice import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix lattice size and iterations N = 24 every = 3 num_plots = 3 n_iter = num_plots*every # Set up a container - initialize with single element image = np.zeros((N,N)) image[int(N/2), int(N/2)] = 1 # Define neighbours x_neighbour = np.array([1, 0, 1, -1, 0, -1]) y_neighbour = np.array([-1, -1, 0, 1, 1, 0]) # Set up plot fig, axs = plt.subplots(1, num_plots+1, figsize=(20, 5)) fig.suptitle(&#39;Triangular lattice evolution&#39;) axs[0].set_title(&#39;Initial State&#39;) for x in range(num_plots+1): axs[x].scatter(N/2, N/2, color=&#39;black&#39;) axs[x].set_xticks([]) axs[x].set_yticks([]) axs[x].set_xlim(0,N) axs[x].set_ylim(0,N) axs[x].set_aspect(&#39;equal&#39;) if x &gt; 0: axs[x].set_title(&quot;Time Step: %i&quot; %(x*every)) # Set indicator for stopping plots plot_id = 1 # Loop update for i in range(1, n_iter+1): updated = np.zeros((N,N)) # Loop over lattice for x in range(1, N-1): for y in range(1, N-1): # Reset cumulative sum cumul=0 # Loop over neighbours and store &quot;on&quot; states for k in range(x_neighbour.shape[0]): cumul += image[x+x_neighbour[k], y+y_neighbour[k]] # If cell is empty and only 1 neighbour then update and plot if image[x,y] == 0 and cumul == 1: updated[x,y] = 1 # Loop over remaining plots for p in range(plot_id, num_plots+1): axs[p].scatter(y+(x-N/2)/2, N/2 + 0.866*(x-N/2), color=&#39;black&#39;) # Update image image += updated # Update plot identifier if i % every == 0: plot_id += 1 # End loop plt.show() . A we can see this rule generates a &quot;snowflake&quot;. . In a similar way to before we can try other rules if we so wish. However we shall not do so here for brevity. We could also try moving from a 2 state automata (cells taking states $0$ or $1$) to a multi-state system, however the number of rules to specify such an automata grows quickly (e.g. for a 4 state system in 1d we would require $4^3 = 64$ sub-rules) and the number of patterns increases rapidly also (e.g. for the same 4 state system in 1d there are over $8.5e37$ rule sets!). However via symmetry we can reduce this space slightly. Moreover we could, as above, define simple rules such as &quot;if over 50% of neighbours are on then on&quot; and so forth. . We now turn our attention to specific applications of cellular automata. . Game of Life . The Game of Life was developed by Conway in 1970 - before work on other cellular automata had taken place. It is called &quot;the game of life&quot; as it creates patterns that appear as if they are &quot;living&quot; when animated. In the language we have used so far it is a 2-state 2d cellular automata where the update rules are: . Any active cell with two or three active neighbours remains active. | Any inactive cell with three active neighbours becomes active. | All other active cells cells become inactive. | All other inactive cells remain inactive. | Again despite it&#39;s apparent simplicity the game of life is capable of showing some complex behaviour. . Let&#39;s code it up and take a look: . # Conway&#39;s Game of Life import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import HTML %matplotlib inline # Fix random seed (for initialization only) np.random.seed(123) # Set Grid size and initialize randomly N = 100 image = np.random.randint(low=0, high=2, size=(N,N)) ###### Main Code ###### # Update function to simulate a single evolution time step def update(): # Take copy of previous time step old = image.copy() # Loop over all cells for i in range(N): for j in range(N): # Count number of active neighbours with loop n_sum = 0 for x in range(-1,2): for y in range(-1,2): idx = (i+x) % N idy = (j+y) % N n_sum += old[idx,idy] # Remove current cell n_sum -= old[i,j] # Apply Conway&#39;s rules and update if old[i,j] == 1: if n_sum == 2 or n_sum == 3: image[i,j] = 1 else: image[i,j] = 0 if old[i,j] == 0: if n_sum == 3: image[i,j] = 1 else: image[i,j] = 0 ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, N-1), ylim=(0, N-1)) axes.set_xticks([], []) axes.set_yticks([], []) viz = plt.imshow(image, cmap=&quot;binary&quot;) # Define animation step def animate(frame): update() viz.set_array(image) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=50, interval=150) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect As we can see there appears to be living organisms within our simulation! Somewhat reminiscent of bacteria. Of course there is no such thing as an &quot;organism&quot;, they are just &quot;lightbulbs&quot; turning on and off in sequence. This raises a philosophical question of whether this constitutes a &quot;life&quot; of whether it is just a homuncular functionism (in laymans terms: are we seeing life where it does not exist, like how we often &quot;see&quot; animals in clouds or faces within muffins in a coffee shop). . Since the game of life has been studied so thoroughly there are a number of stable behaviours people have discovered including &quot;static objects&quot;, &quot;blinkers&quot; that appear to oscilate fixed in space, &quot;gliders&quot; that travel along the screen and &quot;ray guns&quot; that appear to &quot;shoot&quot; bullets across the screen. Some of these have been classified on the wikipedia entry: https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life#Examples_of_patterns We could reproduce some of these behaviours by choosing suitable starting conditions in the code above. . It has also been shown that the game of life is capable of performing computation. So you could add two integers together (for example) using the game of life. Moreover it has been shown that the game of life is Turing complete, this essentially means that (with a large enough automata) it can recreate any code. So, in theory, we could recreate the entire Windows operating system entirely out of the game of life! (Of course this is not an efficient way to do this). This is very surprising that such a (seemingly) simple program is capable of creating such complexity. This clearly have implications for evolution, where emergent behaviour and characteristics could be built out of much simpler fundamental iterative rules. If we could codify these rules we could (in theory) put them in a computer and recover the entire evolutionary history of life on earth. . We could modify Conway&#39;s procedure above to test different rule sets to investigate how the &quot;life&quot; changes. We could imagine this akin to different environments in an evolutionary setting. We could also add &quot;noise&quot; to the system, so the rules above are only implemented correctly a certain percentage of the time (e.g. &quot;Any active cell with two or three active neighbours remains active with 80% probability and becomes inactive with 20% probability&quot;). This could be an analogue to mutations in the evolutionary system. We could also investigate &quot;adaptive&quot; rules, whereby the rules change over time (perhaps determinstically or perhaps dependent on the state of the system). Each of these are simple modifications to the base code above. . Termite and Wood Chips . We now look at a simple model infuenced by cellular automata: the termite and woodchip model. One could argue that this is in fact an agent based model rather than a cellular automata. However the line between cellular automata and agent based model is somewhat fuzzy. . The model environment is a 2d grid (square lattice) taking states $0$ (empty) or $1$ (containing a &quot;woodchip&quot;). In the environment there are &quot;termites&quot; that wander around according to a random walk. If the termite encounters a woodchip it picks it up (the grid cells state changes from $1$ to $0$). The termite will then carry the woodchip, if it encounters another woodchip while holding one then the termite will drop the woodchip at the next empty cell that it encounters (that cell changing from state $0$ to $1$). This is repeated multiple times. . As this is a dynamic model it helps to visualize with an animation. We shall denote empty cells as &quot;white&quot; and woodchips by &quot;black&quot;. Termites will be represented by &quot;red circles&quot; traversing the space. . This is a very simple model. We can see an example implementation below: . # Termite and Wood Chip model import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import HTML %matplotlib inline # Fix random seed np.random.seed(123) ###### Model Parameters ###### N = 50 # Grid size N_m = 100 # Number of termites Chip_p = 0.3 # Proportion of grids containing wood chips num_frames = 250 # Number of animated frames # Initialize # Grid of woodchips Grid = np.zeros((N, N)) for i in range(N): for j in range(N): if np.random.random() &lt; Chip_p: Grid[i, j] = 1 Initial_Grid = Grid.copy() # Initial Location of termites Mites = np.random.randint(low=0, high=N+1, size=(N_m, 2)) # Holding woodchip? Hold = np.zeros(N_m) # Need to drop woodchip? Drop = np.zeros(N_m) # Define update step def update(): # Loop over all termites for m in range(N_m): # Select direction to move if np.random.random() &gt; 0.5: # Move vertical if np.random.random() &gt; 0.5: Mites[m,1] = (Mites[m,1] + 1) % N else: Mites[m,1] = (Mites[m,1] - 1) % N else: # Move horizontal if np.random.random() &gt; 0.5: Mites[m,0] = (Mites[m,0] + 1) % N else: Mites[m,0] = (Mites[m,0] - 1) % N x = Mites[m,0] y = Mites[m,1] # If mite encounters chip and isn&#39;t holding # The pick up if (Grid[x,y] == 1) and (Hold[m] == 0): Grid[x,y] = 0 Hold[m] = 1 # If termite is holding and enconters chip # then note it is ready to drop chip if (Grid[x,y] == 1) and (Hold[m] == 1) and (Drop[m] == 0): Drop[m] = 1 # If termite encounters empty grid cell and is ready to drop # ready to drop then drop chip if (Grid[x,y] == 0) and (Hold[m] == 1) and (Drop[m] == 1): Grid[x,y] = 1 Hold[m] = 0 Drop[m] = 0 ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, N-1), ylim=(0, N-1)) axes.set_xticks([], []) axes.set_yticks([], []) viz = plt.imshow(Grid, cmap=&quot;binary&quot;) m_viz = plt.scatter(Mites[:,0], Mites[:,1], color=&#39;red&#39;) # Define animation step def animate(frame): update() viz.set_array(Grid) m_data = np.array((Mites[:,1], Mites[:,0])).T m_viz.set_offsets(m_data) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=num_frames, interval=25) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect We can see the termites busy working away, we cannot yet see much of a pattern. We do notice however that the chips are slowly becoming more concentrated (there are defined regions of black and white). If we continue the process for many more iterations we can see that the termites do indeed create wood chip piles. . We will not animate the entire process since it will be a long animation and a large amount of data, instead we will just plot the initial state of the woodchips and the state following 10 million time steps: . If we left this run even longer we would be left with a single large pile (or a small number of large piles) of woodchips. . This is really quite a remarkable emergent behaviour. Each individual termite does not show anything we would term &quot;intelligence&quot; and there is no communication between the termites. Yet if we only observe chip piles our first instinct would be that they are a result of some sort of intelligent behaviour and/or cooperation or perhaps even some global oversight telling how each individual termite should behave. It seems almost unfathomable that this could be the result of only a few simple rules defining individual behaviour. . Schelling Model . We now move onto another application of Cellular Automata through the Schelling model. As with the previous model there&#39;s an argument as to whether this is an agent-based model or a cellular automata. And as with the previous models the enviroment shall be a 2d square lattice (although this is not a requirement and it can be modified to essentially any geometry). . Schelling developed this model in the 1970s, reportedly on graph paper using coins to represent the agents - illustrating how simple automata based models can be. Thankfully now computation makes it easier to investigate the model. . Schelling&#39;s model aims to study how spatial segregation occurs. In our example each &quot;cell&quot; in the square lattice can be in one of three states: empty (denoted by $0$ or white), populated by agent of type A (denoted by $1$ or red) or populated by agent of type B (denoted by $-1$ or blue). There is a global &quot;threshold&quot; that represents how tolerant (or intolerant) the agents are, if the percentage of neighbours of the same type is above this threhold the agent is &quot;satisfied&quot; and the cell remains in the same state. If the percentage of neighbours is below this threshold the agent is &quot;unsatisfied&quot; - the cell will then become empty and some other (randomly selected) empty cell will become occupied by an agent of the same type (so the number of A, B and empty states remains constant over time). This is a simple model of agents &quot;moving location&quot; to satisfy some preference. . Since agents &quot;look&quot; for an empty cell it is difficult to implement a synchronous update scheme (e.g. if 2 agents are moving in the same step and select the same empty cell who gets it?) To avoid this we will randomly select an agent to move (cell to update) in each time step, this is fine for our purposes. We will also assume periodic (toroidal) boundary conditions so that each agent has the same number of neighbours and to avoid &quot;edge effects&quot;. . Again this is remarkably simple as a model. We likely wouldn&#39;t believe this is capable of producing any &quot;interesting&quot; behaviour. . Let&#39;s code up an example: . # Schelling&#39;s Segregation Model import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import HTML %matplotlib inline # Fix random seed np.random.seed(123) ###### Model Parameters ###### N = 100 # Lattice size pct_empty = 0.25 # Percentage of cells empty A_B_ratio = 0.5 # Ratio of agents A:B threshold = 0.4 # Threshold for satisfaction N_step = 200 # Number of agent moves per frame to reduce animation size num_frames = 150 # Number animation frames ###### Set up ###### # Set up lattice cells = np.zeros((N,N)) # Set up lookup array for convenience lookup = np.arange(N*N).reshape(N,N) # Set initial conditions according to population proportions for i in range(N): for j in range(N): if np.random.random() &gt; pct_empty: if np.random.random() &gt; A_B_ratio: cells[i,j] = 1 else: cells[i,j] = -1 initial_cells = cells.copy() # Define a single update step def update(): for step in range(N_step): # Non-zero cells nz nz = lookup[cells != 0] # zero cells z z = lookup[cells == 0] # Select random non-zero cell active = np.random.choice(nz) # Retrieve x and y coordinates active_x = active % N active_y = int((active - active_x) / N) # Current state state = cells[lookup == active][0] # Caclulate neighbour proportions n_prop n_same = 0 for x in range(-1, 2): for y in range(-1, 2): xid = (active_x + x) % N yid = (active_y + y) % N if cells[xid, yid] == state: n_same += 1 / 8 # If n_prop is less than threshold select new empty state if n_same &lt; threshold: cells[lookup == active] = 0 new_cell = np.random.choice(z) cells[lookup == new_cell] = state ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, N-1), ylim=(0, N-1)) axes.set_xticks([], []) axes.set_yticks([], []) viz = plt.imshow(cells, cmap=&quot;bwr&quot;) # Define animation step def animate(frame): update() viz.set_array(cells) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=num_frames, interval=75) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect To highlight the segregation we look at the initial (random) configuration and the configuration at the end of the animation: . We can see that even if the agents are fairly &quot;tolerant&quot; (they require only 40% of the surrounding cells to be the same type as them) the system eventually ends up with clusters of agents of the same type. This is quite a remarkable result that goes completely against common sense where one would expect low levels of tolerance (high threshold for satisfaction) to lead to segregation. In fact in the Schelling model very low levels of tolerance leads to less segregation - since the agents are rarely happy and continue to search never settling down. . To observe this let&#39;s look at what happens if we make the threshold $80 %$ - then agents are only happy when at least 7 out of 8 neighbours are of their type: . We can see here the result of high levels of intolerance does not lead to segregation. . Now one could argue the real world application of this model and the assumptions it makes. However it does raise an interesting point regarding emergence: localised behaviour (of individuals) does not always translate to global behaviour (of the entire system). This is a big problem for the social sciences where they have traditionally opted for the reductionist method as used in the traditional (natural) sciences. Schelling&#39;s simple model shows that this is not always appropriate and the assumption that local and global behaviours are related in all cases is a fallacy. This is fairly troubling when the results from traditional social sciences influences governmental behaviour! Once you are aware of this disconnect you begin to notice this fallacy everywhere. . Despite its simplicity the Schelling model has been shown to produce segregation patterns that closely matches observed residential patterns in cities across the world. . Again if we wanted to it is relatively easy to think of extensions to this model. The obvious example being: what if we have multiple agent types? Another could be: what happens if we allow heterogeneous thresholds (i.e. each agent has their own threshold rather than all agents having the same)? As before adding noise is also an option. Or we could believe that with more exposure agents become more tolerant and so their tolerances change over time depending on exposure. Perhaps we want to allow &quot;foresight&quot; - so an agent evaluates the open cells and chooses the one according to their preference. In addition to that we could allow a &quot;moving delay&quot; so that by the time the agent moves the situation could be different to what they originally intended. It is interesting to consider if these extensions change the behaviour in meaningful ways. . Conclusion . In this blog we started by looking at simple 1d 2-state cellular automata, we moved through to 2 spatial dimensions. We then took these ideas and applied them to create 3 models: Conway&#39;s game of life, Termite-Woodchip and Schelling&#39;s segregation model. Through each of these we noticed that from simple deterministic rules there can be sophisticated emergent behaviours. This has a profound impact on our understanding of nature. . Some of these observations include: . Simple deterministic rules can produce (seemingly) random behaviour - what does this say about the nature of &quot;randomness&quot; in our universe? | Simple deterministic rules can produce chaotic results | Some sets of rules exhibit computational irreducibility while others do not | Simple rules (such as Conway&#39;s Game of Life) can be Turing complete and capable of universal computation | Simple individual behaviour can give the illusion of sophistication and/or large scale cooperation/governance | Observed large scale behaviour does not necessarily correlate with individual local behaviour and vice versa (in the words of PW Anderson: &quot;More is different&quot;) - Schelling&#39;s model shows that are intuitions can be wrong | Following on from the above scientific reductionism is not always appropriate and emergent behaviours may become the next &quot;scientific enlightenment&quot; | .",
            "url": "https://www.lewiscoleblog.com/cellular-automata",
            "relUrl": "/cellular-automata",
            "date": " â€¢ Apr 21, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Gillespie Algorithm",
            "content": "The Gillespie algorithm is one of the most historically important stochastic simulation algorithms ever created. At its heart the intuition behind it is very simple and it is re-assuring that it &quot;works&quot; - this is not always the case with stochastic simulation where the &quot;obvious&quot; idea can sometimes have unintended debilitating consequences. . The algorithm was first presented by Doob (and is sometimes refered to as the Doob-Gillespie algorithm) in the mid 1940s. It was implemented by Kendall in the 1950s. However it wasn&#39;t until the mid 1970s that Gillespie re-derived the method by studying physical systems that it became widely used. In publishing the method he essentially created the entire fields of systems biology and computational chemistry by opening the door to what is possible through stochastic simulation. . Background . In this blog we will consider applying the Gillespie method to the area of chemical reaction kinetics, this is the application Gillespie originally had in mind. The concepts described will carry over to other applications. . Imagine we wish to model a particular chemical reaction. We could use a determistic approach to model the reaction, this will require setting up a family of coupled differential equations. In doing so we will essentially &quot;ignore&quot; any microscopic behaviour and look at the reaction system at a &quot;high level&quot;. This can mean we miss out on a lot of the &quot;detail&quot; of the reaction which may be of interest to us. Further in some cases this approach may not even be applicable, for example to set up a differential equation we assume that we have large quantities of reactants that are perfectly mixed, this allows us to &quot;average over&quot; all reactants to create nice smooth dynamics. This may not reflect reality if there are only relatively few reactants in a system. An alternate approach is to use a stochastic &quot;discrete event&quot; model - this is where we model individual reactions seperately as discrete events occuring in time. This matches our physical intuition of how reactions occur: we wait until the reactants &quot;bump&quot; into each other in the right way before a reaction occurs. One way to summarise this mathematically is through the use of a &quot;master equation&quot;. . In the sciences a master equation represents the time evolution properties of a multi-state jumping system, by which we mean a system that &quot;jumps&quot; between distinct states through time (in contrast a &quot;diffusion system&quot; varies gradually). The system in question being stochastic in nature we are concerned with observing how the state distribution varies over time, for example: with some initial condition what is the probability of finding the system in a particular state within the next X seconds/minutes/years? Of course the time units depend on the nature of the system (e.g. if we construct a master equation for predator/prey dynamics we are unlikely to be interested in microsecond timescales, however if looking at a chemical reaction we are unlikely to find a timescale in days useful.) If we want to display the master equation mathematically we use a transition rate matrix $A(t)$ - this can evolve in time or it can be static. . We can then express the master equation in the form: $$ frac{d mathbf{P}_t}{dt} = A(t) mathbf{P}_t $$ Where vector $ mathbf{P}_t$ represents the probability distribution of states at time t - obscured by notation is an initial condition. Those from a mathematical or probabilistic background will recognise this as a Kolmogorov backwards equation for jump processes. If we expand the notation a little such that $P_{ij}(s,t)$ represents the probability of the system being in state $i$ at time $s$ and state $j$ at time $t$ then we can note that the transition rate matrix satisfies: begin{align} A_{ij}(t) &amp;= left[ frac{ partial P_{ij}(t,u)}{du} right]_{u=t} A_{ij}(t) &amp; geq 0 quad quad quad quad forall i neq j sum_j A_{ij}(t) &amp;= 0 quad quad quad quad forall i end{align} Further we can note that if there is a distribution $ pi$ such that: $$ pi_j A_{ij}(t) = pi_i A_{ij}(t) $$ For all pairs of states $(i,j)$ then the process satisfies detailed balance and the process is a reversible Markov process. . Gillespie Algorithm . The Gillespie algorithm is allows us to model the exact dynamics described by the master equation. In some (simple) cases we can solve the master equation analytically, but for complicated examples (e.g. say we have 50 different types of reaction occuring) this may not be feasible and so the Gillespie algorithm (or some sort of simulation method) is necessary. In pseudo code we can write down the Gillespie algorithm as: . Initialization - initialize the system, in the context of reaction kinetics this amounts to the setting up initial chemical concentrations | Monte-Carlo - Randomly simulate the time to the next event | Given an event has occurred randomly select which event has occured | | Update - based on 2. move the model time forward to the event time and update the state of the system | Repeat - Iterate through steps 2. and 3. until some stopping criteria is met | This essentially follows our intuition and there is no &quot;technical trickery&quot; such as fancy sampling methods, acceptance/rejection, etc. It is just a clean simple method - which is nice! Since we model by event as opposed to discretizing time steps this is an &quot;exact&quot; simulation method - meaning any trajectory simulated will follow the master equation dynamics exactly. However due to the random nature of any trajectory we will have to loop over these steps multiple times to find &quot;typical&quot; reaction paths (or whatever property we are trying to study). . An Example . To illustrate the algorithm in action we will take a simple reaction. We will have following forward reaction $$A + B to AB$$ Where two monomers $A$ and $B$ react to form a dimer $AB$. The corresponding reverse reaction being: $$AB to A + B$$ We will denote the rate of the forward reaction to be $r_f$ and the rate of the backward reaction to be $r_f$. If we let the number of molecules present be denoted by: $N_A, N_B$ and $N_{AB}$ then the rate of any reaction occurring is: $$R = r_f N_A N_B + r_b N_{AB}$$ Also given a reaction has occured the probability of the forward reaction having taken place is: $$ mathbb{P}(A + B to AB) = frac{r_f N_A N_B}{R}$$ For a model such as this we typically want to remove any &quot;path dependence&quot; - the arrival of the next reaction event is independent of reactions that have occurred previously (given the concentration of reactants). To satisfy this constraint typically reactions events are taken to follow a Poisson process. Under this assumption the number of reactions occuring within a time period $ Delta T$ follows a $Poisson(R Delta T)$ distribution. Moreover the time between reactions is then follows an exponential distribution. Thus if we sample $u sim U[0,1]$ then we take the time until next reaction to be $ tau = frac{1}{R}ln left( frac{1}{u} right)$. (Note: here I have used that $U$ and $(1-U)$ have the same distribution). . A basic implementation of this can be seen below: . # An implenetation of the Gillespie algorithm # applied to a pair of reactions: # A + B -&gt; AB # AB -&gt; A + B import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed for repeatability np.random.seed(123) ###### Fix model parameters ###### N_A0 = 25 # Initial number of A molecules N_B0 = 35 # Initial number of B molecules N_AB0 = 5 # Initial number of AB molecules rf = 2 # Forward reaction rate rb = 1 # Backwards reaction rate steps = 25 # Number of reactions per trajectory cycles = 100 # Number of trajectories iterated over # Set up holder arrays T = np.zeros((cycles, steps+1)) N_A = np.zeros((cycles, steps+1)) N_B = np.zeros((cycles, steps+1)) N_AB = np.zeros((cycles, steps+1)) # Store initial conditions N_A[:,0] = N_A0 N_B[:,0] = N_B0 N_AB[:,0] = N_AB0 ###### Main Code Loop ###### for i in range(cycles): for j in range(steps): # Calculate updated overall reaction rate R = rf * N_A[i,j] * N_B[i,j] + rb * N_AB[i,j] # Calculate time to next reaction u1 = np.random.random() tau = 1/R * np.log(1/u1) # Store reaction time T[i, j+1] = T[i,j] + tau # Select which reaction to occur Rf = rf * N_A[i,j] * N_B[i,j] / R u2 = np.random.random() # Update populations if u2 &lt; Rf: N_A[i,j+1] = N_A[i,j] - 1 N_B[i,j+1] = N_B[i,j] - 1 N_AB[i,j+1] = N_AB[i,j] + 1 else: N_A[i,j+1] = N_A[i,j] + 1 N_B[i,j+1] = N_B[i,j] + 1 N_AB[i,j+1] = N_AB[i,j] - 1 # Calculate an average trajectory plot ave_steps = 100 T_max = T.max() # Set up average arrays T_ave = np.linspace(0,T_max,ave_steps+1) N_A_ave = np.zeros(ave_steps+1) N_B_ave = np.zeros(ave_steps+1) N_AB_ave = np.zeros(ave_steps+1) N_A_ave[0] = N_A0 N_B_ave[0] = N_B0 N_AB_ave[0] = N_AB0 # Pass over average array entries for i in range(1, ave_steps+1): tmax = T_ave[i] A_sum = 0 B_sum = 0 AB_sum = 0 t_count = 0 # Pass over each trajectory and step therein for j in range(cycles): for k in range(steps): if T[j,k] &lt;= tmax and T[j,k+1] &gt; tmax: t_count += 1 A_sum += N_A[j,k] B_sum += N_B[j,k] AB_sum += N_AB[j,k] # Caclulate average - taking care if no samples observed if t_count == 0: N_A_ave[i] = N_A_ave[i-1] N_B_ave[i] = N_B_ave[i-1] N_AB_ave[i] = N_AB_ave[i-1] else: N_A_ave[i] = A_sum / t_count N_B_ave[i] = B_sum / t_count N_AB_ave[i] = AB_sum / t_count ###### Plot Trajectories ###### fig, axs = plt.subplots(3, 1, figsize=(10,20)) # Plot average trajectories axs[0].plot(T_ave, N_A_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[0].set_title(&#39;Number A Molecules&#39;) axs[0].set_ylim((0,35)) axs[0].set_xlim((0,0.125)) axs[1].plot(T_ave, N_B_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[1].set_title(&#39;Number B Molecules&#39;) axs[1].set_ylim((0,35)) axs[1].set_xlim((0,0.125)) axs[2].plot(T_ave, N_AB_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[2].set_title(&#39;Number AB Molecules&#39;) axs[2].set_xlabel(&quot;Time&quot;) axs[2].set_ylim((0,35)) axs[2].set_xlim((0,0.125)) # Plot each simulated trajectory for i in range(cycles): axs[0].plot(T[i,:], N_A[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[1].plot(T[i,:], N_B[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[2].plot(T[i,:], N_AB[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) plt.show() . In these plots we can see the various trajectories along with their average. If we increase the number of molecules and the number of trajectories we can get a &quot;smoother&quot; plot. Since we have the full evolution of the system we can also look at some other statistics, for example let&#39;s suppose we are interested in the distribution in the number of molecules of each type at time 0.5. We can also plot this using our samples: . time = 0.025 N_A_time = np.zeros(cycles) N_B_time = np.zeros(cycles) N_AB_time = np.zeros(cycles) for i in range(cycles): for j in range(1, steps): if T[i,j] =&gt; time and T[i,j-1] &lt; time: N_A_time[i] = N_A[i,j] N_B_time[i] = N_B[i,j] N_AB_time[i] = N_AB[i,j] # If trajectory doesn&#39;t span far enough take latest observation if T[i, steps] &lt; time: N_A_time[i] = N_A[i, steps] N_B_time[i] = N_B[i, steps] N_AB_time[i] = N_AB[i, steps] plt.hist(N_A_time, density=True, bins=np.arange(35), label=&quot;A&quot;, color=&#39;lightgrey&#39;) plt.hist(N_B_time, density=True, bins=np.arange(35), label=&quot;B&quot;, color=&#39;dimgrey&#39;) plt.hist(N_AB_time, density=True, bins=np.arange(35), label=&quot;AB&quot;, color=&#39;red&#39;) plt.legend() plt.show() . If instead of a system of 2 reactions we instead wanted to look a system of a large number of reactions we could modify the method above quite simply. Instead of the calculation of $R$ (overall reaction rate) consisting of 2 terms it will consist of a larger number of terms depending on the nature of the individual reactions. The probability of selecting a particular reaction type would then equally be in proportion to their contribution to $R$. . We can also notice that there is nothing &quot;special&quot; about the method that means it only applies to reaction kinetics. For example: the example code above could equally be a &quot;marriage and divorce model&quot; for heterosexual couples: A representing women and B representing men, AB representing a marriage. Through defining the &quot;reactions&quot; slightly differently it doesn&#39;t take much modification to turn this into a infection model: for example there could be 3 states: susceptible to infection, infected and recovered (potentially with immunity) with transition rates between each of these states. . We can see then that the Gillespie algorithm is very flexible and allows us to model stochastic systems that may otherwise be mathematically intractable. Through the nature of the modelling procedure we can sample from the system exactly (upto the precision of floating point numbers within our computers!) . There is a downside to exact simulation however: it can be very slow! In the example above the speed isn&#39;t really an issue since the system is so simple. However if we were modelling many different reaction types (say the order of 100s) then to allow for adequate samples we will need to run many trajectories, this can quickly spiral into a very slow running code! Thankfully however the method has been adapted in many ways to combat this issue. . Hybrid-Gillespie . We can note that calculating deterministic results from an ODE is (much) quicker than implementing the Gillespie simulation algorithm since there is no random element. However we notice that we do not have to model every reaction type using the same Gillespie approach. For example suppose we have one reaction type that is much slower than the others, say the order of 10 times slower. We could model this reaction via a determinstic ODE approach and simply rely on Gillespie for the more rapidly changing dynamics. Of course this is not applicable in every situation - as with any modelling or approximation used we should be sure that it is applicable to the situation at hand. For brevity we will not code an example of this here but it should be easy enough to modify the code above (for example by adding that molecule $A$ can &quot;disappear&quot; from the system with a rate 1/10 times the rate of the backward reaction). . Tau Leaping . Tau leaping modifies the Gillespie methodology above, it sacrifices exact simulation in favour of an approximate simulation that is quicker to compute. The main idea behind tau-leaping is also intuitive: instead of modelling time to the next event we &quot;jump&quot; forward in time and then compute how many reactions we would expect to see within that time frame and updating the population amounts in one step. By updating the population amounts in one go we should be able to compute much faster. It should be clear that this is an approximation to the Gillespie algorithm. The size of the &quot;leaps&quot; determines how efficient the method is and how accurate the approximation is. If we make very large steps we can model many reactions per step which speeds up the implementation, however the simulation will also be less accurate since the populations will be updated less frequently. Conversely a very small leap size will mean many leaps will not see a reaction and so the algorithm will run more slowly, however this should result in dynamics very close to the Gillespie method. Often choosing the leap size requuires some trial and error. . we can write pseudo-code for the tau-leaping process as: . Initialize - Set initial conditions for the system and set leaping size | Calculate event rates - for each event types depending on state of the system | Monte-Carlo - for each event type sample number of events occuring within the leap | Update - Update system state based on number of events | Repeat - Repeat steps 2-4 until some stopping criteria is met | Recall: in the example above we used an exponential waiting time between reactions. This means the reactions occur as a poisson process - as a result the number of reactions occuring within a given timeframe will follow a poisson distribution. We also have to be careful to not allow a negative population (at least in the example presented - in other systems this may be reasonable). . We can modify our example above to use Tau-leaping as: . # An implenetation of the Gillespie algorithm # with tau leaping # Applied to a pair of reactions: # A + B -&gt; AB # AB -&gt; A + B import numpy as np import matplotlib.pyplot as plt from scipy.stats import poisson %matplotlib inline # Fix random seed for repeatability np.random.seed(123) ###### Fix model parameters ###### N_A0 = 25 # Initial number of A molecules N_B0 = 35 # Initial number of B molecules N_AB0 = 5 # Initial number of AB molecules rf = 2 # Forward reaction rate rb = 1 # Backwards reaction rate leap = 0.005 # Size of leaping steps steps = 25 # Number of leaps per trajectory cycles = 100 # Number of trajectories iterated over # Set up holder arrays T = np.arange(steps+1)*leap N_A = np.zeros((cycles, steps+1)) N_B = np.zeros((cycles, steps+1)) N_AB = np.zeros((cycles, steps+1)) # Store initial conditions N_A[:,0] = N_A0 N_B[:,0] = N_B0 N_AB[:,0] = N_AB0 ###### Main Code Loop ###### for i in range(cycles): for j in range(steps): # Calculate updated reaction rates Rf = rf * N_A[i,j] * N_B[i,j] Rb = rb * N_AB[i,j] # Calculate number of reactions by type uf = np.random.random() ub = np.random.random() Nf = poisson.ppf(uf, Rf*leap) Nb = poisson.ppf(ub, Rb*leap) # Apply limits to prevent negative population Limitf = min(N_A[i,j], N_B[i,j]) Limitb = N_AB[i,j] Nf = min(Nf, Limitf) Nb = min(Nb, Limitb) # Update populations N_A[i,j+1] = N_A[i,j] + Nb - Nf N_B[i,j+1] = N_B[i,j] + Nb - Nf N_AB[i,j+1] = N_AB[i,j] + Nf - Nb # Calculate average arrays N_A_ave = N_A.mean(axis=0) N_B_ave = N_B.mean(axis=0) N_AB_ave = N_AB.mean(axis=0) ###### Plot Trajectories ###### fig, axs = plt.subplots(3, 1, figsize=(10,20)) # Plot average trajectories axs[0].plot(T, N_A_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[0].set_title(&#39;Number A Molecules&#39;) axs[0].set_ylim((0,35)) axs[0].set_xlim((0,0.125)) axs[1].plot(T, N_B_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[1].set_title(&#39;Number B Molecules&#39;) axs[1].set_ylim((0,35)) axs[1].set_xlim((0,0.125)) axs[2].plot(T, N_AB_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[2].set_title(&#39;Number AB Molecules&#39;) axs[2].set_xlabel(&quot;Time&quot;) axs[2].set_ylim((0,35)) axs[2].set_xlim((0,0.125)) # Plot each simulated trajectory for i in range(cycles): axs[0].plot(T[:], N_A[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[1].plot(T[:], N_B[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[2].plot(T[:], N_AB[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) plt.show() . We can see here that even though the trajectories from tau-leaping are less exact the procedure has produced smoother average results for the same number of simulation steps (approximately the same running time). . And again we can look at the distribution at time=0.025: . time = 0.025 N_A_time = np.zeros(cycles) N_B_time = np.zeros(cycles) N_AB_time = np.zeros(cycles) for i in range(cycles): for j in range(1, steps): if T[j] &gt;= time and T[j-1] &lt; time: N_A_time[i] = N_A[i,j] N_B_time[i] = N_B[i,j] N_AB_time[i] = N_AB[i,j] # If trajectory doesn&#39;t span far enough take latest observation if T[i, steps] &lt; time: N_A_time[i] = N_A[i, steps] N_B_time[i] = N_B[i, steps] N_AB_time[i] = N_AB[i, steps] plt.hist(N_A_time, density=True, bins=np.arange(35), label=&quot;A&quot;, color=&#39;lightgrey&#39;) plt.hist(N_B_time, density=True, bins=np.arange(35), label=&quot;B&quot;, color=&#39;dimgrey&#39;) plt.hist(N_AB_time, density=True, bins=np.arange(35), label=&quot;AB&quot;, color=&#39;red&#39;) plt.legend() plt.show() . Here we can see improved distributions with (what appears to be) less noise. To justify this we would want to run more tests however. . Note: this is the most basic implementation of the tau-leaping procedure. In certain situations this needs to be manipulated to improve behaviour, for example if the poisson draw is often large enough to cause the population to go negative then a truncation procedure (or acceptance/rejection scheme) needs to be employed in such a way as to retain the average reaction rates. In this simple example we ignore this complication, there are some occasions where the number of $A$ molecules hits zero so there will be some bias in the estimates presented above. . Adaptive Tau-Leaping . The &quot;problem&quot; with the tau-leaping method above is that it is very sensitive to the leap size. It is also possible that as the system evolves what started out as a &quot;good&quot; leap size becomes &quot;bad&quot; as the dynamics change. One possible solution to this is to use an &quot;adaptive&quot; method whereby the leap size varies depending on the dynamics. The main idea is to limit the leap sizes from being so large that the populations can reach an unfavourable state (e.g. negative population sizes) or jump to a state &quot;too far away&quot;. . There are many ways to do this, one of the more popular was developed by Y. Cao and D. Gillespie in 2006. In order to describe the method we will need to introduce some notation. We let $ mathbf{X}_t = left( X_t^i right)_{i=1}^N$ to be a vector of population sizes at time t. We intorduce variables $v_{ij}$ to represent the change in component $i$ of the population when an event $j$ occurs - we will use $i$ indices to refer to components of the population vector and $j$ indices to refer to event types. $R_j( mathbf{X}_t)$ is the rate of event $j$ with population $ mathbf{X}_t$. In this method we look to bound the relative shift in rates at each step by a parameter $ epsilon$. . In pseudo-code we can describe the process via: . Initialize - Set initial conditions for the population | Calculate event rates - $R_j$ for each event types depending on state of the system | Calculate auxiliary variables - for each state component $i$ begin{align} mu_i &amp;= sum_j v_{ij} R_j sigma_j^2 &amp;= sum_j v_{ij}^2 R_j end{align} | Select highest order event - for each state component $i$, denote the rate of this event as $g_i$ | Calculate time step $$ tau = min_i left( min left( frac{max left( frac{ epsilon X_i}{g_i}, 1 right)}{| mu_i|} , frac{max left( frac{ epsilon X_i}{g_i}, 1 right)^2}{ sigma_j^2} right) right) $$ | Monte-Carlo - for each event type sample number of events occuring within the leap step $ tau$ | Update - Update system state based on number of events | Repeat - Repeat steps 2-7 until some stopping criteria is met | Step 4. involves selecting the highest order event - this essentially is the &quot;most important&quot; event that each $i$ is involved in. For very complex systems this may not be an obvious thing to do and will require more finesse. We can see that aside from steps 3-5 this is the exact same scheme as the previous example. . There are other adaptive leaping schemes that one could use each with different pros and cons. . We can modify the code above to use this scheme via: . # An implenetation of the Gillespie algorithm # With adaptive tau-leaping # Applied to a pair of reactions: # A + B -&gt; AB # AB -&gt; A + B import numpy as np import matplotlib.pyplot as plt from scipy.stats import poisson %matplotlib inline # Fix random seed for repeatability np.random.seed(123) ###### Fix model parameters ###### N_A0 = 25 # Initial number of A molecules N_B0 = 35 # Initial number of B molecules N_AB0 = 5 # Initial number of AB molecules rf = 2 # Forward reaction rate rb = 1 # Backwards reaction rate eps = 0.03 # Epsilon adaptive rate steps = 25 # Number of reactions per trajectory cycles = 100 # Number of trajectories iterated over # Set up holder arrays T = np.zeros((cycles, steps+1)) N_A = np.zeros((cycles, steps+1)) N_B = np.zeros((cycles, steps+1)) N_AB = np.zeros((cycles, steps+1)) # Store initial conditions N_A[:,0] = N_A0 N_B[:,0] = N_B0 N_AB[:,0] = N_AB0 ###### Main Code Loop ###### for i in range(cycles): for j in range(steps): # Calculate updated reaction rates Rf = rf * N_A[i,j] * N_B[i,j] Rb = rb * N_AB[i,j] # Calculate auxiliary variables mu_A = Rf - Rb mu_B = Rf - Rb mu_AB = Rb - Rf sig2_A = Rf + Rb sig2_B = Rf + Rb sig2_AB = Rf + Rb # Select highest order reactions g_A = Rf g_B = Rf g_AB = Rb # Caclulate internal maxima - taking care of divide by zero if g_A == 0: max_A = 1 else: max_A = max(eps*N_A[i,j]/g_A,1) if g_B == 0: max_B = 1 else: max_B = max(eps*N_B[i,j]/g_B, 1) if g_AB == 0: max_AB = 1 else: max_AB = max(eps*N_AB[i,j]/g_AB, 1) # Calculate minima for each component min_A = min(max_A / abs(mu_A), max_A**2 / sig2_A) min_B = min(max_B / abs(mu_B), max_B**2 / sig2_B) min_AB = min(max_AB / abs(mu_AB), max_AB**2 / sig2_AB) # Select tau leap size leap = min(min_A, min_B, min_AB) # Calculate number of reactions by type uf = np.random.random() ub = np.random.random() Nf = poisson.ppf(uf, Rf*leap) Nb = poisson.ppf(ub, Rb*leap) # Apply limits to prevent negative population Limitf = min(N_A[i,j], N_B[i,j]) Limitb = N_AB[i,j] Nf = min(Nf, Limitf) Nb = min(Nb, Limitb) # Update populations and times N_A[i,j+1] = N_A[i,j] + Nb - Nf N_B[i,j+1] = N_B[i,j] + Nb - Nf N_AB[i,j+1] = N_AB[i,j] + Nf - Nb T[i,j+1] = T[i,j] + leap # Calculate an average trajectory plot ave_steps = 100 T_max = T.max() # Set up average array holders T_ave = np.linspace(0,T_max,ave_steps+1) N_A_ave = np.zeros(ave_steps+1) N_B_ave = np.zeros(ave_steps+1) N_AB_ave = np.zeros(ave_steps+1) N_A_ave[0] = N_A0 N_B_ave[0] = N_B0 N_AB_ave[0] = N_AB0 # Pass over average array entries for i in range(1, ave_steps+1): tmax = T_ave[i] A_sum = 0 B_sum = 0 AB_sum = 0 t_count = 0 # Pass over each trajectory and step therein for j in range(cycles): for k in range(steps): if T[j,k] &lt;= tmax and T[j,k+1] &gt; tmax: t_count += 1 A_sum += N_A[j,k] B_sum += N_B[j,k] AB_sum += N_AB[j,k] # Caclulate average - taking care if no samples observed if t_count == 0: N_A_ave[i] = N_A_ave[i-1] N_B_ave[i] = N_B_ave[i-1] N_AB_ave[i] = N_AB_ave[i-1] else: N_A_ave[i] = A_sum / t_count N_B_ave[i] = B_sum / t_count N_AB_ave[i] = AB_sum / t_count ###### Plot Trajectories ###### fig, axs = plt.subplots(3, 1, figsize=(10,20)) axs[0].plot(T_ave, N_A_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[0].set_title(&#39;Number A Molecules&#39;) axs[0].set_ylim((0,35)) axs[0].set_xlim((0,0.125)) axs[1].plot(T_ave, N_B_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[1].set_title(&#39;Number B Molecules&#39;) axs[1].set_ylim((0,35)) axs[1].set_xlim((0,0.125)) axs[2].plot(T_ave, N_AB_ave, marker=&#39;&#39;, color=&#39;red&#39;, linewidth=1.9, alpha=0.9) axs[2].set_title(&#39;Number AB Molecules&#39;) axs[2].set_xlabel(&quot;Time&quot;) axs[2].set_ylim((0,35)) axs[2].set_xlim((0,0.125)) for i in range(cycles): axs[0].plot(T[i,:], N_A[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[1].plot(T[i,:], N_B[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) axs[2].plot(T[i,:], N_AB[i,:], marker=&#39;&#39;, color=&#39;grey&#39;, linewidth=0.6, alpha=0.3) plt.show() . As with the previous tau-leaping algorithm there the trajectories are noticably less exact than the original Gillespie formulation. However owing to the variable time step the trajectories do appear slightly less granular than in the previous tau-leaping formulation. Again the average trajectory is smoother than in the original method for (approximately) the same amount of run-time. . Looking at the time=0.025 distributions once again: . time = 0.025 N_A_time = np.zeros(cycles) N_B_time = np.zeros(cycles) N_AB_time = np.zeros(cycles) for i in range(cycles): for j in range(1, steps+1): if T[i,j] &gt;= time and T[i,j-1] &lt; time: N_A_time[i] = N_A[i,j] N_B_time[i] = N_B[i,j] N_AB_time[i] = N_AB[i,j] if T[i, steps] &lt; time: N_A_time[i] = N_A[i, steps] N_B_time[i] = N_B[i, steps] N_AB_time[i] = N_AB[i, steps] plt.hist(N_A_time, density=True, bins=np.arange(35), label=&quot;A&quot;, color=&#39;lightgrey&#39;) plt.hist(N_B_time, density=True, bins=np.arange(35), label=&quot;B&quot;, color=&#39;dimgrey&#39;) plt.hist(N_AB_time, density=True, bins=np.arange(35), label=&quot;AB&quot;, color=&#39;red&#39;) plt.legend() plt.show() . Again the distributions for a fixed time appear to have become less noisy. . In a small scale simple example such as this we would expect any &quot;improvements&quot; from a scheme like this to be minor, as we run more complicated examples we would expect a bigger performance differential. . Conclusion . In this blog post we have seen 3 variations of the Gillespie algorithm: the original, tau-leaping and an adaptive tau leaping scheme. We have seen that the original variation produces exact simulations of a specified system, via tau leaping we have seen that we can approximate this and still get reasonable results in a quicker time. Which is important when dealing with more complicated and larger systems. . At this point we should also see the flexibility inherent in the Gillespie framework and why it has been applied in many different areas. We can also see that the algorithm is a &quot;gateway&quot; into agent-based schemes - instead of using a purely stochastic mechanism for selecting reaction types/times we could (for example) model individual molecules moving around in space and if they come within a certain radius of each other at a certain speed then a reaction occurs. This would turn the Gillespie algorithm into a full agent-based model for reaction kinetics (the benefit of doing this in most situations is likely slim to none however). .",
            "url": "https://www.lewiscoleblog.com/gillespie-algorithm",
            "relUrl": "/gillespie-algorithm",
            "date": " â€¢ Apr 14, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Flock vs Predator",
            "content": "If you have ever looked up at the sky in early evening you might have been greeted by the spectacle of many birds flying in unison creating elaborate and complex patterns in the sky. Or perhaps on the discovery channel you have seen a documentary on the sea and have seen schools of fish creating similar elaborate patterns. . . There are many evolutionary reasons for why these patterns may form: firstly is the &quot;safety in numbers&quot; concept whereby having many individuals grouped together protects against predators. Predators can only attack the edge of the pattern, the number of individuals on the edges is significantly fewer than the individuals in the middle of the pattern - thus increasing safety for the largest number of individuals. The patterns may also trick predators into thinking the pattern is some larger threatening entity and so less likely to attack in the first place. There is also the possiblity that by creating a group pattern that is large enough to be seen from far away it attracts more individuals to join the group. . This phenomena is known as &quot;flocking&quot; - in this blog post we will investigate how flocks form. . An Agent Based Approach . One way in which we could reverse engineer these patterns is to have a &quot;central controller&quot; that tells where each flock member should be located at each moment in time. While this may produce results that are seemingly indistinguishable from real-world flocking behaviour we would be very hard pressed to argue that this is what happens in nature. Instead we would like each member to have some level of autonomy and a simple limited set of rules that when combined produces the desired result - if such a model can be found it will allow us to analyse and study flock formation in more detail. Any model containing many members each following simple rules within an environment can be classified as an &quot;agent based model&quot; (members herein being referred to as agents). Agent based models span further than flocking models, they can be used anywhere where it&#39;s possible to define simple rules for local behaviour and where large scale/population models (e.g. differential equations) do not provide details on the scales that are of interest to us. Agent based models have been used to model a whole manner of areas from biologically inspired predator/prey interactions, spreading of infection through to the social sciences and idea spreading and the economy. . Model Design . Flocking models have been studied fairly extensively. In this blog I will present my implementation of a flocking model, there will be &quot;prey&quot; agents who are large in number and form flocks to try and protect themselves. There will be a small number of &quot;predator&quot; agents who try and eat/kill the prey. Each class of agent has their own dynamic rules for how they move in space. . Rules for Prey . For the prey agents we will assume that there are 5 forces operating to direct the agents. These forces are equivalent to acceleration vectors, we will integrate over time to change the velocity and integrate velocity to get the change in position. We can describe the forces at bay as follows: . Flocking Forces - This represents the attraction of an agent to move towards a density of other agents (join the flock). To do this we consider the velocity vector of agents which we denote $ mathbf{v_i}$. We assume that we will only flock towards others if they are near us, we thus have a &quot;flocking radius&quot; which we denote $r_f$ which is how far the agent can &quot;see&quot;. We define a vector quantity: $ mathbf{V_i} = sum_{j in N_{r_f}(i)} mathbf{v_j} $. Where we denote the set of agents within a neighbourhood of radius $r$ of agent $i$ as: $N_{r}(i)$. The strength of this force is defined by a parameter $ alpha$. We then define the flocking force applied to agent $i$ as: $$ mathbf{F}_{flock}^i = alpha frac{ mathbf{V_i}}{| mathbf{V_i}|}$$ | Repulsive Forces - If we are in a crowd we like our own space, if somebody gets to close we move away. At the most extreme level there is a limit on how close one can get to another owing to the size of their bodies. The repulsive force acts to stop agents getting too close together. This force operates only on a radius $r_0$ - if somebody is very far away they won&#39;t impact how we move in space. We denote $| mathbf{d_{ij}}|$ to be the scalar distance (magnitude) between agents $i$ and $j$. We use boldface $ mathbf{d_{ij}}$ to represent the vector distance. The strength of this force is defined by the parameter $ epsilon$. We can then express this force applied to agent $i$ as: $$ mathbf{F}_{rep}^i = epsilon sum_{j in N_{r_0}(i)} left(1 - frac{| mathbf{d_{ij}}|}{r_0} right)^{ frac{3}{2}} frac{ mathbf{d_{ij}}}{| mathbf{d_{ij}}|} $$ | Self-Propulsion Forces - This force represents a desire to &quot;keep moving&quot;. In some situations there is a tendency for sustained movement - for example in a march there is a desire to keep moving at a constant velocity. We represent this by the self-propulsion force. The strength of this force is controlled via a parameter $ mu$. For agent $i$ we can express this as: $$ mathbf{F}_{prop}^i = mu left(v_0 - | mathbf{v_i}| right) frac{ mathbf{v_i}}{| mathbf{v_i}|} $$ | Random Forces - In the model we may wish to allow for random forces, that is each agent has it&#39;s own autonomy. To do this we will just apply a uniform random draw. We will use a parameter $r_{amp}$ (random amplitude) to denote the strength of this force. | Predator-Repulsive Forces - One of the driving forces for any prey is to escape predators. In this model we will assume that this force trumps all others, that is if a predator is close an agent will run away and ignore all other desires of flocking/self-propulsion/etc. This operates in the exact same way as the repulsive force above but instead of repulsion from other agents it considers repulsion from predators. We define the radius over which this occurs as $r_p$ and the strength of this force $ delta$ - these replace $r_0$ and $ epsilon$ in the formula above. We also apply a &quot;speed limit&quot; to the agents to prevent them travelling arbitrarily quickly. | . Rules for Predators . For predators the dynamics are much simpler. We assume that a predator has infinite vision, it will look for the nearest prey and run towards it at a fixed speed. However we will add a (possibly small) random component to the movement - this could represent indecisiveness on the part of the predator or the evasive tactics employed by the prey. The only model assumptions for predators is the predator speed $pv_{max}$ and the proportion of movement that is random $pr_{amp}$. . Environment . We will assume a simple environment consisting of a unit-square. The boundaries will be periodic (i.e. the left edge will &quot;connect&quot; to the right edge and top to bottom). Like the game asteroids this assumes the dynamics take place on the surface of a torus. This assumption simplifies the dynamics since there is no need to consider any boundary conditions - if we assume a fixed &quot;wall&quot; around the environment we would need to define rules as to what happens when an agent tries to cross (e.g. do they &quot;reflect&quot; off walls? Do they &quot;stick&quot;? Do they slow down and change direction to avoid hitting the wall all together?). For convenience in calculating distances in the periodic boundary world we have a helper function called &quot;buffer&quot; which replicates any agents near the walls and places them outside the unit-square - this ensures that distances are calculated correctly. . Other Model Assumptions . There are a few other modelling assumptions made, one of the major decisions of any agent based model is deciding whether to use synchronous or asynchronous updating. We will assume synchronous updating here, each agent will make their movements at the same time. This avoids the problem of agents who are updated &quot;later&quot; within a tick of an asynchronous scheme gaining an &quot;advantage&quot; over those updated first (this would be a particular issue for predator-prey relations where if a predator waits until the prey has moved it is more likely to catch the prey!) . To avoid the problem of a predator following a single prey-agent aimlessly we will use an &quot;eat&quot; dynamic. Here if a predator gets within a small distance of a prey it will &quot;eat&quot; it, the prey will disappear and instantly re-spawn to a random location in the environment with a random starting velocity. There will be no limit to how many prey a predator can eat in a single step and it is assumed the predator can eat without stopping/slowing down. . The Code . An implementation of this model can be seen below. I use matplotlib&#39;s animaton feature to make an animation of the system through time. In the animation prey are represented by small blue crosses and predators by larger red diamonds. . import numpy as np from matplotlib import animation import matplotlib.pyplot as plt from IPython.display import HTML %matplotlib inline ###### Model Parameters ###### dt = 0.01 # Time step num_frames = 250 # Number of animated frames burn = 1000 # Number of burn in iterations # Flock Parameters N = 250 # Number of flocking agents r0 = 0.1 # Repulsion force range eps = 2.0 # Repulsion force strength rf = 0.25 # Flocking force range alpha = 10.0 # Flocking force strength v0 = 0.25 # Target speed vmax = 0.75 # Maximum speed mu = 0.1 # Self-Propulsion force ramp = 1.0 # Random force rp = 0.5 # Predator-repulsion force range delta = 5.0 # Predator-repulsion force strength # Predator Parameters M = 3 # Number of predator agents pv_max = 1.0 # Predator speed pramp = 0.25 # Predator random force eat = 0.025 # Eating radius ###### Fix Random Seed ###### seed = 123 np.random.seed(seed) ###### Helper Functions ###### def buffer(rb, x, y, vx, vy): &quot;&quot;&quot; buffer(rb, x, y, vx, vy) Takes copies of agents near the edges of the unit square and extends outwards by a buffer margin rb. This makes it easier to calculate distances between agents for flocking and repulsive forces Inputs: rb - buffer range x - array of x coordinate positions [0,1] y - array of y coordinate positions [0,1] vx - array of x-component of velocity vy - array of y-component of velocity Outputs: nb, xb, yb, vxb, vyb - Buffered copies of inputs &quot;&quot;&quot; _N = x.shape[0] # Initialize buffer arrays xb[0:_N] = x yb[0:_N] = y vxb[0:_N] = vx vyb[0:_N] = vy # nb is a counter already have _N agents nb = _N-1 for i in range(nb+1): if x[i] &lt;= rb: # left edge nb+=1 xb[nb] = x[i]+1 yb[nb] = y[i] vxb[nb] = vx[i] vyb[nb] = vy[i] if x[i] &gt;= 1 - rb: # right edge nb+=1 xb[nb] = x[i]-1 yb[nb] = y[i] vxb[nb] = vx[i] vyb[nb] = vy[i] if y[i] &lt;= rb: # bottom edge nb+=1 xb[nb] = x[i] yb[nb] = y[i]+1 vxb[nb] = vx[i] vyb[nb] = vy[i] if y[i] &gt;= 1 - rb: # top edge nb+=1 xb[nb] = x[i] yb[nb] = y[i]-1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &lt;= rb) and (y[i] &lt;= rb): # bottom left corner nb+=1 xb[nb] = x[i]+1 yb[nb] = y[i]+1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &lt;= rb) and (y[i] &gt;= 1 - rb): # top left corner nb+=1 xb[nb] = x[i]+1 yb[nb] = y[i]-1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &gt;= 1 - rb) and (y[i] &lt;= rb): # bottom right corner nb+=1 xb[nb] = x[i]-1 yb[nb] = y[i]+1 vxb[nb] = vx[i] vyb[nb] = vy[i] if (x[i] &gt;= 1 - rb) and (y[i] &gt;= 1 - rb): # top right corner nb+=1 xb[nb] = x[i]-1 yb[nb] = y[i]-1 vxb[nb] = vx[i] vyb[nb] = vy[i] return nb, xb, yb, vxb, vyb def force(nb, xb, yb, vxb, vyb, x, y, vx, vy, mb, pxb, pyb): &quot;&quot;&quot; force(nb, xb, yb, vxb, vyb, x, y, vx, vy, mb, pxb, pyb) Calculate force applied to agents (x,y) divided into: - flocking force (flockx, flocky) - repulsive force (repx, repy) - Self propulsion (fpropx, fpropy) - Random force (frandx, frandy) - Predator repulsive force (fpredx, fpredy) Inputs: nb - number of buffered agents xb - buffered agent x position array yb - buffered agent y position array vxb - buffered agent velocity x coordinate array vyb - buffered agent velocity y coordinate array x - agent x position array y - agent y position array vx - agent x coordinate velocity array vy - agent y coordinate velocity array mb - number buffered predator agents pxb - predator x-coordinate position array pyb - predator y-coordinate position array Global Variables Called: rf - Flocking force range r0 - Repulsion force range alpha - Flocking force strength mu - Self propulsion force v0 - Target velocity ramp - Random force strength rp - Predator repulsion force range Outputs: fx, fy - x and y coordinate forces &quot;&quot;&quot; _N = x.shape[0] for i in range(_N): repx = 0 repy = 0 flockx = 0 flocky = 0 nflock = 0 fpredx = 0 fpredy = 0 for j in range(nb): d2 = (xb[j] - x[i])**2 + (yb[j] - y[i])**2 if (d2 &lt;= rf**2) and (i != j): flockx += vxb[j] flocky += vyb[j] nflock += 1 if d2 &lt;= r0**2: d = np.sqrt(d2) repx += eps*(1- d / r0)**1.5 * (x[i] - xb[j]) / d repy += eps*(1- d / r0)**1.5 * (y[i] - yb[j]) / d normflock = np.sqrt(flockx**2 + flocky**2) if nflock == 0: normflock = 1 flockx = alpha * flockx / normflock flocky = alpha * flocky / normflock vnorm = np.sqrt(vx[i]**2 + vy[i]**2) fpropx = mu * (v0 - vnorm) * (vx[i] / vnorm) fpropy = mu * (v0 - vnorm) * (vy[i] / vnorm) frandx = ramp * (2*np.random.random() - 1) frandy = ramp * (2*np.random.random() - 1) fx[i] = (flockx + frandx + fpropx + repx) fy[i] = (flocky + frandy + fpropy + repy) for k in range(mb): d2 = (pxb[k] - x[i])**2 + (pyb[k] - y[i])**2 if (d2 &lt;= rp**2): d = np.sqrt(d2) fpredx += delta*(1- d / rp)**1.5 * (x[i] - pxb[k]) / d fpredy += delta*(1- d / rp)**1.5 * (y[i] - pyb[k]) / d if fpredx**2 + fpredy**2 &gt; 0: fx[i] = fpredx fy[i] = fpredy return fx, fy def pred_dist(px1, py1, x1, y1): &quot;&quot;&quot; pred_dist(px1, py1, x1, y1) Returns an array of distances from predator to an array of prey Inputs: px1 - predator x coordinate py1 - predator y coordinate x1 - array of prey x coordinates y1 - array of prey y coordinates Outputs: distance array &quot;&quot;&quot; dx1 = np.abs(px1-x1) dx2 = np.abs(1+px1-x1) dx3 = np.abs(px1-(1+x1)) dy1 = np.abs(py1-y1) dy2 = np.abs(1+py1-y1) dy3 = np.abs(py1-(1+y1)) dx = np.minimum(np.minimum(dx1, dx2), dx3) dy = np.minimum(np.minimum(dy1, dy2), dy3) return np.sqrt(dx**2+dy**2) def direction(start_x, start_y, target_x, target_y): &quot;&quot;&quot; direction(start_x, start_y, target_x, target_y) Find a unit direction vector from a starting point to a target Inputs: start_x - starting point x coordinate start_y - starting point y coordinate target_x - target point x coordinate target_y - target point y coordinate Returns dir_x - direction in x dir_y - direction in y &quot;&quot;&quot; dx1 = target_x - start_x dx2 = target_x - (start_x + 1) dx3 = target_x - (start_x - 1) dy1 = target_y - start_y dy2 = target_y - (start_y + 1) dy3 = target_y - (start_y - 1) if abs(dx1) &lt;= (abs(dx2) and abs(dx3)): dir_x = dx1 elif abs(dx2) &lt;= (abs(dx1) and abs(dx3)): dir_x = dx2 elif abs(dx3) &lt;= (abs(dx1) and abs(dx2)): dir_x = dx3 if abs(dy1) &lt;= (abs(dy2) and abs(dy3)): dir_y = dy1 elif abs(dy2) &lt;= (abs(dy1) and abs(dy3)): dir_y = dy2 elif abs(dy3) &lt;= (abs(dy1) and abs(dy2)): dir_y = dy3 return dir_x, dir_y def unitize(vec_x, vec_y): &quot;&quot;&quot; unitize(vec_x, vec_y) Returns a vector of same direction as input but with unit size &quot;&quot;&quot; norm = np.sqrt(vec_x**2 + vec_y**2) return vec_x/norm, vec_y/norm def update(): &quot;&quot;&quot; update() Update function that updates position and velocities of prey and predator agents by one time step Inputs: None Outputs: None (updates values of global variables) &quot;&quot;&quot; global x global y global vx global vy global px global py global vpx global vpy global eat_count x_old = x.copy() y_old = y.copy() nb, xb, yb, vxb, vyb = buffer(max(r0,rf), x, y, vx, vy) mb, pxb, pyb, vpxb, vpyb = buffer(rp, px, py, vpx, vpy) fx, fy = force(nb, xb, xb, vxb, vyb, x, y, vx, vy, mb, pxb, pyb) # Use force to calculate speeds vx += fx * dt vy += fy * dt # Apply speed limit of maximum velocity vmod = np.sqrt(vx**2 + vy**2) vmult = np.where(vmod &lt; vmax, vmod, vmax) / vmod vx *= vmult vy *= vmult # Calculate new positions and re-position in unit square x += vx * dt y += vy * dt x = (1 + x) % 1 y = (1 + y) % 1 # Predator calculation for i in range(M): pred_x = px[i] pred_y = py[i] # Calculate distances to prey dist = pred_dist(pred_x, pred_y, x_old, y_old) # If prey within eat range then eat and regenerate prey for j in range(N): if dist[j] &lt; eat: x[j] = np.random.random() y[j] = np.random.random() vx[j] = 2*np.random.random() - 1 vy[j] = 2*np.random.random() - 1 eat_count += 1 # Find closest prey min_dist = np.min(dist) mask = dist == min_dist prey_x = x_old[mask][0] prey_y = y_old[mask][0] # Calculate direction to move in vpx_t, vpy_t = direction(pred_x, pred_y, prey_x, prey_y) vpx_t, vpy_t = unitize(vpx_t, vpy_t) # Simulate random component and normalize vpx_rand = 2*np.random.random() - 1 vpy_rand = 2*np.random.random() - 1 vpx_rand, vpy_rand = unitize(vpx_rand, vpy_rand) # Combine prey-direction and random components vpx[i] = (1-pramp)*vpx_t + pramp*vpx_rand vpy[i] = (1-pramp)*vpy_t + pramp*vpy_rand vpx[i], vpy[i] = unitize(vpx[i], vpy[i]) # Scale to maximum velocity vpx[i] *= pv_max vpy[i] *= pv_max # Move predator forward px[i] += vpx[i]*dt py[i] += vpy[i]*dt # Re-position predators to unit square px = (1 + px) % 1 py = (1 + py) % 1 ###### Main Code ###### ###### Set Up Arrays ###### # Flock arrays x = np.zeros(N) y = np.zeros(N) vx = np.zeros(N) vy = np.zeros(N) fx = np.zeros(N) fy = np.zeros(N) # Flock buffer arrays xb = np.zeros([4*N]) yb = np.zeros([4*N]) vxb = np.zeros([4*N]) vyb = np.zeros([4*N]) # Predator arrays px = np.zeros(M) py = np.zeros(M) vpx = np.zeros(M) vpy = np.zeros(M) # Predator buffer arrays pxb = np.zeros([4*M]) pyb = np.zeros([4*M]) vpxb = np.zeros([4*M]) vpyb = np.zeros([4*M]) # Eat counter initialization eat_count = 0 # Initialize positions and velocities for i in range(N): x[i] = np.random.random() y[i] = np.random.random() vx[i] = 2*np.random.random() - 1 vy[i] = 2*np.random.random() - 1 for i in range(M): px[i] = np.random.random() py[i] = np.random.random() vpx[i] = 2*np.random.random() - 1 vpy[i] = 2*np.random.random() - 1 # Burn simulations to improve initial conditions for i in range(burn): update() # Reset Eat count eat_count = 0 ###### Animate ###### # Initialize figure figure = plt.figure() axes = plt.axes(xlim=(0, 1), ylim=(0, 1)) axes.set_xticks([], []) axes.set_yticks([], []) scatter_prey = axes.scatter(x, y, marker=&#39;x&#39;, s=25, c=&#39;Blue&#39;) scatter_pred = axes.scatter(px, py, marker=&#39;D&#39;, s=100, c=&#39;Red&#39;) # Define animation step def animate(frame): update() prey_data = np.array((x, y)).T scatter_prey.set_offsets(prey_data) pred_data = np.array((px, py)).T scatter_pred.set_offsets(pred_data) # Display animation function def display_animation(anim): # Close initialized static plot plt.close(anim._fig) # Returns animation to HTML return HTML(anim.to_jshtml()) # Set up FuncAnimation anim = animation.FuncAnimation(figure, animate, frames=num_frames, interval=50) # Call the display function display_animation(anim) . &lt;/input&gt; Once Loop Reflect Since it&#39;s a bit hard to follow all that is going on with regards to eating prey, in this animation we can track the number of prey eaten: . print(&quot;Number of eaten prey per predator per frame:&quot;, eat_count / M / num_frames) . Number of eaten prey per predator per frame: 0.05466666666666666 . As the dynamics evolve we can see flocks form, these are occasionally interrupted as a predator &quot;breaks&quot; them apart. When this happens the predator can get &quot;confused&quot; and doesn&#39;t know which flock is the one it should follow - showing how flocking is beneficial to the prey agents! Through changing parameters we can observe a wide variety of behaviours owing to the relatively large parameter space. . These emergent behaviours are particularly interesting. It is, seemingly, impossible to predict how the dynamics will behave without running a simulation (or if it is predictable it is not apparently obvious). The model rules seem to exhibit a &quot;computational irreducibility&quot; - there is no &quot;short-cut&quot; to computing them. This concept is prevelant in the social sciences, we can study a lot about individuals yet have no way of extending these ideas to populations. As such many problems in complex systems (e.g. social sciences, some areas of biology, etc) are in some sense incompatible with the prevelant scientific principle of reductionism - the study of irreducible computation (and agent based modelling) will no doubt have a large impact on how we understand these systems in the future. . Suggested Extensions or Improvements . Part of the beauty of agent based models such as the one presented here is we can modify the code to investigate different behaviours. Some things that you may want to consider for yourselves include: . Study parameter space - The obvious first thing to do is study the parameters of this model. There are a large number of parameters and so a large variation in potential behaviour. In particular we would like to create a &quot;zoo&quot; of behaviours - a taxonomy of what is possible with the model and whether this captures what is observed in nature. We can also look for &quot;reasonable ranges&quot; of parameters that produce desired outputs. This could also include the creation of summary statistics describe the behaviours. | Improved predator dynamics - At the moment the predators are not very smart, they are incapable as working as a team. We may want to consider the effect of adding a &quot;repulsion force&quot; betwen predators so that they can cover the space more effectively. We could also allow for the predators to take the speed of prey into account, so a close fast moving prey may be less attractive than a slower prey slightly further away. The predators could &quot;learn&quot; too - perhaps after being well fed they can move more quickly or have a larger eating radius? Perhaps we want to have limited vision for predators? We could really go to town with this and implement a reinforcement learning algorithm to look for &quot;successful&quot; strategies for eating prey. | Non-static population size - The model so far assumes the populations stay the same. We may wish to allow for the prey to decrease as they are eaten. Perhaps the predators, when well fed, can reproduce and if they don&#39;t eat for a long time they die off. | Remove homogeneity - Each agent current has the same dynamics properties. We may wish to allow for some heterogeneity in the populations - some prey may be slower/have less good vision and so are easier targets. What if we create classes of prey/predator agents? Perhaps some are highly flock seeking while others prefer to be alone. | Environment - We have only looked at this model with periodic boundary conditions. What happens if there are &quot;walls&quot; at the edges so prey can get trapped? What happens if we put barriers/obstacles in the environment? | Different forces - We could also consider adding different forces or replacing the existing ones. For example we may not believe that agents are able to effectively calculate forces based on all their neighbours within a radius, instead perhaps we want to refactor the model so that the agents consider their N-nearest neighbours only instead. Perhaps we could imagine that there is a section of the environment where &quot;food&quot; for the prey exists so there is a force that draws them towards that area. Perhaps we want to implement a force that &quot;slows down&quot; the prey if they have been moving quickly (effectively the agents get &quot;tired&quot;). It&#39;s not hard to think of other possibilities. | New agent types - Suppose that instead of a simple predator-prey relation we have multiple species, some happily coexisting while others eat the other. The &quot;prey&quot; in the existing model could have an attractive force to a species higher up the food chain that eats their predator! We could also investigate &quot;rock paper scissors&quot; type behaviour where no one species is &quot;dominant&quot; by design. | Performance enhancements - The code as presented is reasonably efficient but could be made to run quicker. Replacing the current code with Cython/Numba would be the obvious choice but there are also opportunities for more structural changes that could improve efficiency. For example we could approximate the forces using a quadtree (as in the Barnes-Hut N-body algorithm). | . Conclusion . In this blog post we have seen how to implement a basic flocking algorithm with predators added to the environment. Although each agent in the model is &quot;dumb&quot;, complex behaviours of the population can emerge out of relatively simple rules. From this we can notice some interesting applications for these methods - for example we could model crowd behaviour in rock concerts/sports events to increase safety, or we could model evacuation procedures for large buildings/venues that could influence their design. We could also use these techniques for video games and CGI in movies where we may wish to generate large crowds that look &quot;natural&quot; and not repeated/regular. . Hopefully this post acts as a good motivation for the use of agent-based models in general and in particular how they can be used to study emergent behaviours in complex systems. .",
            "url": "https://www.lewiscoleblog.com/flocking-model",
            "relUrl": "/flocking-model",
            "date": " â€¢ Apr 7, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Spin Glass Models 5: Applications and Extensions",
            "content": ". This is the fifth blog post in a series - you can find the previous blog post here . . If you have read my previous few blog posts you will have some appreciation for spin-glasses and some models used to study and simulate them. In this blog post we will look a few applications and extensions of spin-glasses. . Potts Spin-Glass Model . The Potts spin-glass model is a generalization of the Ising model we looked at previously. Whereas the Ising model allows only &quot;up&quot; or &quot;down&quot; spins the Potts model is a &quot;q-spin&quot; model where spins can take any of $q$ possible values. The Hamiltonian is typically specified as: $$H = - sum_{x,y} J_{xy} delta( sigma_x sigma_y) - sum_x h_x sigma_x $$ Where $ delta(.)$ represents a Kronecker delta function. For $q=2$ this is equivalent to the Ising model. . Another representation of a Potts model is the vector Potts model, here spins take values as defined as angles: $$ theta_i = frac{2 pi i}{q} $$ For $i$ between 1 and $q$. The Hamiltonian can then be defined as: $$H = - sum_{x,y} J_{xy} cos( sigma_x - sigma_y) - sum_x h_x sigma_x $$ The case for $q to infty$ is called the XY-model. . On a 2d square lattice we find that a phase transition exists for $q&gt;4$ for $q leq 4$ there is a 2nd order phase transition (as with the Ising model $q=2$). The techniques and simulation procedures discussed previously can be adapted to the Potts model with minimal change. . Other generalizations of this model have been used in various physical and biological applications. The Potts model has also been used in image processing. . Markov-Random-Field . The Ising model can also be thought of as the simplest example of a Markov-Random-Field (MRF). The theory and application of these could easily take a blog post (or more) by itself. Instead we will just look at some basic definitions and the major theorems. . A Markov-Random-Field is an example of a probabilistic graphical model (PGM). We define a MRF via a set of random variables and an undirected graph: $(X, G)$. Unlike other some other types of PGM an MRF graph is undirected and there is no requirement for the graph to be acyclic. For a given graph $G = (V,E)$ with specified vertices and edges, a set of random variables $(X_v)_{v in V}$ forms a Markov-Random-Field over G if they satisfy the folowing properties: . Pairwise Markov Property: any two non-adjacent variables are conditionally independent given all other variables: $$X_u perp X_v | X_{V / {u,v }}$$ | Local Markov Property: A variable is conditionally independent of all other variables given its neighbors: $$X_u perp X_{V /N(u)} | X_{N(u)} $$ Where $N(u)$ represents the set of neighbours of vertex $u$ according to $G=(V,E)$ | Global Markov Property: Any two subsets of variables are conditionally independent given a separating subset: $$X_A perp X_B | X_S $$ Where every path from a node in $A$ to a node in $B$ passes through $S$. | One useful way of specifying the MRF is through the conecpt of clique potentials. A subgraph $C subset G$ is a clique if it is a fully connected subgraph, the set of all cliques of $G$ is denoted: $Cl(G)$. We can then specify a joint distribution of random-variables $ mathbf{X} = (X_v)_{v in V}$ via clique potential functions: $ phi_C(X_C)$ such that: $$P( mathbf{X} = mathbf{x}) = prod_{C in Cl(G)} phi_C (x_C) $$ We note that every joint distribution specified in this was is a MRF. However the reverse is not necessarily true, we can however factorise a joint distribution this way if: . The distribution is strictly positive (known as the Hammersley-Clifford theorem) | The graph is chordal | . A convenient form of clique potential is the exponential form which we can denote as: $$ P( mathbf{X} = mathbf{x}) = frac{1}{Z} exp left( sum_{C in Cl(G)} w_C f_C(x_C) right)$$ With: $$ Z = sum_{ mathbf{x}} exp left( sum_{C in Cl(G)} w_C f_C(x_C) right) $$ Which is nothing more than a general form of the Gibbs distribution we looked at in previous blog posts. To see this for the Ising model the cliques are adjacent pairs of spins $C_{xy} = ( sigma_x, sigma_y)$ with $f_{C_{xy}} = sigma_x sigma_y$ and $w_{C_{xy}} = - J_{xy}$ - this is the simplest form of a MRF. We can then extend some of the ideas and results we have looked at previously to a much larger class of probabilistic models. . Markov random fields have been used in a variety of applications relating to computer vision and image/video processing including texture synthesis, compression, enhancement, etc. They have also been used in machine learning and computational biology. The ideas are also used in combinatorial optimization problems owing to their similarity to spin glasses. . Hopfield Network . We now look at a(n artificial) neural-network model developed by John Hopfield in 1982. This model is no longer particularly in favour as it doesn&#39;t have particularly strong performance but is interesting nonetheless. We will look at Hopfield networks as a mechanism for pattern storage and recall, they can be used in different applications. The model took spin glass models as a direct inspiration. . A Hopfield network is a single layer recurrent network with a fully connected geometry, each neuron receives input from every other neuron and also gives output to every other neuron. We can specify the network through a weight matrix $(W_{ij})_{ij}$ which has the following properties: . Symmetry: $W_{ij} = W_{ji} ; ; ; forall i,j$ | No self-connectivity: $W_{ii} = 0 ; ; ; forall i$ | . The neurons themselves take discrete values of $ sigma_i pm 1$ denoting &quot;on&quot; or &quot;off&quot; states. At each update step we define a quantity: $$ theta_i = sum_j W_{ij} sigma_j + b_i$$ Where $b_i$ is a bias term (we can take $b_i = 0$). If $ theta_i &gt; 0$ then we set $ sigma_i = 1$ and vice-versa. We can also note that this induces an energy function: $$ E = - frac{1}{2} sum_{ij} W_{ij} sigma_i sigma_j $$ Which is exactly the Hamiltonian we investigated in our spin glass blog posts. We can see that the Hopfield Network is in fact nothing more than a Sherrington-Kirkpatrick spin glass viewed in a different way. From fairly basic arguments we can see that for each step of the dynamics described above the energy decreases. . Now suppose we want the network to &quot;remember&quot; patterns, that is we want it to remember a certain &quot;on-off&quot; configuration of neurons. To do this we have to select the correct weight matrix. By noting that the dynamics suggest each step leads to an energy decrease we want our stored patterns to be local energy minima of the network. Suppose we want to store an $N$ bit pattern: $ hat{x} = left( hat{x}_1, hat{x}_2, ... , hat{x}_N right)$ if we take: $W_{ij} = c hat{x}_i hat{x}_j$ for some constant $c&gt;0$ (representing the learning rate), then we have: $$ theta_i = sum_j W_{ij} sigma_j = sum_j c hat{x}_i hat{x}_j sigma_j = c sum_j hat{x}_i = c(N-1) hat{x}_i$$ And so we have $ hat{x}$ forms a(n attractive) fixed point of the dynamics. Starting from a noisy input configuration eventually we should end up with the stored pattern. . We can naturally extend this further if we want to store $P$ different patterns $ left( hat{x}^p right)_{p=1}^P$: $$W_{ij} = c sum_p hat{x}_i^p hat{x}_j^p$$ Typically we would take $c$ to scale with the number of patterns, such as: $c= frac{1}{P}$. . From basic arguments and intuition we can see that recall will deteriorate as we add more and more patterns. Can we construct an argument to find a &quot;capacity&quot; of the network? Yes! And it is quite simple. We start by re-writing the sum for $ theta_i( hat{x}^p)$ (that is $ theta_i$ evaluated at one of the stored patterns $p$): $$ theta_i( hat{x}^p) = sum_j W_{ij} hat{x}^p_j = frac{1}{P} sum_j sum_q hat{x}_i^q hat{x}_j^q hat{x}^p_j = hat{x}_i^p + frac{1}{P} sum_j sum_{q neq p} hat{x}_i^q hat{x}_j^q hat{x}^p_j $$ Where the second term (the double summation) is referred to as the crosstalk term, if this is less than 1 then $ hat{x}^p$ is a fixed point and the network has &quot;remembered&quot; it. We can then define: $$ C_i^p = - hat{x}_i^p frac{1}{P} sum_j sum_{q neq p} hat{x}_i^q hat{x}_j^q hat{x}^p_j $$ If $C_i^p&lt;0$ then $ hat{x}^p$ will be a fixed point. For $C_i^p&gt;1$ then there is instability. We want to find $P(C_i^p &gt; 1)$. By considering the limit of a large number of neurons and patterns we can consider the case of the terms $ hat{x}_i^q hat{x}_j^q$ being uniformly random. By applying the central limit theorem we get: $C_i^p sim N(0, sigma^2)$ with $ sigma^2 = frac{P}{N}$ - that is the ratio of the number of stored patterns over the number of neurons. For example if we have $ frac{P}{N} = 0.37$ then the probability of observing an error is around $5%$. From further analysis (not shown here) we can find that these errors can &quot;add up&quot;, for stability we require: $ frac{P}{N} &lt; 0.138$. So for a $10x10$ neuron array we will only want to store 13 patterns or fewer. . So far we have assumed that each pattern is treated equally by the network, we can remember some patterns &quot;more strongly&quot; by assigning them a multiplicity (essentially the number of times the pattern is &quot;remembered&quot;). With higher multiplicity the pattern will have a larger basin of attraction. However we have to consider the sum of multiplicities now as opposed to the number of unique patterns. For example for the $10x10$ neuron case, if we have a pattern with multiplicity $10$ then we can only store a maximum of 3 extra patterns (of multiplicity 1). . So far the picture looks rosy with our Hopfield-Network, unfortunately there are a few issues that prevents them being widely used. Firstly we have the occurence of &quot;spurious states&quot; - these are non-remembered states that are attractors of the network dynamics. If we start near one of these spurious states the dynamics will converge towards them, which is obviously not ideal. These are due to the complex energy landscape of the Sherrington-Kirkpatrick spin-glass. One simple example of a spurious state would be any inverse image of a stored pattern (i.e. flipping +1 neuron activations to -1 and vice versa). There can be other spurious states however. . We now present a basic implementation of a Hopfield network as an illustration. We will consider a $7x7$ neuron grid and aim to store $4$ patterns. We will &quot;flatten&quot; the neuron grid to a $49$ element single dimension array for convenience. . # A code implementing a Hopfield network # Storing a number of 2d images for recall import matplotlib.pyplot as plt import numpy as np # Set random seed for reproducibility np.random.seed(123) num_patterns = 4 grid_width = 7 grid_height = 7 grid_size = grid_width*grid_height # Fix stored patterns stored = np.zeros((num_patterns, grid_size)) stored[0] = [-1, -1, +1, +1, -1, -1, -1, -1, +1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, +1, +1, +1, +1, +1, -1 ] stored[1] = [-1, +1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, +1, +1, -1, +1, +1, +1, +1, +1, -1, +1, +1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, +1, +1, +1, +1, +1, +1 ] stored[2] = [-1, +1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1, +1, +1, +1, +1, -1, -1, -1, -1, -1, -1, -1, +1, +1, -1, -1, -1, -1, -1, +1, -1, +1, +1, +1, +1, +1, +1 ] stored[3] = [-1, -1, -1, +1, +1, -1, -1, -1, -1, +1, -1, +1, -1, -1, -1, +1, -1, -1, +1, -1, -1, +1, -1, -1, -1, +1, -1, -1, +1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, +1, -1, -1, -1, -1, -1, -1, +1, -1, -1 ] # Display the patterns fig, ax = plt.subplots(1, num_patterns, figsize=(10, 5)) for i in range(num_patterns): ax[i].imshow(stored[i].reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[i].set_xticks([]) ax[i].set_yticks([]) ax[i].set_title(&quot;Pattern %i&quot; %(i+1)) fig.suptitle(&quot;Remembered Patterns&quot;, x=0.5, y=0.8, size=&#39;xx-large&#39;) plt.show() # Create network weights matrix W = np.zeros((grid_size, grid_size)) for i in range(grid_size): for j in range(grid_size): if i == j or W[i, j] != 0.0: continue w = 0.0 for n in range(num_patterns): w += stored[n, i] * stored[n, j] W[i, j] = w / num_patterns W[j, i] = W[i, j] # Test noisy inputs noise_rate = 0.25 num_cycles = 5 fig, ax = plt.subplots(num_patterns, 3, figsize=(10, 15)) for x in range(num_patterns): original_image = stored[x] noisy_image = original_image.copy() for i in range(grid_size): if np.random.random() &lt; noise_rate: noisy_image[i] *= -1 test_image = noisy_image.copy() for _ in range(num_cycles): for i in range(grid_size): test_image[i] = 1.0 if np.dot(W[i], test_image) &gt; 0 else -1.0 # Display results ax[x,0].imshow(noisy_image.reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[x,0].set_xticks([]) ax[x,0].set_yticks([]) ax[x,0].set_title(&quot;Noisy Input Pattern %i&quot; %(x+1)) ax[x,1].imshow(test_image.reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[x,1].set_xticks([]) ax[x,1].set_yticks([]) ax[x,1].set_title(&quot;Hopfield Network Prediction&quot;) ax[x,2].imshow(original_image.reshape((grid_height, grid_width)), cmap=&#39;binary&#39;) ax[x,2].set_xticks([]) ax[x,2].set_yticks([]) ax[x,2].set_title(&quot;Target Pattern %i&quot; %(x+1)) fig.suptitle(&quot;Noisy Image Retrieval (Noise Rate=%d&quot; %(noise_rate*100) +&quot;$ %$)&quot;, x=0.5, y=0.91, size=&#39;xx-large&#39;) plt.show() # Calculate energies of stored patterns energy = np.zeros(num_patterns) for x in range(num_patterns): e = 0 for i in range(grid_size): for j in range(grid_size): e += W[i,j] * stored[x][i]* stored[x][j] e *= -0.5 print(&quot;Energy of pattern &quot;,x+1,&quot;:&quot;,e) . Energy of pattern 1 : -294.0 Energy of pattern 2 : -447.0 Energy of pattern 3 : -454.0 Energy of pattern 4 : -301.0 . In this toy example we have seen results that are not unreasonable, however even in this relatively simple example we see that patterns 2 and 3 are not recalled exactly - most likely as there is a high degree of overlap between them, each time the network dynamics converged to a spurious pattern. . Hopfield networks were the basis for the development of Boltzmann machines however which are stochastic versions of the Hopfield network. We will not touch on these here. . NK Model . We now focus our attention on another model with a similarity to spin-glasses: the NK model propsed by Stuart Kauffmann. Described as a &quot;tunable ruggedness&quot; model by Kauffmann, the NK model originally proposed as a model of evolution (not to be confused with an &quot;evolutionary algoirthm&quot;). The model controls the size of a landscape and the number of local optima via the 2 parameters $N$ and $K$. The $N$ parameter denotes the degrees of freedom or the &quot;size&quot; of the system. the parameter $K$ represnts the level of interactivity between those degrees of freedom. $K$ varies from $K=0$ (leading to a smooth landscape) through to $K=N-1$ the most rugged/peaked. . In the context of spin-glass models we can express the Hamiltonians corresponding to the NK model as: begin{align} K=0 implies H &amp;= - sum_{i} J_{i} sigma_{i} K=1 implies H &amp;= - sum_{i} J_{i} sigma_{i} - sum_{ij} J_{ij} sigma_{i} sigma_{j} K=2 implies H &amp;= - sum_{i} J_{i} sigma_{i} - sum_{ij} J_{ij} sigma_{i} sigma_{j} - sum_{ijk} J_{ijk} sigma_{i} sigma_{j} sigma_{k} end{align} And so on. through this interpretation we can see that the NK model is nothing more than a generalization of the spin glass models we have looked at previously. However the model did not use spin-glasses as a basis, it was developed as a way of studying evolutionary systems and how one can navigate a fitness landscape. . As one would expect given the similarity the NK model shares some of the properties of spin-glasses; most notably that it has a very complicated landscape that often makes it hard to find even local optima. If the model sufficiently captures the properties of evolutionary systems this has some interesting implications. . We&#39;ll now present an NK model outside of the context of spin-glasses - namely as an evolution model as originally intended. We start by considering a sequence of genes $(s_i)_{i=1}^N$. We want to observe what happens to these genes over time. Within this model we assume an organism ($ mathbf{s}$) is completely defined by its gene sequence. The fitness of an organism is defined as an average of the fitness of its genes: $$ F( mathbf{s}) = frac{1}{N} sum_i f(s_i)$$ The fitness function depends on the value of the gene itself but also other genes - in biology this is known as &quot;epistasis&quot;. We use the matrix $ left( A_{ij} right)_{i,j=1}^N$ to denote dependence. If $A_{ij} = 1$ then gene $i$ depends on gene $j$ and is zero otherwise. Note that this matrix denotes a directed network and need not be symmetric. We can then compute a fitness landscape for each organism according to this equation. . In the NK model we assume that initially the entire population consists of one organism (sequence of genes). At random a mutation occurs to one of the genes, this new mutation either dies out quickly (if it has a lower fitness) or it reproduces faster than the original organism (which dies out) and becomes the sole organism in the system. The assumption here of course is that genetic mutations are far enough apart in time that the system can &quot;settle&quot; at each step. . Given a fitness landscape we can specify the dynamics of the NK model as: . From $ mathbf{s} = (s_1, ..., s_N)$ pick a gene $i in {1, ... , N}$ at random and mutate its value to create a new organism $ mathbf{ hat{s}}$ | If $F( mathbf{ hat{s}}) &gt; F( mathbf{s})$ use $ mathbf{ hat{s}}$ as the new state, otherwise keep $ mathbf{s}$ and repeat | Clearly this is simply a greedy hill climber algorithm and the dynamics will end up in a nearby local maxima. The main attraction of the NK model is through the development of the fitness landscape. . The NK model has been developed further in the NKCS model. The NK model essentially looks at genetic variation of one species, the NKCS model extends this idea to multiple species. To do this we assume each species follows its own NK dynamics, its dynamics also interacts with $S$ other species. Each gene in a species is coupled to $C$ randomly chosen genes from the $S$ species. The dynamics can then be described as: . Sequentially select each species during a time step | With the selected species mutate one of the genes | Calculate the updated fitness and either accept or reject this mutation and repeat | Whereas the NK model always ends up in a local fitness maxima this is not necessarily true of the NKCS model since for a fixed point we would require each species to be in a fitness maxima at the same time. This allows the model to display some more sophsticated dynamics such as criticality, self-organised-criticality and co-evolution (and others). . In addition to evolution these models can also be used as a model of immunity. Along with this NKCS models have been used as a model of technological evolution. The model is general enough that it can be applied to, essentially, any evolutionary system. . These models can get fairly complicated, in the future I may write a full length post on models of evolution. . Conclusion . We have now seen that we can extend our spin-glass models to more sophisticated spin-glass models through the Potts model which are more physically plausible. We also saw that spin-glass models were a major influence on some of the first artificial neural networks. Lastly by looking at spin-glasses from a different point of view we have seen how they can inform our understanding of evolutionary systems. . We did not cover it here but spin-glasses have also influenced combinatorial optimization problems. We touched upon this in a previous post where we introduced the concept of simulated annealing. These techniques have been used in computer chip design (where one needs to optimize size/geometry of chip design versus performance). They have also been used in studying protein folding where there is a large universe of possible orientations that need to be searched according to a number of constraints. .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-5",
            "relUrl": "/spin-glass-models-5",
            "date": " â€¢ Mar 31, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Spin Glass Models 4: Ising Model - Simulation",
            "content": ". This is the fourth blog post in a series - you can find the previous blog post here . . Setup . In this blog we will limit ourselves to trying to find minimum energy states of the Ising model - we could use these schemes in order to estimate other thermodynamic properties also. The models will be in 2-dimensions on a square lattice since the results of these will be easier to display, it should be fairly obvious how we could modify this code for other dimensions. We will also use Gaussian distributed interaction strengths since we do not require mathematical tractability when we are simulating. . Interactions . Unlike the Sherrington-Kirkpatrick model we will have to be a bit &quot;clever&quot; in defining the interaction strengths. I have decided to do this using 4 (NxN) arrays representing the up, down, left and right interaction strengths. This is a bit wasteful in terms of memory use but I think it is worth the trade-off to improve readability of the code. We then have the following symmetries: . up[i,j] == down[i-1,j] left[i,j] == right[i,j-1] . Where these indices exist, for the boundary coniditons we will take the following: . up[0,j] == 0 down[N-1,j] == 0 left[i,0] == 0 right[i,N-1] == 0 . For all possible $i, j$. This is a fixed boundary condition, the edge cases will have only 3 neighbours and corner cases only 2. This can introduce some instability for small spin-glasses but this is fine for our purposes. Modifying this to allow for periodic boundary conditions (i.e. a toroidal geometry) would not be particularly difficult. . Mixing, Convergence, etc. . The methods presented below are Markov-Chain Monte-Carlo (MCMC) methods. These methods are notoriously finicky and require parameter tuning, running multiple chains, etc. in order to ensure good performance. As to not get distracted in this blog post I will not mention these concerns but please keep them in mind when reading on. For now we will run a single chain and look at the results it produces, in practice one would not do this. . Local Update Methods . We begin by looking at local update methods. In the Sherrington-Kirkpatrick blog post we looked at one such (very bad) method: the greedy gradient descent. Local update methods mean at each simulation step we update 1 site only. . Metropolis-Hastings . First we look at the &quot;classic&quot; approach to simulating the Ising model. In many texts and online references this will be the only method that is presented, it leads some to believe (incorrectly) that the simulation method is somehow part of the Ising model itself. This is the method I presented (without elaboration) in my Cython/Numba blog post. . We present Metropolis-Hastings in it&#39;s most general form first (our application to the Ising model will appear below): . Initialize the system in state $x_t$ | Sample a new proposed state from some distribution $Q(x&#39;_{t+1} | x_t)$ | Accept this new state with probability $min left[ alpha, 1 right]$, else $x_{t+1} = x_t$, where: $$ alpha = frac{P(x&#39;_{t+1})Q(x_t |x&#39;_{t+1})} {P(x_t)Q(x&#39;_{t+1} |x_{t})} $$ | Repeat steps 2. and 3. | In this context $P(.)$ represents the probability distribution we wish to estimate. We notice that this does not need to be standardized (i.e. we can ignore any difficult partition functions) so this is a (relatively) easy algorithm to implement. We find that the sequence $x_t$ forms a Markov-Chain - hence the term Markov-Chain Monte Carlo (MCMC). . We can see that that Metropolis-Hastings consists of 2 steps: a proposal step then an acceptance/rejection step. We see that there are essentially no constraints on $Q(.)$ so we can use something easy to sample from. We only need to be careful that it has a support that contains the support of $P(.)$ - otherwise we will not be able to sample all possible values as required. Selecting a suitable $Q(.)$ is at the heart of the algorithm however, the best performance will be where $Q$ and $P$ are very similar. Since $P$ is generally unknown in advance this often requires a bit of trial and error. There are many ways to test whether the algorithm is doing a &quot;good job&quot; but we will not cover them here (in a later set of blog posts I should probably go deeper into MCMC methods - here we&#39;re just concerned with the Ising model in particular). . Moving back to the Ising model picture, we can apply the Metropolis-Hastings methodology via: . First pick a random site | Calculate the change in energy associated with &quot;flipping&quot; this sites spin (going from +1 to -1 or vice versa) | If the energy decreases accept the flipped state and start again with a new site | If energy increases accept the flipped state with some acceptance probability or keep the same state | Begin again | We can see that in contrast to the &quot;greedy hill climber&quot; approach in the Sherrington-Kirkpatrick blog post we are not simply declining into a local minima, there is some probability that we can jump to a state of higher energy and escape a local minima to find a &quot;better&quot; one. In theory if we wait long enough we should find ourselves in a global minimum state. We now derive the acceptance probability as given by Metropolis-Hastings: . Recall that the Gibbs distribution for the Ising model with a given configuration is: $$ P( sigma) = frac{exp(- beta H_{ sigma})}{Z_{ beta}} $$ . Since we are uniformly selecting new states the $Q(.)$ terms in $ alpha$ cancel. If we denote the propsed state as: $ hat{ sigma}$ then the relative likelihood is: $$ alpha = frac{P( hat{ sigma})}{P( sigma)} = frac{exp(- beta H_{ hat{ sigma}})}{exp(- beta H_{ sigma})} = exp left(- beta left(H_{ hat{ sigma}} - H_{ sigma} right) right) = exp(- beta Delta H) $$ . To summarise by Metropolis-Hastings we then accept the new configuration with probability: $$ p_{flip} = min left[1, exp(- beta Delta H) right] $$ . In this particular case we can calculate $ Delta H$ quite efficiently since we are changing the spin of only 1 site, the contributions of all other sites &quot;cancels out&quot;. We can write this down as: $$ Delta H = 2 sigma_{i,j} sum_{(x,y) in langle i, j rangle} J_{x,y} sigma_{x,y} $$ . This introduces some efficiency in a code implementation since we do not have to calculate the energy of the whole configuration each time we want to update a site&#39;s spin. . An example implementation of this can be seen below: . # An implementation of an Ising spin-glass of size NxN # With fixed boundary conditions using Metropolis-Hastings # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Updates occur at randomly selected sites import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 32 spins = np.random.choice([-1, 1], (N,N)) # Fix number of timesteps and some containers timesteps = 10000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction arrays # Have 4 arrays: up, down, left right # These represent the interaction strengths to the # up/down/left/right neighbours of a site # There is a symmetry between these matrices # This is not the most memory efficient solution s_h = 1 s_v = 1 up = np.zeros((N,N)) down = np.zeros((N,N)) left = np.zeros((N,N)) right = np.zeros((N,N)) up[1:N,:] = np.random.rand(N-1,N) * s_v down[0:N-1,:] = up[1:N,:] left[:,1:N] = np.random.rand(N,N-1) * s_h right[:,0:N-1] = left[:,1:N] mag[0] = spins.sum() for i in range(N): for j in range(N): if i == 0: up_neighbour = 0 down_neighbour = spins[i+1,j] elif i == N-1: up_neighbour = spins[i-1,j] down_neighbour = 0 else: up_neighbour = spins[i-1,j] down_neighbour = spins[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = spins[i,j+1] elif j == N-1: left_neighbour = spins[i,j-1] right_neighbour = 0 else: left_neighbour = spins[i,j-1] right_neighbour = spins[i,j+1] energy[0] += spins[i,j]*(up[i,j]*up_neighbour + down[i,j]*down_neighbour + left[i,j]*left_neighbour + right[i,j]*right_neighbour) # Avoid double count - each neighbour pair # counted twice in above since loop over each site energy[0] /= 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1 # Define proposal step def proposal(s_array): _N = s_array.shape[0] return np.random.choice(_N, 2) def energy_change(spin_site, bt, s_array, up_array, down_array, left_array, right_array): i = spin_site[0] j = spin_site[1] if i == 0: up_neighbour = 0 down_neighbour = s_array[i+1,j] elif i == N-1: up_neighbour = s_array[i-1,j] down_neighbour = 0 else: up_neighbour = s_array[i-1,j] down_neighbour = s_array[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = s_array[i,j+1] elif j == N-1: left_neighbour = s_array[i,j-1] right_neighbour = 0 else: left_neighbour = s_array[i,j-1] right_neighbour = s_array[i,j+1] dE_tmp = 2*s_array[i,j]*(up_array[i,j]*up_neighbour + down_array[i,j]*down_neighbour + left_array[i,j]*left_neighbour + right_array[i,j]*right_neighbour) return dE_tmp def acceptance(bt, energy): if energy &lt;= 0: return -1 else: prob = np.exp(-bt*energy) if prob &gt; np.random.random(): return -1 else: return 1 # Define update step dE = 0 dM = 0 def update(bt, s_array, up_array, down_array, left_array, right_array): global dE global dM # Proposal Step site = proposal(s_array) # Calculate energy change dE = energy_change(site, bt, s_array, up_array, down_array, left_array, right_array) dM = -2*s_array[site[0],site[1]] # Acceptance step accept = acceptance(bt, dE) if accept == -1: s_array[site[0], site[1]] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts, s_array, up_array, down_array, left_array, right_array): s_temp = s_array.copy() for i in range(ts): update_step = update(beta, s_temp, up_array, down_array, left_array, right_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, up, down, left, right) mag = mag / (N**2) energy = energy / (N**2) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . Gibbs Sampling . Many presentations of the Ising model only show the Metropolis-Hastings scheme, as such there is a misconception that the Metropolis-Hastings sampling is somehow part of the Ising model itself. This is not true, an alternative to Metropolis-Hastings is Gibbs-Sampling. We can describe Gibbs-Sampling in general terms as: . Pick an initial state $ pmb{x_t} = left(x^1_t, x^2_t, ... , x^N_t right)$ | For each $N$ dimension sample: $x^i_{t+1} sim P left(x^i_{t+1} | x^i_{t+1}, x^2_{t+1}, ... , x^{i-1}_{t+1}, x^{i+1}_t, ... x^N_i right)$ and define next state as: $ pmb{x_{i+1}} = left(x_1^{i+1}, x_2^{i+1}, ... , x_N^{i+1} right)$ | Repeat sampling for each successive state | We can see this is just a special case of Metropolis-Hastings, if we denote $ pmb{x_t^{-j}} = left(x_t^1, x_t^2, ... , x_t^{j-1}, x_t^{j+1}, ..., x_t^N right)$ (all components apart from the jth). Then we can set: $$Q(x_{t+1}^j pmb{x_t^{-j}} | pmb{x_t} ) = P(x_{t+1}^j | pmb{x_t^{-j}})$$ In the Metropolis-Hastings scheme, the acceptance probablilty then becomes: begin{align} min left[1 , frac{Q(x_{t+1}^j pmb{x_t^{-j}} | pmb{x_t} ) P( pmb{x_t})}{Q( pmb{x_t} | x_{t+1}^j pmb{x_t^{-j}} ) P(x_{t+1}^j pmb{x_t^{-j}})} right] &amp; = min left[1 , frac{P( pmb{x_t})P(x_{t+1}^j | pmb{x_t^{-j}})}{P(x_{t+1}^j pmb{x_t^{-j}})P(x_{t}^j | pmb{x_t^{-j}})} right] &amp;= min left[1 , frac{P(x_{t}^j | pmb{x_t^{-j}})P( pmb{x_t^{-j}})P(x_{t+1}^j | pmb{x_t^{-j}})}{P(x_{t+1}^j | pmb{x_t^{-j}})P( pmb{x_t^{-j}})P(x_{t}^j | pmb{x_t^{-j}})} right] &amp;= min left[1, 1 right] = 1 end{align} Which results in the Gibbs procedure as described above. . We can implement Gibbs sampling in the context of an Ising model as: . Pick a random site $(i,j)$ | Set spin to +1 with probability $p_{ij}$, or -1 with probability $(1-p_{ij})$ | Begin again | The probability is defined as: $$ p_{ij} = frac{1}{1+ exp(- beta Delta H / sigma_{i,j})} $$ . At lower temperatures this should behave similarly to the Metropolis-Hastings. However the Metropolis-Hastings method has approximately twice the probability of accepting energetically unfavourable states, as such the Gibbs might be less efficient. . Note: this is true for the Ising model with methods as descibed - it is note a general point on Gibbs/Metropolis-Hastings. When sampling from higher dimensional distributions Gibbs samplers sample each dimension independently whereas Metropolis-Hastings samples points from the high-dimensional space. In some situations the Gibbs sampler can perform significantly better. . Since this is only a minor update to the Metropolis-Hastings code we will not present it here. . Glauber Dynamics (and Heat-Bath) . Another confusion I have seen is that Metropolis-Hastings is the only acceptance-rejection scheme. This is not true, we can also define alternative acceptance probabilites. One such example is Glauber dynamics where we can express the acceptance probability as: $$ p_{flip} = frac{1}{2} left( 1 - tanh left( beta Delta H / 2 right) right) frac{exp(- beta Delta H/2)}{exp( beta Delta H/2) + exp(- beta Delta H/2)} $$ We will not derive this here nor simulate using Glauber dynamics (but we could modify 1 or 2 lines of the code above to ahcieve this), it is just to illustrate another option. For the Ising model Glauber dynamics has a lower acceptance probability for all possible states, as such its performance is likely to be slightly worse than Metropolis-Hastings. . For the Ising model Glauber-Dynamics is identical to the Heat-Bath method. . Simulated Annealing and Simulated Tempering . We now move onto our first &quot;improvement&quot; to the Metropolis-Hastings scheme: simulated annealing. . The concept is very simple and takes inspiration from physical systems. Essentially we just start the system in a high temperature and gradually cool it down. This makes intuitive sense since at higher temperatures we have an increased chance of jumping out of local-minima and get closer to a better overall minima. However we will struggle to actually locate the minima at high temperature for the same reason. In contrast with a low temperature we will be able to locate a nearby local-minima very accurately but will not be able to jump out of it to find a better one. By starting off &quot;hot&quot; the samples will jump between many local minima, as we slowly cool down there should be fewer jumps between local minima and it should eventually get stuck in the domain of a &quot;good&quot; local minima (not far from the global minima ideally) as we cool the temperature further we should get closer and closer to that minima. This is similar to the process of annealing metals by heating them then cooling them to reorganize the crystalline structure. . Simulated annealing has proved very useful in the field of combinatorial optimization in situations where we want to quickly generate &quot;good&quot; solutions (not necessarily &quot;best&quot;). Variations on the idea can be seen in many areas (e.g. variable learning rate algorithms in deep learning can be thought of as a form of simulated annealing). We can also note there is nothing in the method that is particular to Metropolis-Hastings (or the other variations presented) - it can be used with pretty much any simulation method. . We have not descibed &quot;how&quot; we would want to decrease the temperature over time. This is part of the &quot;issue&quot; with simulated annealing (and many MCMC algorithms in general) - there is just as much art as their is science to implementing them. There are no real hard fast rules for getting good results, one in essense has to just try various options until something works (or on well studied problems borrow schemes from others). . We will take a simple approach where we decrease temperature by 10% every 500 steps starting from a temperature of 4 (this was chosen arbitrarily - it is not a suggestion of what might work well in this situation!) We can make use of the code example above for Metropolis-Hastings to give a compact implementation of: . # An implementation of a Metropolis-Hastings algorithm # with simulated annealing applied to a 2d Ising spin glass # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 32 spins = np.random.choice([-1, 1], (N,N)) # Set up initial beta beta = 1/4 def _main_loop_SA(ts, bt_initial, s_array, up_array, down_array, left_array, right_array): s_temp = s_array.copy() bt_live = bt_initial for i in range(ts): if ts % 500 == 0: bt_live *= 1/0.9 update_step = update(bt_live, s_temp, up_array, down_array, left_array, right_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop_SA(timesteps, beta, spins, up, down, left, right) mag = mag / (N**2) energy = energy / (N**2) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . We can see that the system settled down to a lower energy state more quickly and smoothly than with the vanilla Metropolis-Hastings scheme. . Although simulated annealing can improve on vanilla Metropolis-Hastings it can still struggle to find a global minima of the system. There are various &quot;hacks&quot; that can further improve this however - one such example being the concept of restarting. Again this is a very &quot;obvious&quot; thing to try - we store the &quot;best&quot; state we&#39;ve visited so far in a simulation, if we &quot;get stuck&quot; somewhere above this energy level we &quot;jump back&quot; to this best state and try again. . We can extend the simulated annealing idea further to the concept of &quot;simulated tempering&quot;. Here we treat the temperature of the system as a variable in itself, the teperature can go up as well as down during the simulation. This can further improve convergence properties since it allows the system to escape energy boundaries more easily by increasing temperature. This can remove the need to use restarting since a higher temperature is always available as an option at all times. . One such simulated tempering scheme is &quot;parallel tempering&quot; - as the name suggests this involves running many Markov-Chains in parallel and &quot;jumping&quot; between chains as the algorithm progresses. In some instances the cost of running multiple chains is less than the improvement in performance. Again however there is an art to selecting the correct number of chains and temperatures to run in parallel, most times there is no substitute for just trying things and running tests on the results. For the interests of brevity we will not present a full code here - but we note that for 2 chains at temperatures $T_1$ and $T_2$ our proposed update is to switch the states between the two chains (or swap temperatures of the 2 chains) the acceptance probability is then: $$p_{flip} = min left[1, exp((H_1-H_2)( beta_1 - beta_2)) right] $$ Where $H_1$ is the energy as defined by the Hamiltonian for chain with temperature $T_1 = 1/ beta_1$ and similar for $H_2$. This can be easily adapted to more chains and temperatures. . Cluster Update Methods . We now have a few options for simulating the Ising model, however they are by no means perfect. The issue still remains of falling into local optima instead of a global optima. From our previous mathematical study we know that energy minima for the Ising model are &quot;far away&quot; from each other, that is they have very little overlapping spins. By flipping individual spins 1 by 1 it is very hard to make the chains explore the energy landscape fully. The natural way to solve this is to flip multiple spins simultaneously at each step. From the general definition of the Metropolis-Hastings method there is nothing stopping us in following this line of reasoning. . Unfortunately this makes things much harder, the complications arise in finding a valid scheme for flipping multiple spins at once. We have glossed over the mathematical foundations of MCMC here but the proposal/acceptance probabilities need to be selected in &quot;smart way&quot; in order for the resulting Markov chain to have certain properties. When looking at more than 1 spin at a time in the Ising model this proved fairly difficult. This is evidenced by the original Metropolis-Hastings scheme being proposed in 1953 yet the first multi-spin method not being proposed and justified until the late 1980s. . Wolff Algorithm . The main idea of the algorithm is to look for &quot;clusters&quot; of spin sites with the same spin. We then decide to flip the spin of all the sites within this cluster at once. We then pick a new cluster and repeat this process as necessary. The pseudocode for this algorithm as it applies to the Ising model is: . A site $x$ with spin $ sigma_x$ is selected at random and added to an empty cluster | For each neighbour $y$ of $x$ such that $ sigma_y = sigma_x$ we add $y$ to the cluster stack with probability $p_{xy} = 1 - exp(-2 beta J_{xy})$ else move onto next neighbour | After all neighbours are exhausted select next site in the cluster stack and repeat the previous step until the cluster stack is exhausted | Once the cluster is fully specified flip the spins of all sites in the cluster and begin again | We can see that like the Gibbs sampling algorithm, here the Wolff algorithm is &quot;rejection free&quot; - that is all proposed sites are flipped. We also note that there is nothing in this method that is incompatible with simulated annealing/tempering - these techniques are often used together. . Some of the more computer-science focussed readers may be thinking: &quot;creating clusters is computationally expensive, will a brute force local update method not be better?&quot; Which is a valid concern; there a 2 things to consider here - firstly there are many efficient cluster generating algorithms from percolation theory which can help speed up this process (we presented a very simple method above for clarity.) And secondly the local update methods will take a very long time to make &quot;big jumps&quot; away from the current configuration - even though there is some probability to make unfavourable movements most of the time these will not be accepted, to jump to a different local energy minima we would need many such unfavourable moves which means we could be waiting for a very long time! This is why spin glass models form a good test bed for optimization algorithms as they are one of the simplest to define models with &quot;difficult&quot; energy landscapes. . We should find this algorithm performs better in general, especially near the 2nd order phase transition whereby successive samples become increasingly correlated (whereas we would want more independent samples). We will try and produce plots of the 2nd order themodynamic variables: heat capacity and susceptibility. We should expect to see an approximate discontinuity at the critical temperature. To do this we will re-run the Wolff algorithm multiple times for each target temperature. For each target temperature we will run 1500 &quot;burn&quot; steps and then evaluate our variables over the next 2500 steps. This should be long enough to get some reasonable results. . Note that the Wolff algorithm does not &quot;converge&quot; to a low energy state like the preceeding algorithms, instead it samples from the entire space in a &quot;smart way&quot; - even if it finds itself in the global energy minima there is still a relatively high probability of escaping. As such the graphs we produced before will look more &quot;wiggly&quot; - I&#39;ve heard the term &quot;fat caterpillar&quot; used to describe the graph of a well mixed MCMC algorithm. If we were interested in finding a ground state we could keep track of the configuration corresponding to the lowest observed energy state so far (we will not do this in our code however). . In the example below we&#39;ll run a constant $J=1$ to try and reproduce the heat capacity we found analytically in the previous blog post as a proof of concept. I will leave the functionality for general interaction strengths should you wish to experiment. . An implementation of this method can be seen below (note: this is a very slow code since we&#39;re using Python lists! It would be a prime candidate for being sped up using Cython/Numba/etc.): . # An implementation of the Wolff algorithm # With simulated annealing applied to a # 2d Ising spin glass import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model NxN and initial spins N = 32 spins_initial = np.random.choice([-1, 1], (N,N)) # Fix time-steps burn = 1500 evaluation = 2500 time_steps = burn + evaluation # Initialize interaction arrays # Have 4 arrays: up, down, left right # These represent the interaction strengths to the # up/down/left/right neighbours of a site # There is a symmetry between these matrices # This is not the most memory efficient solution s_h = 1 s_v = 1 up = np.zeros((N,N)) down = np.zeros((N,N)) left = np.zeros((N,N)) right = np.zeros((N,N)) # Using J=1 constant so graphs are easier to generate # Replace with comments to give an EA spin glass up[1:N,:] = 1 #np.random.rand(N-1,N) * s_v down[0:N-1,:] = up[1:N,:] left[:,1:N] = 1 #np.random.rand(N,N-1) * s_h right[:,0:N-1] = left[:,1:N] # Create function to find neigbour sites def nbr_udlr(s_site, s_array): _N = s_array.shape[0] i = s_site[0] j = s_site[1] if i == 0: up_site = 0 down_site = [i+1, j] elif i == _N-1: up_site = [i-1,j] down_site = 0 else: up_site = [i-1,j] down_site = [i+1, j] if j == 0: left_site = 0 right_site = [i,j+1] elif j == N-1: left_site = [i,j-1] right_site = 0 else: left_site = [i,j-1] right_site = [i,j+1] return [up_site, down_site, left_site, right_site] # Create function to return interactions strength def int_strength(s_site, udlr, up_array, down_array, left_array, right_array): if udlr == 0: return up_array[s_site[0], s_site[1]] if udlr == 1: return down_array[s_site[0], s_site[1]] if udlr == 2: return left_array[s_site[0], s_site[1]] if udlr == 3: return right_array[s_site[0], s_site[1]] def energy_calc(s_array, up_array, down_array, left_array, right_array): _N = s_array.shape[0] energy = 0 for i in range(_N): for j in range(_N): if i == 0: up_neighbour = 0 down_neighbour = s_array[i+1,j] elif i == N-1: up_neighbour = s_array[i-1,j] down_neighbour = 0 else: up_neighbour = s_array[i-1,j] down_neighbour = s_array[i+1,j] if j == 0: left_neighbour = 0 right_neighbour = s_array[i,j+1] elif j == N-1: left_neighbour = s_array[i,j-1] right_neighbour = 0 else: left_neighbour = s_array[i,j-1] right_neighbour = s_array[i,j+1] energy += s_array[i,j]*(up_array[i,j]*up_neighbour + down_array[i,j]*down_neighbour + left_array[i,j]*left_neighbour + right_array[i,j]*right_neighbour) return -energy/2 def wolff_step(bt, s_array, up_array, down_array, left_array, right_array): _N = s_array.shape[0] initial_site = np.random.choice(_N, 2) initial_site = [initial_site[0], initial_site[1]] old_spin = s_array[initial_site[0], initial_site[1]] cluster = [initial_site] stack = [initial_site] while stack != []: site = stack[np.random.choice(len(stack))] # Cycle neigbours nbr = nbr_udlr(site, s_array) for i in range(4): nbr_live = nbr[i] if nbr_live == 0: continue nbr_spin = s_array[nbr_live[0], nbr_live[1]] if nbr_spin == old_spin: if nbr_live not in cluster: p = 1 - np.exp(-2*bt*int_strength(site, i, up_array, down_array, left_array, right_array)) if np.random.random() &lt; p: cluster.append(nbr_live) stack.append(nbr_live) stack.remove(site) for site in cluster: s_array[site[0], site[1]] *= -1 return s_array ###### Main Code # Create useful constants N1 = evaluation*N*N N2 = evaluation*evaluation*N*N # Define temp ranges temp_steps = 20 temp_min = 1.75 temp_max = 2.75 temp_array = np.linspace(temp_min, temp_max, num=temp_steps) M = np.zeros(temp_steps) E = np.zeros(temp_steps) C = np.zeros(temp_steps) X = np.zeros(temp_steps) for t in range(temp_steps): spins = spins_initial.copy() M1 = 0 M2 = 0 E1 = 0 E2 = 0 beta = 1/temp_array[t] for i in range(time_steps): spins = wolff_step(beta, spins, up, down, left, right) if i &gt; burn: mag_tmp = abs(spins.sum()) M1 += mag_tmp M2 += mag_tmp**2 energy_tmp = energy_calc(spins, up, down, left, right) E1 += energy_tmp E2 += energy_tmp**2 M[t] = M1 / N1 E[t] = E1 / N1 C[t] = (E2/N1 - E1**2/N2)*beta**2 X[t] = (M2/N1 - M1**2/N2)*beta # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].scatter(temp_array, M, color=&#39;Red&#39;) axs[0, 0].set_title(&quot;Magnetism&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;Magnetism&quot;) axs[0, 1].scatter(temp_array, E, color=&#39;Blue&#39;) axs[0, 1].set_title(&quot;Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;Energy&quot;) axs[1, 0].scatter(temp_array, X, color=&#39;Red&#39;) axs[1, 0].set_title(&quot;Susceptibility&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;Susceptibility&quot;) axs[1, 1].scatter(temp_array, C, color=&#39;Blue&#39;) axs[1, 1].set_title(&quot;Heat Capacity&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;Heat Capacity&quot;) plt.show() . The plots here are a bit noisy but they loosely match our previous theoretical findings. . Swendsen-Wang Algorithm . Another cluster algorithm is the Swendsen-Wang. Unlike the Wolff algorithm Swendsen-Wang looks at multiple clusters concurrently and applies a spin-flip to all clusters. It was propsed 2 years prior to the Wolff method. . In pseudo code it can be presented as: . From an initialized spin configuration for each neighbour pair of sites $ langle x, y rangle$ we specify a bond: $b_{x,y} in {0, 1 }$ Where we sample according to: begin{align} &amp; mathbb{P}(b_{x,y} = 0 | sigma_x neq sigma_y) = 1 &amp; mathbb{P}(b_{x,y} = 1 | sigma_x neq sigma_y) = 0 &amp; mathbb{P}(b_{x,y} = 0 | sigma_x = sigma_y) = exp(-2 beta J_{xy}) &amp; mathbb{P}(b_{x,y} = 1 | sigma_x = sigma_y) = 1 - exp(-2 beta J_{xy}) end{align} | Generate clusters using bonds. If there exists a bond between sites $b_{x,y} = 1$ then the sites belong to same cluster | For each cluster with probability 1/2 flip all spins within the cluster to get a new configuration | Repeat process of generating bonds and clusters | We can see this is slightly different to the Wolff algorithm since it looks at multiple clusters within a given step. The performance of the Swendsen-Wang is slightly worse than that of the Wolff since it has a lower probability of flipping large clusters (in the case of Ising models). Both algorithms have been adpated and used to alternative spin glass models (as well as models outside of spin glasses). . We won&#39;t present an implementation of this method here since we already looked at the Wolff algorithm for an example of a cluster algorithm. . Conclusion . In this blog post we have looked at a variety of MCMC methods for simulating Ising models. We started by looking at various local update methods, which we now know do not behave optimally. We extended these ideas to cluster update methods which can show better performance for Ising models. We also looked at the very intuitive simulated annealing and simulated tempering methods, which have been used in optimization problems far outside the realms of spin glass models or even statsitical physics in general. . . This is the fourth blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-4",
            "relUrl": "/spin-glass-models-4",
            "date": " â€¢ Mar 24, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Spin Glass Models 3: Ising Model - Theory",
            "content": ". This is the third blog post in a series - you can find the previous blog post here . . In this blog post we are going to look at another spin-glass model. In the previous post we looked at the Sherrington-Kirkpatrick model which allowed us to study the spin-glass analytically. However there is a certain lack of &quot;realism&quot; in this model - the infinite interaction range means that the Sherrington-Kirkpatrick (in a sense) does not occupy any space, there is no concept of dimension nor geometry. While providing interesting mathematical results, some of which appear to be universal to spin-glass systems, we would like to look at a different model that captures more of the real world system and perhaps might uncover new behaviours. . Introducing the Ising Model . One model that we can look at is the Edwards-Anderson Ising model. We have actually looked at these models achronologically: Edwards-Anderson proposed the Ising model before Sherrington-Kirkpatrick proposed theirs. In fact Sherrington-Kirkpatrick developed their model partly due to the difficulty in deal with the Ising model. I have presented the models in this order in this series of blog posts because it feels more natural to look at models in increasing complexity order rather than presenting a historical account. . The main difference between Ising and Sherrington-Kirkpatrick is the extent over which the interactions can occur, instead of an infinite range the Ising model only allows &quot;nearest-neighbour&quot; interactions. This relaxation means that there is a concept of dimension and geometry to an Ising spin-glass. For example we could think of spins oriented on a line segment (a finite 1d Ising model), a square lattice (a 2d Ising model) or something more exotic like spins oriented on the nodes of a 32 dimensional hexagonal lattice. Through careful use of limits we could also consider infinite-dimensional Ising models too. The Hamiltonian follows the form one would expect: $$ H = - sum_{ lt x,y gt} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ Where we use the notation $ lt x,y gt$ to denote the sum occuring over neighbour pairs. . As before the selection of $J_{xy}$ is somewhat arbitrary, however unlike the Sherrington-Kirkpatrick we do not have to scale this with lattice size to retain meaningful &quot;average energy&quot; or &quot;average magnetism&quot; measurements when we increase the size of the system. If we take $J_{xy} = J$ for some fixed $J$ we get the Ising model of Ferromagnetism, if we allow $J_{xy}$ to vary according to some distribution we get the Edwards-Anderson Ising Model. In this blog we will use the term &quot;Ising model&quot; to refer to either situation, the mathematics of moving from the fixed $J$ case usually involves integrating against the density function, this can lead to more complicated formulae so in this blog we will mainly focus on the ferromagnetic case unless otherwise stated. . As before spins can only take values $ pm 1$ - in some literature this is referred to as &quot;Ising spins&quot; (e.g. you can read references to &quot;Sherrington-Kirkpatrick with Ising spins&quot; - this means an infinite interaction range with binary spins). . A pictorial representation of a 3d square lattice Ising spin-glass can be seen below: . For the rest of this article we will limit ourselves to talking about &quot;square&quot; lattices since this is where the majority of research is focussed. Using different lattice types won&#39;t change the story too much. . In studying these models it is also worth noting what happens on the &quot;boundary&quot; of the lattice: for finite size systems this can become an issue. In this blog post we will skirt over the issue, where necessary we will assume periodic boundary conditions (i.e. there is a toroidal type geometry to the system) so each &quot;edge&quot; of the lattice is connected to an opposing one, so in a sense there are no boundary conditions to specify. . 1D Ising Model . We will now consider a 1 dimensional Ising model where spins are located on a line segment. This simplication means that finding the ground state (minimal energy spin configuration) is trivial: we pick a site in the middle of the line segment (the origin) we will pick a spin ($ pm 1$) at random. We then propagate out from the origin in the left and right direction, we pick the spin for the next site according to the interaction strength for example: if $ sigma_0 = +1$ and $J_{0,1} &gt; 0$ then $ sigma_1 = +1$ and so on. We can see that (as with all spin glass systems) there is a symmetry: if we flip all the spins direction we end up with an equivalent energy - we call these &quot;pairs&quot; of configurations. It doesn&#39;t take much convincing to realise that the pair of configurations we constructed is a unique minima for the system. To see this imagine we have only one pair of spins that opposes the direction indicated by the interaction strength. For a large enough system we can eventually find another interaction strength that is greater in magnitude (by definition having a spin pair as indicated by the sign of the interaction strength). By simply flipping the relative orientation of the pairs we will end up with a configuration of lower energy - which is a contradiction. . We will now look to solve the 1d Ising model analytically. For simplicity we will assume no external magnetic field, this just makes the formulae look &quot;prettier&quot;. The crux of understanding any spin glass system is finding the Gibbs distribution, as before: $$P( sigma) = frac{exp(- beta H_{ sigma})}{Z_{ beta}}$$ Where: $$H = - sum_{ lt x,y gt} J_{xy} sigma_x sigma_y$$ And $ beta$ is the inverse temperature. We can write an expression for the partition function as: $$ Z_{ beta} = sum_{ sigma} exp(- beta H_{ sigma}) = sum_{ sigma} exp(- beta sum_{ lt x,y gt} J_{xy} sigma_x sigma_y) $$ Suppose we look at a line segment with $N$ spins, we can then write this as: $$ Z_{ beta} = sum_{ sigma_1, ..., sigma_N} exp(- beta (J_{1,2} sigma_1 sigma_2 + J_{2,3} sigma_2 sigma_3 + ... + J_{N-1,N} sigma_{N-1} sigma_N))$$ We notice that we can factorise this summation as: $$ Z_{ beta} = sum_{ sigma_1, ..., sigma_{N-1}} exp(- beta (J_{1,2} sigma_1 sigma_2 + ... + J_{N-2,N-1} sigma_{N-2} sigma_{N-1})) sum_{ sigma_N}exp(- beta J_{N-1,N} sigma_{N-1} sigma_N) $$ The internal sum can then be evaluated: $$ sum_{ sigma_N}exp(- beta J_{N-1,N} sigma_{N-1} sigma_N) = exp( beta J_{N-1,N} sigma_{N-1} ) + exp(- beta J_{N-1,N} sigma_{N-1} ) = 2cosh( beta J_{N-1,N} sigma_{N-1}) = 2cosh( beta J_{N-1,N}) $$ The last equality making use of the fact $ sigma_{N-1} = pm 1$ and that $cosh(x) = cosh(-x)$. By repeating this process we can express the partition function as: $$ Z_{ beta} = 2 prod_{i=1}^{N-1} 2 cosh( beta J_{i,i+1}) $$ We can evaluate this exactly. If we are sampling the interaction strengths according to some distribution we can find expected values (or other statistical properties) by integrating against the density function as usual, since this adds to notational complexity we will assume that the interaction strengths are fixed for now. We can then calculate thermal properties using the equations: . begin{align} F &amp;= - frac{1}{ beta} ln Z_{ beta} = - T ln 2 - T sum_{i=1}^{N-1} ln (2 cosh( beta J_{i,i+1})) U &amp;= - frac{ partial}{ partial beta} ln Z_{ beta} = - sum_{i=1}^{N-1} J_{i,i+1} tanh( beta J_{i,i+1}) C &amp;= frac{ partial U}{ partial T} = sum_{i=1}^{N-1} ( beta J_{i,i+1})^2 sech^2( beta J_{i,i+1}) S &amp;= frac{U - F}{T} = ln2 + sum_{i=1}^{N-1} left( - beta J_{i,i+1} tanh( beta J_{i,i+1}) + ln(2cosh( beta J_{i,i+1}) right) end{align}Where: F - is the Helmholtz-Free Energy U - is the thermodynamic energy (ensemble average) C - is the heat capacity S - is the entropy . We can find the expected value of a given instantiation of interactions through an integral such as: $$ mathbb{E}(U) = - sum_{i=1}^{N-1} int J_{i,i+1} tanh( beta J_{i,i+1}) Q(J_{i,i+1}) dJ_{i,i+1} $$ With $Q(J)$ being the density function of the distribution and the integral occuring over its support. Similar formulae exist for the other thermodynamic variables and you can calculate other statistics (e.g. variance) in the usual way. . We can plot the values of these variables for a given set of interaction weights: . # This code creates a 4 figure plot showing how # Thermodynamic variables change with temperature # For a 1d Ising Model with Gaussian interactions import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix seed np.random.seed(123) # Fix number of points N = 100 # Instantiate interaction strength J = np.random.normal(0,1,size=N-1) # Set temperature ranges T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min beta = 1 / T # Set up holders for variables F = np.zeros(T_steps) U = np.zeros(T_steps) C = np.zeros(T_steps) S = np.zeros(T_steps) # Loop over T_steps and calculate at each step for i in range(T_steps): F[i] = - T[i] * np.log(2) - T[i] * (np.log(2*np.cosh(beta[i]*J))).sum() U[i] = - (J * np.tanh(J * beta[i])).sum() C[i] = ((beta[i] * J)**2 * (np.cosh(beta[i] * J))**-2).sum() S[i] = (U[i] - F[i]) / T[i] # Divide by number of points to give a scale invariant measure F = F / N U = U / N C = C / N S = S / N # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].plot(T, F) axs[0, 0].set_title(&quot;Free Energy&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;F / N&quot;) axs[0, 1].plot(T, U) axs[0, 1].set_title(&quot;Average Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;U / N&quot;) axs[1, 0].plot(T, C) axs[1, 0].set_title(&quot;Heat Capacity&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;C / N&quot;) axs[1, 1].plot(T, S) axs[1, 1].set_title(&quot;Entropy&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;S / N&quot;) plt.show() . From these plots we can see there is no phase transition taking place - this is true for all 1d Ising models. . We can also calculate the correlation function for the system. By following a similar line of logic as before we find: $$ langle sigma_n sigma_{n+r} rangle = prod_{i=0}^{r} tanh( beta J_{n+i, n+i+1})$$ . We can plot this as a function of $r$ starting at the first position: . # This code creates a plot displaying # The correlation function # For a 1d Ising Model with Gaussian interactions import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix seed np.random.seed(123) # Fix number of points N = 100 # Instantiate interaction strength J = np.random.normal(0,1,size=N-1) # Create holders for variables r = np.arange(N) corr_array = np.zeros(N) # Fix beta beta = 1 # Calculate correlations tanh_array = np.tanh(beta * J) for i in range(N): corr_array[i] = tanh_array[:i].prod() plt.plot(r[0:20], corr_array[0:20]) plt.title(&quot;Correlation Function&quot;) plt.ylabel(r&quot;$ langle sigma_{1} sigma_{r} rangle$&quot;) plt.xlabel(&quot;r&quot;) plt.xticks(np.arange(11)*2) plt.ylim((-1,1)) plt.xlim((0,20)) plt.show() . As we can see there is a specific &quot;structure&quot; to the correlation function given an instantiation - this is not particularly surprising. If we picked an alternate starting state (e.g. the 2nd site) this graph can look totally different, the decay in absolute value will be similar however. We also notice that the correlation decays to zero fairly rapidly suggesting there isn&#39;t a long range structure to the model. . 2d Ising Model . We now turn our attention to the 2d Ising model. The mathematics here will, understandably, get more complicated. We will require a bit more sophistication to solve this system. The solution was originally published by Lars Onsager in 1944, with alternative proofs and derivations published later. The derivation itself is fairly involved and would take a blog post (at least) by itself to cover - I may come back to this at a later date. For now I will simply present the result for the free energy (F) in absence of a magnetic field ($h=0$): $$ - frac{ beta}{N} F = ln2 + frac{1}{8 pi^2} int^{2 pi}_{0} int^{2 pi}_{0} ln left[ cosh(2 beta J_H) cosh(2 beta J_V) - sinh(2 beta J_H)cos( theta_1) - sinh(2 beta J_V)cos( theta_2) right]d theta_1 d theta_2 $$ Where instead of $J_{xy}$ being sampled from a gaussian distribution we assume fixed interaction strengths $J_H$ and $J_V$ in the horizontal and vertical directions. . For simplicity we will take: $J_H = J_V = J$ and we can derive the thermodynamic properties (per site - e.g. $f= frac{F}{N}$) of this system as: begin{align} f &amp;= frac{-ln2}{2 beta} - frac{1}{2 pi beta} I_0( beta, J) u &amp;= -J coth(2 beta J) left[ 1 + frac{2}{ pi} left( 2tanh^2(2 beta J) -1 right) I_1( beta, J) right] c &amp;= 2J beta^2 left[U csch(2 beta J)sech(2 beta J) + frac{8J}{ pi} sech^2(2 beta J) I_1( beta, J) - frac{2 beta J}{ pi}(cosh(4 beta J)-3)^2 sech^6(2 beta J) I_2( beta, J) right] s &amp;= frac{U - F}{T} end{align} . For convenience I created 3 new functions $I_0( beta,J), I_1( beta, J)$ and $I_2( beta, J)$ to make the equations a little shorter. These functions are defined as: begin{align} I_0( beta, J) &amp;= int^ pi_0 ln left[ cosh^2(2 beta J) + sinh^2(2 beta J) sqrt{1 + csch^4(2 beta J) - 2 csch^2(2 beta J) cos(2 theta)} right] d theta I_1( beta, J) &amp;= int^{ frac{ pi}{2}}_0 left[1 - 4csch^2(2 beta J)( 1 + csch^2(2 beta J))^{-2} sin^2( theta) right]^{- frac{1}{2}} d theta I_2( beta, J) &amp;= int^{ frac{ pi}{2}}_0 sin^2( theta) left[1 - 4csch^2(2 beta J)( 1 + csch^2(2 beta J))^{-2} sin^2( theta) right]^{- frac{3}{2}} d theta end{align} . As before we can produce plots of these: . # This code creates a 4 figure plot showing how # Thermodynamic variables change with temperature # For a 2d Ferromagnetic Ising Model with fixed interaction import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad %matplotlib inline # Fix seed np.random.seed(123) # Instantiate interaction strength J = 1 # Set temperature ranges T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min beta = 1 / T # Set up holders for variables f = np.zeros(T_steps) u = np.zeros(T_steps) c = np.zeros(T_steps) s = np.zeros(T_steps) # Set up integrands for I0, I1, I2 def integrand0(x, b, j): return np.log(np.cosh(2*b*j)**2 + np.sinh(2*b*j)**2 * np.sqrt(1 + np.sinh(2*b*j)**(-4) - 2*np.sinh(2*b*j)**(-2) * np.cos(2*x))) def integrand1(x, b, j): return (1 - 4*np.sinh(2*b*j)**(-2)*(1 + np.sinh(2*b*J)**(-2))**(-2)*np.sin(x)**2)**(-0.5) def integrand2(x, b, j): return np.sin(x)**2 * integrand1(x, b, j)**3 # Loop over T_steps and calculate at each step for i in range(T_steps): bt = beta[i] I0 = quad(integrand0, 0, np.pi, args=(bt, J)) I1 = quad(integrand1, 0, np.pi/2, args=(bt, J)) I2 = quad(integrand2, 0, np.pi/2, args=(bt, J)) f[i] = - np.log(2) / (2 * bt) - I0[0] / (2 * np.pi * bt) u[i] = -J*np.tanh(2*bt*J)**(-1) * ( 1 + (2/np.pi)*(2*np.tanh(2*bt*J)**2 -1)*I1[0]) c[i] = 2*bt**2*J*( u[i]*np.sinh(2*bt*J)**(-1)*np.cosh(2*bt*J)**(-1) + (8*J / np.pi)*np.cosh(2*bt*J)**(-2)*I1[0] - (2*bt*J/np.pi)*((np.cosh(4*bt*J)-3)**2)*np.cosh(2*bt*J)**(-6)*I2[0] ) s[i] = (u[i] - f[i])*bt # Create plots fig, axs = plt.subplots(2, 2, figsize=(10,10), gridspec_kw={&#39;hspace&#39;: 0.25, &#39;wspace&#39;: 0.25}) axs[0, 0].plot(T, f) axs[0, 0].set_title(&quot;Free Energy&quot;) axs[0, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;f&quot;) axs[0, 1].plot(T, u) axs[0, 1].set_title(&quot;Average Energy&quot;) axs[0, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;u&quot;) axs[1, 0].plot(T, c) axs[1, 0].set_title(&quot;Heat Capacity&quot;) axs[1, 0].set(xlabel=&quot;T&quot;, ylabel=&quot;c&quot;) axs[1, 1].plot(T, s) axs[1, 1].set_title(&quot;Entropy&quot;) axs[1, 1].set(xlabel=&quot;T&quot;, ylabel=&quot;s&quot;) plt.show() . We find there is a critical temperature $T_c$ where the specific heat equation diverges. We can compute the value of this as satisfying: $$ sinh left( frac{2J_H}{T_c} right)sinh left( frac{2J_V}{T_c} right) = 1 $$ In the case where $J_H = J_V = J$ we have: $$T_c = frac{2 J}{ln left(1+ sqrt{2} right)} $$ This is an example of a second order phase transition since the discontinuity only arises under a second derivative. . For temperatres under this critical temperature we have that the spontaneous magnetization can be calculated as: $$ m = left[ 1 - csch^2 left( frac{2J_H}{T} right)csch^2 left( frac{2J_V}{T} right) right]^{ frac{1}{8}} $$ . We can plot this (for $J_H = J_V = J = 1$) as: . # This code plots the spotaneous magnetization of # a 2d Ising model on a square-lattice import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set up temperature array T_min = 1e-4 T_max = 5 T_steps = 1000 T = np.arange(T_steps)/T_steps *(T_max - T_min) + T_min # Fix interaction strength constant J = 1 m = np.power(np.maximum(1 - np.sinh(2*J/T)**-4,0), 1/8) plt.plot(T,m) plt.xlabel(&quot;T&quot;) plt.ylabel(&quot;Magnetization&quot;) plt.title(&quot;2d Ising Model Spontaneous Magnetization&quot;) plt.xlim((0,5)) plt.show() print(&quot;Critical Temp (Tc):&quot;, (2*J) / np.log(1 + np.sqrt(2))) . Critical Temp (Tc): 2.269185314213022 . The correlation function is harder to compute, here we just present the correlation function between elements on the diagonal of the lattice: . $$ langle sigma_{0,0} sigma_{N,N} rangle = det left[ A_N right]$$ Where $A_N = (a_{n,m})_{n,m=1}^N$ is an $NxN$ matrix with entries: $$ a_{n,m} = frac{1}{2 pi} int_0^{2 pi} e^{i(n-m) theta} sqrt{ frac{sinh^2(2 beta J) - e^{-i theta}}{sinh^2(2 beta J) - e^{i theta}} } d theta $$ . We can plot this as so: . # This code plots the diagnoal correlation function # For the 2d square-lattice Ising model # Lattice size = NxN # At the critical temperature import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad %matplotlib inline # Set up integrand function imag = complex(0,1) def integrand(x, n, m, beta, J): return np.exp(imag*(n-m)*x)*np.sqrt((np.sinh(2*beta*J)**2-np.exp(-imag*x))/(np.sinh(2*beta*J)**2-np.exp(imag*x))) # Set up constants J = 1 beta = np.log(1 + np.sqrt(2)) / (2*J) # Set up arrays N_max = 20 N_array = np.arange(N_max+1) correl_array = np.zeros(N_max+1) for x in range(N_max + 1): N = N_array[x] A_N = np.zeros((N, N)) for n in range(N): for m in range(N): I = quad(integrand, 0, 2*np.pi, args=(n, m, beta, J)) A_N[n,m] = I[0]/(2*np.pi) correl_array[x] = np.linalg.det(A_N) # Plot correlations plt.plot(N_array, correl_array) plt.xlabel(&quot;N&quot;) plt.ylabel(r&quot;$ langle sigma_{0,0} sigma_{N,N} rangle$&quot;) plt.title(&quot;Correlation Function&quot;) plt.xticks(np.arange(11)*2) plt.ylim((0,1)) plt.xlim((0,20)) plt.show() . We can see that even though we have short range interactions (nearest neighbour) this gives rise to longer-range structure within the model. In fact we find for temperatures below the critical temperature there is an infinite correlation length, whereas for temperatures above the critical temperature the correlation length is finite. . Infinte Dimension Ising Model . Next we consider the situation where the Ising model exists in infinite dimensions. In this case each site has infinitely many neighbours, as such a mean-field approximation is valid and we have a model that is somewhat similar to the Sherrington-Kirkpatrick fully connected geometry. (Please excuse the lack of rigour here; one has to be careful in how limits are defined and it is a non-trivial exercise. For this blog post I don&#39;t want to go down into that sort of detail.) . If each site has infinitely many neighbours we only need to concern ourselves with the ratio of positive and negative spin neighbours. By mean field if we take the probability of a positive spin as $p$ then via the Gibbs distribution we have: $$ frac{p}{1-p} = exp(2 beta H)$$ The average magnetization can then be calculated: $$ mathbb{E} left[M right] = (1)p + (-1)(1-p) = 2p - 1 = frac{1 - exp(2 beta H)}{1 + exp(2 beta H)} = tanh(2 beta H)$$ . By investigating this function we can gain insight into spontaenous magnetization. Other similar arguments can be invoked for the other themodynamic properties. It is possible to show, as with the Sherrington-Kirkpatrick, that a phase transition occurs. . n-d Ising Model ($n geq3$) . Finally we look at the case where the dimension of the model is finite but strictly greater than 2. In this situation things get much trickier, in fact there are not many defined mathematical results in these systems and this is the subject of current research. As such I will just briefly outline some of the approaches that have been suggested to study these systems and their properties (presented in chronological order from when they were proposed): . Replica-Symmetry Breaking - From our previous post on Sherrington-Kirkpatrick models we briefly looked at this sort of method. They were an obvious first choice in trying to deal with short range interactions. It has been shown that the &quot;standard&quot; techniques are insufficient but there have been extensions proposed that have shown some promise. Like in the infinite range model it suggests states have an ultrametric structure and uncountably many pure states. | Droplet Scaling - The first such argument being presented by Rudolf Peierls. The main idea is to consider the arisal of &quot;loops&quot; or &quot;islands&quot; of spins (clusters of atoms all with the same spin being enclosed by some boundary). We then aim to count the number of such loops, often in high or low temperature ranges via approximation. This leads to only 2 pure states. | Chaotic Pairs - Has properties somewhat similar to a combination of the preceeding 2 methods, like replica symmetry breaking there are infinitely many thermodynamic states however the relationship between them is much simpler and has simple scaling behaviour - much like droplet scaling. | TNT - This interpretation does not itself specify the number of pure states, however it has been argued that it most naturally exists with 2 pure states - much like droplet scaling. However it has scaling properties more similar to that of replica symmetry breaking. | . For completeness: a pure state of a system is a a state that cannot be expressed as a convex combination of other states. . As far as I am aware there is currently no conesensus on which (if any) of the options presented is the correct interpretation. . One of the main questions that we would like to answer is whether there is a phase transition (first order). This is unresolved currently. Another question we might ask is whether there exists multiple ground state pairs (i.e. we can find 2 different configurations that have minimal energy that are not merely negative images of each other) - again this is unresolved. In infinite dimensions we can see this would be true, in 1d we know this cannot be true - for other dimensions it is unclear (although it is believed it is probably not true for d=2). . In addition to this there are many other unanswered questions that are the subject of current research into Ising type models. . Conclusion . In this blog post we have investigated some of the properties of square-lattice Ising models in various dimensions. In particular we have seen that there is no phase transition in 1d, a second order phase tranisition in 2d, a phase transition in infinite dimension and currently other dimensions are unresolved. We can see that the short-range interactions cause a lot of headaches when trying to analyse these systems. In the next blog post in this series we will begin to look at ways of simulating Ising models (and other spin glass models generally). . . This is the third blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-3",
            "relUrl": "/spin-glass-models-3",
            "date": " â€¢ Mar 17, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Spin Glass Models 2: Sherrington-Kirkpatrick",
            "content": ". This is the second blog post in a series - you can find the previous blog post here . . From a previous blog post we now have a reasonable understanding of what a spin glass is, some of the (frankly) bizarre behaviours they exhibit and some motivation behind why we might want to wish to study them. In this blog post we will embark on studying our first spin glass model due to Sherrington-Kirkpatrick. (Note: this model is sometimes also called the fully-connected Ising model.) . For our purposes we will just consider finding the properties and behaviours of interest rather than trying to capture exact behaviour of specific materials. As such we will assume measurement units are such that any constants &quot;disappear&quot; this should help with clarity. It is worth keeping this in mind if you look at any research on the topic (particularly in Physics journals) where extra terms may appear, these are typically to correct for units (e.g. to get energy measurements in Joules, distances in metres, etc.) . It is also worth noting that this model was not the first spin glass model and some other models appeared before this one. I have chosen to start with this as it is in some ways the &quot;simplest&quot; model. . Simplification . As with any good model we want to simplify the real world to a set of minimal requirements to capture the behaviour of interest. Recall the Hamiltonian for a general spin glass as: $$ H = - sum_{x,y} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ In a real world spin glass $ sigma$ represent spins in the form of vectors denoting the direction the magnetic moment faces. It turns out that allowing for all range of orientation of spins is unnecessary to observe interesting behaviour. For modelling purposes $ sigma_x = pm 1$ is usually sufficient. . Turning our attention to interacting pairs now, we can simplify significantly here by assuming a fully connected model. That is each atom interacts with every other. This is particularly useful since it allows us to use a range of mathematical &quot;tricks&quot; through mean field type approaches. This is the key to the Sherrington-Kirkpatrick model and what differentiates it from other models (which we will review in a later blog post). . This leaves us with one final assumption to make (excluding the external magnetic field): what values should $J_{xy}$ take. There are two options that we could use here to make our lives easier: the first is $J_{xy} = pm frac{J}{ sqrt{N}}$ - that is we select each interaction to be positive (ferromagnetic) or negative (antiferromagnetic) at random. Another option is to take values via a continuous probability distribution, typically a normal distribution owing to its &quot;nice&quot; mathematical properties. Typically we will want a mean of zero since we do not want the material to exhibit any magnetism in a ground state. We want to scale the standard deviation by $ frac{1}{ sqrt{N}}$ as in the bernoulli case. If we are using simulation we could try more &quot;exotic&quot; distributions (such as double fat-tail, assymetric distributions, etc) The scalings applied to the interaction weights is merely a convenience to allow us to scale the size of the spin glass in such a way as to allow population averages to remain comparable between glass size. . We will thus define the Sherrington-Kirkpatrick Hamiltonian as: $$ H_N = - sum_{x,y} J_{xy} sigma_x sigma_y $$ With $J_{xy} sim N(0, frac{s^2}{N})$ iid (value of $s$ is somewhat irrelevant as long as it&#39;s not too large/small). We have removed the external magnetic field as it isn&#39;t crucial for the story we are telling here. . An Alternate Interpretation . For those that are finding spin glasses and magnet terminology a little confusing or alien for this particular model we can introduce another interpretation in the form of the &quot;Dean Problem&quot;. Imagine you are the dean of a college hall, you have a number of students ($x, y$) who can like or dislike each other ($J_{xy}$). Your job is to place students in one of two groups ($ sigma = pm 1$) so as to maximise the overall happiness of the students ($ sum J_{xy} sigma_x sigma_y$). We can note that this maximization problem is equivalent to the energy minimization problem presented by the spin glass (i.e. the minimization of a negative quantity is equivalent to the maximization of its modulus) . Mathematical Analysis . The nice thing about the assumptions made by Sherrington-Kirkpatrick is that it allows for (comparatively) easy mathematical analysis. This is largely due its regularity (every atom looks like every other), by assuming arbitrarily large spin glasses we can also take limits and look at asymptotic behaviour. In other models this is not always possible and if it is it becomes increasingly more difficult. . First question we will ask is what is the minimum value attained by the Hamiltonian? This is equivalent to: $$M_N = max_{ sigma} sum J_{xy} sigma_x sigma_y = max_{ sigma} left[ - H_N right] $$ Where we are considering $N$ atoms in the system. We are using a slighlty sloppy notation for $max_{ sigma}$ to represent the maximum over all possible configurations. . Studying maximum quantities mathematically is often difficult. To overcome this difficulty we instead look at the Helmholtz free energy function ($F_N$) instead: $$F_N( beta) = frac{1}{N beta} mathbb{E} left[ log sum_{ sigma} exp(- beta H_N) right] $$ . We have done this since we can write the following inequality: $$ frac{1}{N} mathbb{E} left[ M_N right] leq F_N( beta) leq frac{1}{N} mathbb{E} left[ M_N right] + frac{log(2)}{ beta} $$ . This is a deceptively simple inequality, to see why it holds for the lower bound we replace the summation in $F_N$ by the maximum value in the sum (1 term). For the upper bound we replace every term in the sum with this attained maximum ($2^N$ values). The logs/exponents/beta/etc. all cancel leaving the result. This is useful to us because it means in the limit $ beta to infty$ we get the relation: $F_N( beta) to frac{1}{N} mathbb{E} left[ M_N right] $ which is what we are interested in studying. . This is closely related to the Gibbs distribution of the system. This gives us a probability distribution of states of the spin glass: $$G_N( sigma) = frac{exp(- beta H_N( sigma))}{Z_N( beta)} $$ Where $Z_N( beta)$ is the partition function, it is a normalizing constant (i.e. it is a sum over all possible terms of the numerator of $G_N$). We can think of the Gibbs distribution as weighting the configurations according to their free energy. We can see that finding the partition function is the crux of understanding the Gibbs distribution (and through comparison to the free energy the ground states). . One of the first ways this was investigated mathematically was to use a &quot;replica trick&quot;, this is just a result of using the identity: $$ln(Z) = lim_{n to 0} frac{Z^n - 1}{n} $$ . On the partition function. Essentially one takes $n$ independent copies (replicas) of the system and computes an average over all of them. Approximations are then used to take the limit $n$ to zero. For certain system behaviours this method works well but for others it can be inaccurate. In particular looking at very low temperatures (small beta near ground state) the approximations do not perform well. This method assumes certain symmetries between atoms which are not true in practice (due to taking independent replicas and averaging), these methods have been extended to &quot;replica symmetry breaking&quot; (RSB) methods. These methods were further superceded by the work of Parisi using variational principles. The mathematical details would take too long to put in a blog like this. Please see the references for links to papers on the topic. . What we are really interested in with spin glass systems is when a phase transition occurs. To do this physicists look at an order parameter which captures all behaviours of the system. Edwards and Anderson suggested the following order parameter: $$ q = frac{1}{N} sum_x hat{ sigma}_x^2 $$ Where $ hat{ sigma}_x$ represents the average over time of spin $x$. This order parameter is such that for $q=0$ the configuration is non-magnetic. For $q&gt;0$ then it is in a spin glass phase. Using the replica method (and some work!) we can show that under full symmetry we have that $q$ satisfying the self-consistency formula: $$ q = 1 - frac{1}{ sqrt{2 pi}} int_{- infty}^{ infty} exp(-z^2/2) sech^2( beta s sqrt{q} z) dz $$ Using this we can find a phase transition occurs at $T=s$ - temperatures below this the system exhibits glassy behaviour and above this the system is not magnetic (in equilibrium). . Unfortunately assuming this sort of symmetry this $q$ does not behave well at lower temperatures, it does not display all the characteristics of the system. . If we introduce symmetry breaking we re-write the order parameter of the form: $$ q_{ alpha beta} = frac{1}{N} sum_x hat{ sigma}^{ alpha}_x hat{ sigma}^{ beta}_x$$ For two states $ alpha$ and $ beta$ - This is also sometimes called the &quot;spin overlap function&quot;. If we consider the likelihood of observing a system state $ alpha$ as $W_{ alpha}$ (so that $ sum_{ alpha} W_{ alpha} = 1$) we can define the overlap density: $$P_ tilde{J}(q) = sum_{ alpha beta} W_{ alpha}W_ beta delta(q - q_{ alpha beta})$$ Where $ delta(.)$ is the Dirac delta function and the density is defined for some fixed realisation of interaction strengths $ tilde{J}$. Finally we can use this to define the Parisi order parameter function as: $$P(q) = int prod_{xy} Q(J_{xy}) P_ tilde{J}(q) dJ_{xy}$$ With $Q(J_{xy})$ representing the density by which the interaction strength is chosen (e.g. Gaussian). This order parameter does not suffer from the issues of symmetry like the order parameter function above. . In addition to uncovering a phase transition, the new Parisi order parameter uncovers some other interesting properties: . The broken symmetry of the spin glass requires an infinite number of order parameters to characterize | In the limit of large-N there is no self-averaging in the spin glass state. That is there exists distinct samples even as N approaches infinity. This is unlike most other systems where there is no concept of a &quot;sample&quot; when N increases. | Given 2 states of the spin glass, there is no &quot;inbetween&quot; spin glass state - this is called ultrametric structure. This gives the space of spin glass states a very interesting structure. In some sense the states are clustered. More than that it is clustered at any scale you look at - if you look at states within a distance of $d$ of each other you get a number of clusters, if you look at a larger scale $d&#39; &gt; d$ then these small clusters will merge into larger ones. (Distance here is defined using overlap of states). | . Simulation . After all the theory we will now look at a simple simulation of this model. . We start by doing a very naive Monte-Carlo method - we will generate random configurations, calculate the energy and a Gibbs measure. With the results we will estimate the average energy, the average magnetism, ground state energy and the partition function. If we want to find the ground state this would be a very bad method, for an $N$ size spin glass there will be $2^N$ possible configurations so finding any one will be difficult! We can implement this: . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Very naive Monte-Carlo approach import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N N = 1000 # Fix number of timesteps and some containers timesteps = 10000 gibbs = np.zeros(timesteps) energy = np.zeros(timesteps) mag = np.zeros(timesteps) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Fix Temperature for Gibbs distribution beta = 1/(s*0.5) for i in range(timesteps): configuration = np.random.choice([-1, 1], N) energy[i] = -1 * np.dot(configuration, np.dot(configuration, interaction)) / 2 gibbs[i] = np.exp(-beta*energy[i]) mag[i] = configuration.sum() print(&quot;Estimated Ground State Energy: &quot;, energy.min()) print(&quot;Estimated Average Energy:&quot;, energy.mean()) print(&quot;Estimated Partition Function:&quot;, gibbs.mean()) print(&quot;Estimated Average Magnetism:&quot;, mag.mean()) . Estimated Ground State Energy: -78.55263447711921 Estimated Average Energy: 0.1314391563670086 Estimated Partition Function: 1.7717002620362574e+64 Estimated Average Magnetism: 0.5468 . Next we take another bad method: a greedy hill climber (or greedy gradient descent). The idea behind this algorithm is that when we are in one configuration we pick a site at random and look at the change in energy associated with flipping the spin. If the energy is lower we accept the change, if higher we ignore and pick another site at random. This algorithm will converge to some local minima but it will not necessarily be a good global minima. We can code this up as: . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Greedy gradient descent import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 1000 spins = np.random.choice([-1, 1], N) # Fix number of timesteps and some containers timesteps = 100000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Calculate initial values mag[0] = spins.sum() energy[0] = -1 * np.dot(spins, np.dot(spins, interaction)) / 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1/(0.5*s) # Define update step dE = 0 dM = 0 def update(s_array, i_array): &quot;&quot;&quot; update function performs 1 update step to the model inputs: s_array - an array of N spins (+-1) i_array - an array of interaction strengths NxN &quot;&quot;&quot; global dE global dM _N = s_array.shape[0] old_s = s_array.copy() # Select a spin to update site = np.random.choice(_N, 1)[0] # Get interaction vector i_vector = i_array[site,:] # Calculate energy change associated with flipping site spin dE = 2*np.dot(i_vector, s_array)*s_array[site] dM = -2*s_array[site] # Sample random number and update site if dE &lt;= 0: s_array[site] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts , s_array, i_array): s_temp = s_array.copy() for i in range(ts): update_step = update(s_temp, i_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, interaction) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . We can see after about 25,000 steps the system is &quot;stuck&quot; in a local energy minima. If we re-ran the code starting from a different spot we would likely end up in a vastly different configuration. . The last method we will look at is the Metropolis-Hastings algorithm. Given a configuration of spins we will perform an update step by picking a site at random, we will compute the probability of flipping the spin and then accept/reject this change based on a random draw. This process will be repeated for a set number of steps. We will keep track of the energy of the system (the Hamiltonian) and the overall magnetism ($ sum_x sigma_x$). . # An implementation of a Sherrington-Kirkpatrick spin-glass of size N # Connectivity is initialized as a Gaussian distribution N(0, s^2/N) # Updates occur at randomly selected sites import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Fix random seed np.random.seed(123) # Set size of model N and initial spins N = 1000 spins = np.random.choice([-1, 1], N) # Fix number of timesteps and some containers timesteps = 100000 mag = np.zeros(timesteps+1) energy = np.zeros(timesteps+1) # Initialize interaction array s = 1 interaction = np.zeros((N, N)) for i in range(N): for j in range(i): interaction[i, j] = np.random.randn() * s / np.sqrt(N) interaction[j, i] = interaction[i, j] # Calculate initial values mag[0] = spins.sum() energy[0] = -1 * np.dot(spins, np.dot(spins, interaction)) / 2 # Fix beta (inverse temerature) - from analysis we know that # system in glassy-phase for T&lt;s so beta&gt;1/s. Performance # of random updates isn&#39;t good so don&#39;t select temperature # too low beta = 1/(0.75*s) # Define update step dE = 0 dM = 0 def update(s_array, i_array): &quot;&quot;&quot; update function performs 1 update step to the model inputs: s_array - an array of N spins (+-1) i_array - an array of interaction strengths NxN &quot;&quot;&quot; global dE global dM _N = s_array.shape[0] old_s = s_array.copy() # Select a spin to update site = np.random.choice(_N, 1)[0] # Get interaction vector i_vector = i_array[site,:] # Calculate energy change associated with flipping site spin dE = 2*np.dot(i_vector, s_array)*s_array[site] dM = -2*s_array[site] # Calculate gibbs probability of flip prob = np.exp(-beta*dE) # Sample random number and update site if dE &lt;= 0 or prob &gt; np.random.random(): s_array[site] *= -1 else: dE = 0 dM = 0 return s_array def _main_loop(ts , s_array, i_array): s_temp = s_array.copy() for i in range(ts): update_step = update(s_temp, i_array) s_temp = update_step energy[i+1] = energy[i] + dE mag[i+1] = mag[i] + dM #### Run Main Loop _main_loop(timesteps, spins, interaction) # plot magnetism and energy evolving in time fig, ax1 = plt.subplots() ax1.set_xlabel(&quot;Time step&quot;) ax1.set_ylabel(&quot;Magnetism&quot;, color=&#39;blue&#39;) ax1.plot(mag, color=&#39;blue&#39;) ax2 = ax1.twinx() ax2.set_ylabel(&quot;Energy&quot;, color=&#39;red&#39;) ax2.plot(energy, color=&#39;red&#39;) plt.show() . In this code we can see in the beginning the magnetism of the system fluctating and the energy decreasing. This stablises to some sort of &quot;quasi-equilibrium&quot;. . Since the energy always decreases it suggests the system is slowly finding its way to a local energy minimum rather than exploring to find a better energy minima. This is to be expected with such a basic implementation. To observe this better we will re-run the code for a second time from a different starting spot, we will compare the resulting spin array - we should notice that there is quite a large discrepency between the 2 runs - this shows that the system is settling down to a different local energy minima. . old_spins = spins old_energy = energy[timesteps] old_mag = mag[timesteps] spins = np.random.choice([-1, 1], N) #### Run Main Loop _main_loop(timesteps, spins, interaction) # Calculate a distance metric dist = ((old_spins * spins).sum() / N + 1) / 2 print(&quot;Proportion of sites with the same spin is:&quot;, dist) print(&quot;Resting energies of the 2 systems are:&quot;, old_energy, &quot;and:&quot;, energy[timesteps]) print(&quot;Resting magnetism of the 2 systems are:&quot;, old_mag, &quot;and:&quot;, mag[timesteps]) . Proportion of sites with the same spin is: 0.508 Resting energies of the 2 systems are: -608.7546315240614 and: -589.7510068842638 Resting magnetism of the 2 systems are: -84.0 and: 56.0 . The proportion of sites having the same spin is high (around 50%!) and the energy attained is different, suggesting the system has converged to 2 different local minima that are &quot;far apart&quot; from each other. If we want to find the global minima (or at least a &quot;better&quot; minima) we will have to adopt a better strategy. We will introduce some options when we look at the next spin glass model. The code above should act as a warning about blindly simulating and relying on computational power/time to solve complex problems: it doesn&#39;t always work! . We&#39;ll finish this blog post by looking at the Edwards-Anderson order parameter. We will use a basic numerical technique to solve the self consistency integral equation. . from scipy.integrate import quad def integrand(x, c): return np.exp(-x**2/2)*np.cosh(c*x)**(-2) n_approx = 100 beta_min = 0 beta_max = 2 beta_array = np.arange(n_approx + 1)*(beta_max - beta_min)/n_approx + beta_min q_array = np.zeros(n_approx+1) thresh = 0.001 n_max = 100 for i in range(n_approx+1): beta_tmp = beta_array[i] q_old = 0 q_tmp = 1 j = 0 while np.abs(q_old - q_tmp) &gt; thresh and j &lt; n_max: q_old = q_tmp c = beta_tmp*s*np.sqrt(q_old) I = quad(integrand, -np.inf, np.inf, args=(c)) q_tmp = 1 - I[0] / (np.sqrt(2*np.pi)) j =+ 1 q_array[i] = q_tmp plt.plot(beta_array, q_array) plt.xlabel(r&quot;$ beta$&quot;) plt.ylabel(&quot;q&quot;) plt.title(&quot;Edwards-Anderson Order Parameter (s=1)&quot;) plt.show() . This displays the behaviour we expect (approximately) for low beta below $1/s$ the temperature is above $s$ and so $q=0$ (non-magnetic) above this point the system is in the glassy phase. Since we have only approximated here there is some noise around the transition point. As we add more approximation points the transition should become sharper at $ beta = 1/s$. . Conclusion . In this blog post we have introduced the assumptions of the Sherrington-Kirkpatrick (fully connected Ising) spin glass model. We have seen that although fairly involved we can &quot;solve&quot; this model analytically to uncover its properties. We have also a basic implementation in Python - however as we noted this has bad convergence properties so shouldn&#39;t really be used other than for illustration. . References . This blog post was inspired by chapter 5 of &quot;Spin Glasses and Complexity&quot; by Daniel L Stein and Charles M Newman. . Some relevant papers include: . The original paper: &quot;Solvable Model of a Spin-Glass&quot; - Sherrington, Kirkpatrick 1975 | Summary of Parisi Method: &quot;The Sherrington-Kirkpatrick model: an overview&quot; - Panchenko 2012 | . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models-2",
            "relUrl": "/spin-glass-models-2",
            "date": " â€¢ Mar 10, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Spin Glass Models",
            "content": "What is a Spin Glass? . A spin glass is not something you find down the pub (well it could be, but thats not what we&#39;re talking about here). Instead spin glasses are models of certain magnetic materials. Very loosely we can think of the atoms of a magnetic material as having a &quot;spin&quot; relating to the magnetic polarity (&quot;north&quot; or &quot;south&quot; ends of a bar magnet) - we typically call these up-spin and down-spin. In this context we use the term &quot;atom&quot; loosely, it may be an atom in a chemical sense but it could also be a molecule - essentially a minimal element of the material. . In a ferromagnetic material (such as iron) spins orient in the same direction. In contrast antiferromagnetic materials the spins orient to oppose each other. In a spin glass we have some ferromagnetic interactions and some antiferromagnetic interactions, we say the system is &quot;disordered&quot;. . . In the image above we can see various spin configurations. The spins are indicated by arrows (yellow for up and green for down) the ferromagnetic interactions are denoted by blue lines while antiferromagnetic interactions by red. However it is somewhat misleading to think of these occuring only within a square lattice in 2d such as this. Spins also do not have to be 180 degree rotations of each other - they can be at arbitrary angles from each other (but this would make for a messy diagram). A spin glass can also occur in arbitrarily many dimensions, and the interactions do not only have to occur between &quot;nearest neighbours&quot; any atom can have any number of interacting partners. . We can represent the energy of a spin glass system using the Hamiltonian: $$ H = - sum_{x,y} J_{xy} sigma_x sigma_y - h sum_x sigma_x $$ Where $J_{xy}$ denotes an interaction strength between atoms $x$ and $y$. $ sigma_x$ represents the spin/magnetism of an atom $x$ (can be a vector in which case all products are dot products). A system will tend to a state of lowest energy and so $J_{xy} &gt; 0$ represents a ferromagnetic interaction (minimised for pairs of matching spin) and $J_{xy} &lt; 0$ for antiferromagnetic interactions. The range over which the sum applies has been left purposely ambiguous as to allow for various lattice topologies. The second summation reflects an interaction with an external magnetic field (denoted by $h$) if there is no external field the term can be ignored. (It is worth noting that magnetic spins for a specific atom can only take one of two values (rotated through 180 degrees) since electrons have half integer spin) . This allows us to define an important term relating to spin glasses. We can note that at a minimal energy (&quot;ground state&quot;) the spin glass will not be &quot;ordered&quot; (it will appear &quot;random&quot;). We call this property &quot;quenched disorder&quot; - this is due to the similarity to glass materials that are essentially cooled down liquids that get &quot;frozen&quot; into a state of disorder. . It is important to note this is different to pure &quot;randomness&quot;. If we think of a continuum whereby we have complete order on one side (think of a crystalline type structure as an example) and pure randomness on the other the spin glass lives somewhere in the middle - partially structured and ordered but partially random. This is a particularly interesting place to be: in the pure ordered end of the scale there are many established mathematical tools to deal with this situation. Similarly in a pure random situation probability and statistics can provide us tools for study. In the middle things get complicated since you cannot necessarily assume that any one element will &quot;look the same&quot; as any other - thus mean field methods fall down. This occurs in many &quot;real world&quot; phenomena, this can result in a lack of study since it &quot;falls inbetween&quot; different disciplines. . . There is another important term relating to spin glasses called &quot;frustration&quot;. This is where an atom has interactions with other atoms that are in conflict with each other - one interaction would suggest a lowest energy state for the atom is an up spin and another interaction suggests a down spin. An example of this can be seen below: . . We can see that the spin of the centre atom is not clearly defined by interaction with its neighbours. The vertical neighbours interactions suggest the lowest energy state would be a yellow up spin, while horizontal neighbours suggest a green down spin. In a large spin glass with random (or nearly random) configuartions there may be many such frustrated atoms. This gives rise to complexity and questions such as &quot;what is the lowest energy state for a system?&quot; becomes very difficult to answer. In many cases we are not able to determine this analytically. The energy landscape (the Hamiltonian energy for a given configuration of spins given a fixed topology of interactions) can become very complex with lots of local minima, which means &quot;typical&quot; optimization procedures based around greedy hill climbing (and the like) will struggle to find the global minimum. See the plot of energy landscape below as an example: . . If one ends up in a configuration near one of these local minima it requires relatively large changes to the configuration to escape the valley. This leads to a kind of &quot;metastability&quot; in the system where the configuration will &quot;stick&quot; around these points for a long time. As such spin glasses tend to violate the Ergodic principle, this again adds to the mathematical complication in dealing with these systems. . Although the system would &quot;prefer&quot; to be in a lower energy state through the application of a temperature (or placing the system within an external magnetic field) the atoms can have sufficient energy to escape this lower energy state. For high enough temperatures this means a ferromagnetic material can become antiferromagnetic. In most cases there exists a critical temperature where a phase transition occurs. Phase transitions are interesting examples of emergence - one example of a phase transition that everybody is familiar with is the phenomena of melting a solid to create a liquid. It is interesting that this is a very sharp transition - why is it not the case that a solid gradually becomes &quot;softer&quot; and more liquid like? Instead small temperature fluctuations can cause the state of matter to change. It is not immediately obvious why this is the case, other phase transitions exist in other systems and they are often interesting to analyse. . The eagle eyed amongst you may notice that we have ignored the interference between spins themselves. It is true that this will have an impact but in most mathematical models of spin glasses it can be ignored. As with all mathematical models we look for a &quot;minimal description&quot; that captures the behaviour of interest, it turns out that this complication tends not to add much to the model (although I&#39;m sure there exists research with interesting results capturing this interference). . So what are some physical examples of a spin glass in the real world? Technically any iron magnet subject to rust (which is antiferromagnetic) will be a spin glass, however typically the ferromagnetic atoms will still be so prevelent that we can think of it as a ferromagnet. There are other &quot;exotic&quot; molecules (e.g. europium strontium sulphide) that are spin glasses also. Many of the experiments on spin glasses involve melting down a noble metal (e.g. gold or silver) and adding a small amount of dispersed molten iron (typically around 0.1-5%) and cooling the mixture very quickly. Many counter-intuitive and contradictory properties have been found through these experiments including: . By cooling quickly one can avoid the transition from liquid to solid - creating a viscous liquid spin glass | Relaxation times (how long it takes the system to adjust to changes in temperature) can be very slow, way beyond experimental time frames | Interactions with magnetic fields are odd. Absent of a magnetic field a spin glass is not magnetic. By carefully applying and removing external magnetic fields one can create a magnetic spin glass with varying properties (decays, apparent permanence etc.) | Spin glasses appear to have a &quot;memory&quot; of previous states and undergo something akin to an aging process Creating theoretical explanations of these (and other) phenomena is the subject of much research on the subject. | . Why do we care? . Ok, so at this point we may have a base understanding of what a spin glass is and some of the complications and properties therein, but you may be thinking: &quot;but who cares about magnets anyway?&quot; (Unless of course you are Charlie Kelly) It does seem like a lot of work and as a non-physicist it might seem interesting. However in dealing with the complications of spin glasses we can gain a lot of insight into other systems. In the next few bullet points I will try and convince you that it is worth time playing around with spin glasses: . They&#39;re interesting! - Spin glasses exhibit a number of properties that I personally find very interesting, for example: emergent behaviour, &quot;in between&quot; order and randomness, simple concepts to explain but difficult to write down mathematically, etc. | Non-ergodic systems are everywhere! - Although spin glasses themselves are quite stylised if they can give insight into the behaviour of non-Ergodic systems this is very useful. Loosely speaking an Ergodic system does not exhibit path depedence (e.g. whatever the state at present eventually any other state can be reached). When looking at complex systems this is typically not the case. | Frustration occurs more than we would like - We often end up in the situation with &quot;conflicting&quot; information and dealing with this gives rise to many opportunities. | They&#39;re easy to simulate - while some of the properties above make mathematical analysis difficult in all but a few special cases, spin glasses are fairly easy to code up and simulate. If you wonder &quot;what would happen if....?&quot; you can quickly modify a model and play around to see what happens, you don&#39;t need to spend much time thinking about boundary/initial conditions or other technical aspects. | There are many different applications - Given the ubiquity of some of the complications relating to spin glasses the techniques and theory have been applied to many situations including (but not limited to): optimization techniques, neural networks (biological and artificial), machine learning, protein folding, materials science, evolutionary models. The study of quantum spin glasses is also fairly active with applications in quantum computing. | . Conclusion . In this blog post we have been on a whistle-stop tour of the very basic concepts of spin glasses and models of spin glasses. We have seen some of the difficulties with them and what makes them interesting and useful to study. In future blog posts we will look at specific models and mathematical techniques used to study them. . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/spin-glass-models",
            "relUrl": "/spin-glass-models",
            "date": " â€¢ Mar 3, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Standing Ovation Model",
            "content": "Standing Ovations - as a Phenomena . Although not particularly &quot;exciting&quot; for a subject to model, standing ovations have many interesting properties. A truly fantastic performance may elicit an instantaneous standing ovation whereby all audience members instinctively decide to stand. However this cannot be the only factor, we can observe in other performances a few hardcore fans may initially stand and then pass through the crowd as a &quot;wave&quot; through social pressure (or perhaps the inability to see the stage if others in front of you stand!) Eventually resulting in a standing ovation. At other times maybe only a handful of audience members stand, notice very few people around them are standing and then sheepishly sit down again. There is a clear temporal aspect to standing ovations but there is also some sense of criticality whereby a phase transition occurs. While understanding the mechanisms behind this for standing ovations may not have much &quot;value&quot; in of itself these principles turn up in many complex systems and so understanding here can be transferable to more &quot;interesting&quot; situations. . Modelling Standard Ovations . We now move onto the question: how can we model standing ovations? We first consider the initial (instinctive) decision of standing or not following a performance. Clearly for a &quot;perfect&quot; performance (however that may be defined) everybody would show their appreciation and stand. However when the performance is not &quot;perfect&quot; why do some stand and some not? We could model each audience member as having some &quot;error function&quot; that clouds their judgement of the performance, so for (objectively) a good performance we could have that some percieve the performance as bad owing to their error function, and so they decide not to stand. We can express this mathematically: we denote the performance quality as $P$. Each individual has an error-rate $ epsilon$ - this defines a signal $S$ as: $$ S = P + epsilon $$ We can then state that an individual will stand for a signal that is in excess of their threshold $T$. We can take this threshold to be fixed for all individuals by adjusting their $ epsilon$ accordingly. . But what does the error-rate represent? There are a number of interpretations. For example it could represent &quot;stubborness&quot;, some may be difficult to impress. It could also represent differences in knowledge, somebody unfamiliar with jazz-club etiquette may stand/stay seated at the wrong times whereas somebody more seasoned will be more closely tied to the quality of the performance. . For arguments sake let&#39;s say that $P$ can vary from 0 to 1. The error rates are uniformly distributed. We can then code up this initial standing ovation model as below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set audience size N = 50 M = 100 # Fix performance quality as 50% P = 0.5 # Fix seed np.random.seed(123) # Fix all audience as seated (0) initially audience = np.zeros((N, M)) # Set error functions for audience members epsilon = np.random.random((N, M)) - 0.5 def initial_stand(error, p, t): signal = p + error out = signal &gt; t return out * 1 # We set up an array to vary the threshold T = np.arange(100) / 100 Pct = np.zeros(100) # Loop over all thresholds and plot the percentage of audience standing for i in range(100): Pct[i] = initial_stand(epsilon, P, T[i]).mean() plt.plot(T, Pct) plt.xlabel(&quot;Threshold&quot;) plt.ylabel(&quot;Percent Standing&quot;) plt.show() . As expected we get a straight line decreasing with threshold. From the discussion before we know that standing ovations are not entirely determined by the performance quality (and resulting threshold) behaviour. There is a social aspect to the phenomena. We now move onto looking at a temporal model. One way we could do this is that an individual takes a survey of all their neighbours and if enough of them are standing they also decide to stand (somewhat similar to the movement mechanism in a Schelling Segregation Model). However if you are sat in the audience it is unlikely you&#39;ll turn round to see what the people behind you are doing, you will only notice those in front of you. You may also notice those from many rows in front of you rather than just proximal neighbours. Page/Miller suggest a viewing &quot;funnel&quot; where each agent can see as: . Where we can adjust the number of rows somebody can see as a paramter (with 3 presented here). We will make a small adjustment to this method; let&#39;s place more weight on the audience members closest to us. We will calculate the proportion standing in each row in or field of vision and take a weighted sum of these according to the square distance away (e.g. row 2 will be weighted 1/4 of row 1, row 3 will be 1/9 weighted and so on.) This should capture the behaviour that there is more social pressure from those around us, this might lead to a slow &quot;wave&quot; propagating through the audience as this pressure grows. For this model lets also assume that the lights are on in the theatre and we can see all rows up to the front in detail. . For this &quot;funnel&quot; we will ignore the proximal &quot;next door&quot; neighbours from this calculation. We will assume there is a different mechanism for these individuals, we will look at the proportion of neighbours standing (either 0, 0.5 or 1) and we will apply a &quot;neighbour weight&quot; which will weight between this score and the funnel score to give an overall social pressure score as: $$ Social _Pressure = (1 - neighbour _weight) times Funnel _Pressure + neighbour _weight times Neighbour _Pressure $$ . We will use this mechanism of seperating out next door neighbours since if we go to a performance with others they will typically be sat next to us. Further one could argue that somebody is more likely to stand/sit based on their friend/partner&#39;s behaviour than those of a stranger. The neighbour weight parameter allows us to vary the relative importance. . Using the overall social pressure metric a sitting individual will stand if the social pressure exceeds a fixed threshold, similarly a standing individual will sit if the social pressure is smaller than one-less the fixed threshold. We are assuming that there is a symmetry invoked here which may not be the case. For example the &quot;embarrassment&quot; of standing while others are sitting might mean people are quicker to sit if they&#39;re in a small minority standing. However for a first quick and dirty implementation of a model the symmetry assumption will suffice (it will be easy enough to modify later should we desire). . We will also add 2 random components to this model: the first being a rate that if one is sitting down they stand up regardless of the information presented. The other being the reverse of this: a rate by which individuals sit down if they&#39;re standing. We will denote these rates as $ delta, gamma$ respectively. This might allow for the possiblity of a &quot;spontaneous ovation&quot; occuring. . We now have one choice remaining before we can implement this model: how will the updates be made? There are 2 main classes of updates in an agent based model: asynchronous and synchronous. The former essentially means each agent updates &quot;one by one&quot;, the order of upates can be random or by some defined order (in this example we could update those nearest the stage and iterate backwards for example). In a synchronous updating scheme all agents updated their state together once per time-step, of course by doing this one has to be careful in some situations (e.g. if there is an asset that can get depleted who gets to use it first?) but in this simple model there are no such concerns. For this model I think synchronous modelling is appropriate since I believe this is how ovations work in practice (e.g. we do not wait for &quot;our turn&quot; to make a decision), we could investigate how this affects the outcome later if we wish to. . My implementation of this model can be seen below. We will look at how the proportion of audience members standing evolves over time (functions use njit decorator to simply improve run time - looping over the array multiple times in the funnel calculation is fairly slow in pure python): . import numpy as np from numba import njit import matplotlib.pyplot as plt %matplotlib inline # Set audience size N = 50 M = 100 # Fix performance quality as 50% P = 0.5 # Fix Performance threshold T = 0.6 # Fix seed np.random.seed(123) # Fix all audience as seated (0) initially audience = np.zeros((N, M)) # Set error functions for audience members epsilon = np.random.random((N, M)) - 0.5 # Fix number of timesteps T_steps = 100 Pct_Hold = np.zeros(T_steps+1) # Fix social pressure threshold Pressure_T = 0.4 # Fix neighbour weight N_weight = 0.5 # Fix probaility of spontaneous standing delta = 0.1 # Fix probability of spontaneous sitting gamma = 0.05 # Calculate initial reactions to performance @njit def initial_stand(error, p, t): signal = p + error out = signal &gt; t return out * 1 # Function to calculate social pressure @njit def pressure(i, j, aud): rows = aud.shape[0] cols = aud.shape[1] pct_sum = 0 norm = 0 for x in range(i): active_row = i - x - 1 left = max(j - (x+1), 0) right = min(j + (x+1), cols) pct_sum += aud[active_row, left:right+1].mean() / (x+1)**2 norm += (x+1)**-2 if norm == 0: res = 0 else: res = pct_sum / norm return res # Calculate pressure from (nextdoor) neighbours @njit def neighbour_pressure(i, j, aud): cols = aud.shape[1] count = 0 left = 0 right = 0 if j != 0: left = aud[i, j-1] count += 1 if j != cols -1: right = aud[i, j+1] count += 1 return (left + right) / count # Calculate overall pressure score @njit def pressure_score(i, j, aud, n_weight): return pressure(i, j, aud)*(1 - n_weight) + neighbour_pressure(i, j, aud)*n_weight # Spontaneous sitting/standing function @njit def spontaneous(i, j, aud, dlt, gma): rnd = np.random.random() if aud[i,j] == 0 and rnd &gt; (1-dlt): aud[i,j] = 1 elif aud[i,j] == 1 and rnd &gt; (1-gma): aud[i,j] = 0 return aud # Main Code Loop audience = initial_stand(epsilon, P, T) Pct_Hold[1] = audience.mean() for x in range(2, T_steps+1): audience_old = audience.copy() for i in range(N): for j in range(M): up_score = pressure_score(i, j, audience_old, N_weight) down_score = pressure_score(i, j, 1 -audience_old, N_weight) if up_score &gt; Pressure_T and audience_old[i,j] == 0: audience[i,j] = 1 if down_score &gt; Pressure_T and audience_old[i,j] == 1: audience[i,j] = 0 spontaneous(i, j, audience, delta, gamma) Pct_Hold[x] = audience.mean() plt.plot(Pct_Hold) plt.xlabel(&quot;Time Step&quot;) plt.ylabel(&quot;Percent Standing&quot;) plt.show() . By selecting particular parameters we can see a few different behaviours in the model. These include an inital peak of enthusiasm that dies down, a gradual increase as the applause trickles from the front of the auditorium to the back and a rapturous applause leading to nearly all participants standing up very quickly. These are all behaviours that we do see in real life audiences. If we were particularly motivated we could look for real world phenomena in standing ovations and see if these rules are able to replicate them. . Potential Improvements . With agent based models there are always &quot;extra&quot; things that could be added/modified. A few select suggestions include: . Revisit the &quot;funnel&quot; mechanism and compare various schemes | What happens if there are various levels/balconies or perhaps aisles between seats? | What happens with &quot;stooges&quot;? That is if we add agents who always applaud regardless, what is the minimum number required to guarantee a standing ovation? Where are they best placed (perhaps taking into account ticket cost - 10 in the middle compared to 2 at the front), etc. | We could code in &quot;partners&quot; or &quot;groups&quot; more precisely to investigate the impact that has | How does changing to asynchronous updating change the behaviour? | What if every so often an agent &quot;turns round&quot; to view the seats behind them? | and so on | . Conclusion . We have seen how we can code up a basic agent based model for standing ovations. While the subject itself isn&#39;t particularly &quot;useful&quot; in of itself the principles are common to many other systems. We have also seen some of the considerations that go into an agent based model. . As a final remark it is worth noting that this is not how you would produce an agent based model in practice. Typically you would start with a simple model and add features gradually and test the impact, I did not want to write a series of long posts on this model so I &quot;skipped to the end&quot; coding in what I thought some pertinent features could be all at once (although I did not spend long thinking about them!) . References . The original paper on standing ovation models presented by Scott Page and John Miller (can be seen here! for those wanting to read more about their version of the model. .",
            "url": "https://www.lewiscoleblog.com/standing-ovation",
            "relUrl": "/standing-ovation",
            "date": " â€¢ Feb 25, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Barnes-Hut Algorithm",
            "content": "The N-Body Problem . It is well understood that for $N &gt; 2$ that the problem of determining trajectories under gravitation for N-bodies cannot be solved analytically. Instead we must rely on computation to create trajectories. From Newton&#39;s equations we can write the force $ pmb{F_{i,j}}$ between two bodies $i, j$ with masses $m_i, m_j$ and positions $ pmb{p_i}, pmb{p_j}$ as: $$ pmb{F_{i,j}} = frac{G m_i m_j ( pmb{p_i} - pmb{p_j})} { lVert pmb{p_i} - pmb{p_j} rVert^3} $$ . For $N$ bodies we therefore have $ frac{N(N-1)}{2}$ forces to be calculated. For small $N$ this is not a problem, however if we want to model galaxy dynamics with thousands (or more) of bodies this becomes a computational issue. . Barnes Hut Process . With Barnes Hut we can move from $O(N^2)$ complexity to $O(Nln(N))$ - this is a significant improvement for large $N$. . The crux of the Barnes Hut scheme is to approximate the effect of &quot;far away&quot; bodies but calculate exactly the forces attributed to &quot;near&quot; bodies. To do this a quadtree data structure is used (in 3d an octree or higher dimensions a $2^d$tree). To do this the space is partitioned into 4 equal squares at depth 1 (or 8 cubes in 3d or $2^d$ hypercubes in higher dimension) - at depth 2 each of these squares is then partitioned into 4 equal squares and so on. From this a tree is constructed with each node representing the number of bodies within the quadrant at that depth. This is repeated recursively until a depth is reached where each quadrant has 1 or 0 bodies in it. It is easier to understand this with a visual example: . . With this data structure in place we can begin to simulate. For each quadrant in the quadtree we can calculate a centre of mass (in the obvious way). Then given a body we can calculate the force acting on it from a given quadrant by applying the Newtonian force calculation using the centre of mass. We could, for example, apply this to the 4 quadrants at depth 1 to approximate the force acting on the body. This of course might not be particularly accurate if there are many bodies present. To overcome this the Barnes Hut algorithm specifies a critical distance $ theta$ if the distance beween the body and the centre of mass of the quadrant is greater than $ theta$ then it is used as an approximation, if not the algorithm moves to the next depth of the quadtree and tries again, this happens recursively until the distance becomes below $ theta$ or there is only 1 body in the quadrant (as such setting $ theta = 0$ returns to the $O(N^2)$ brute force approach). . While this is presented as an algorithm for the N-body problem it can be used in other situations that involve a spatial dimension and many entities. I am currently considering an application to speed up an agent based model (potentially the subject of another blog post). . Implementation . Initially I will code up an example in Python, I may revisit this and create a more efficient implementation in Cython/C++/etc. I will ignore all &quot;complications&quot; for example I will not give each body a size nor will it consider collisions, etc. Each body is a &quot;point mass&quot; in this example. For now I&#39;ll also ignore the issues relating to plotting trajectories. . We first create a node class to represent nodes within the quadtree. Using this data structure we will then apply a verlet time step to calculate positions: . from copy import deepcopy import numpy as np class node: &quot;&quot;&quot; A class for a node within the quadtree. We use the terminology &quot;child&quot; for nodes in the next depth level - consistent with tree nomenclature If a node is &quot;childless&quot; then it represents a body &quot;&quot;&quot; def __init__(self, x, y, px, py, m): &quot;&quot;&quot; Initializes a childless node m - Mass of node x - x-coordinate centre of mass y - y-coordinate centre of mass px - x- coordinate of momentum py - y-coordinate of momentum pos - centre of mass array mom - momentum array child - child node s - side-length (depth=0 s=1) relpos = relative position &quot;&quot;&quot; self.m = m self.pos = np.array([x,y]) self.mom = np.array([px,py]) self.child = None def next_quad(self): &quot;&quot;&quot; Places node in next quadrant and returns quadrant number &quot;&quot;&quot; self.s = 0.5*self.s return self.divide_quad(1) + 2*self.divide_quad(0) def divide_quad(self, i): &quot;&quot;&quot; Places node in next level quadrant and recomputes relative position &quot;&quot;&quot; self.relpos[i] *= 2.0 if self.relpos[i] &lt; 1.0: quadrant = 0 else: quadrant = 1 self.relpos[i] -= 1.0 return quadrant def reset_quad(self): &quot;&quot;&quot; Repositions to the zeroth depth quadrant (full space) &quot;&quot;&quot; self.s = 1.0 self.relpos = self.pos.copy() def dist(self, other): &quot;&quot;&quot; Calculates distance between node and another node &quot;&quot;&quot; return np.linalg.norm(self.pos - other.pos) def force_ap(self, other): &quot;&quot;&quot; Force applied from current node to other &quot;&quot;&quot; d = self.dist(other) return (self.pos - other.pos) * (self.m * other.m / d**3) def add_body(body, node): &quot;&quot;&quot; Adds body to a node of quadtree. A minimum quadrant size is imposed to limit the recursion depth. &quot;&quot;&quot; new_node = body if node is None else None min_quad_size = 1.e-5 if node is not None and node.s &gt; min_quad_size: if node.child is None: new_node = deepcopy(node) new_node.child = [None for i in range(4)] quad = node.next_quad() new_node.child[quad] = node else: new_node = node new_node.m += body.m new_node.pos += body.pos quad = body.next_quad() new_node.child[quad] = add_body(body, new_node.child[quad]) return new_node def force_on(body, node, theta): if node.child is None: return node.force_ap(body) if node.s &lt; node.dist(body) * theta: return node.force_ap(body) return sum(force_on(body, c, theta) for c in node.child if c is not None) def verlet(bodies, root, theta, G, dt): for body in bodies: force = G * force_on(body, root, theta) body.mom += dt * force body.pos += dt * body.mom / body.m def model_step(bodies, theta, g, step): root = None for body in bodies: body.reset_quad() root = add_body(body, root) verlet(bodies, root, theta, g, step) ########## Main Code ########## # Parameters Theta = 0.7 G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X0 = np.random.random(N_bodies) Y0 = np.random.random(N_bodies) PX0 = np.random.random(N_bodies) - 0.5 PY0 = np.random.random(N_bodies) - 0.5 # Initialize Bodies = [node(x0, y0, pX0, pY0, masses) for (x0, y0, pX0, pY0, masses) in zip(X0, Y0, PX0, PY0, Masses)] # Main Model Loop def Model_Loop_BH(n): for i in range(n): model_step(Bodies, Theta, G, dt) %timeit Model_Loop_BH(N_steps) . 7.97 s Â± 446 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) . This code is not the most efficient possible for this model (in all likelihood one would only use Barnes-Hut for large galaxy simulations and so it&#39;s unlikely that Python would be the best choice). For 100 bodies and 1000 time steps this code runs on my machine in around 8s. . We can also code up a &quot;brute force&quot; approach (calculating forces between all pairs of bodies) to compare, it is likely that the cost of computing the quadtree could be more costly than just computing all forces directly when the number of bodies is suitably small. We could run these codes multiple times to find where this is the case. . import numpy as np # from numba import njit G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X = np.random.random(N_bodies) Y = np.random.random(N_bodies) PX = np.random.random(N_bodies) - 0.5 PY = np.random.random(N_bodies) - 0.5 pos = np.array((X,Y)) mom = np.array((PX, PY)) #@njit def force_array(pos_arr, m_array): n = pos_arr.shape[1] force_arr = np.zeros((2 ,n, n)) for i in range(n): for j in range(i): force_arr[:, i, j] = G * m_array[i] * m_array[j] * (pos[:,i] - pos[:, j]) / np.abs((pos[:,i] - pos[:, j]))**3 force_arr[:, j, i] = - force_arr[:, i, j] return force_arr #@njit def update_mom(step, mom_arr, force_arr): n = mom_arr.shape[1] del_mom = np.zeros_like(mom_arr) for i in range(n): for j in range(n): del_mom[:, i] += step * force_arr[:, i, j] return mom_arr + del_mom #@njit def update_pos(step, pos_arr, new_mom, m_arr): return pos_arr + step * new_mom / m_arr #@njit def main_loop(n, pos_arr, mom_arr): for i in range(n): force = force_array(pos_arr, Masses) mom_arr = update_mom(dt, mom_arr, force) pos_arr = update_pos(dt, pos_arr, mom_arr, Masses) return pos_arr %timeit main_loop(N_steps, pos, mom) . 1min 18s Â± 1.89 s per loop (mean Â± std. dev. of 7 runs, 1 loop each) . This brute force code is not implemented in a particularly efficient way, however it is good enough for illustration purposes. Even for relatively small model sizes (100 bodies for 1000 time steps) the code takes considerably longer than the Barnes-Hut algorithm, with timeit giving an approximate timing of around 78s - about 10 times slower than the Barnes-Hut implementation. . If we use Numba with the njit decorator we can improve this to 3s - which while quicker is not considerably better than the Barnes-Hut algorithm. As the number of bodies increases (and perhaps some fine tuning of the Barnes-Hut parameters) we would expect even more drastic improvements. . import numpy as np from numba import njit G = 1.e-6 dt = 1.e-2 N_bodies = 100 N_steps = 1000 # Fix Seed for Initialization np.random.seed(123) # Initial Conditions Masses = np.random.random(N_bodies)*10 X = np.random.random(N_bodies) Y = np.random.random(N_bodies) PX = np.random.random(N_bodies) - 0.5 PY = np.random.random(N_bodies) - 0.5 pos = np.array((X,Y)) mom = np.array((PX, PY)) @njit def force_array(pos_arr, m_array): n = pos_arr.shape[1] force_arr = np.zeros((2 ,n, n)) for i in range(n): for j in range(i): force_arr[:, i, j] = G * m_array[i] * m_array[j] * (pos[:,i] - pos[:, j]) / np.abs((pos[:,i] - pos[:, j]))**3 force_arr[:, j, i] = - force_arr[:, i, j] return force_arr @njit def update_mom(step, mom_arr, force_arr): n = mom_arr.shape[1] del_mom = np.zeros_like(mom_arr) for i in range(n): for j in range(n): del_mom[:, i] += step * force_arr[:, i, j] return mom_arr + del_mom @njit def update_pos(step, pos_arr, new_mom, m_arr): return pos_arr + step * new_mom / m_arr @njit def main_loop(n, pos_arr, mom_arr): for i in range(n): force = force_array(pos_arr, Masses) mom_arr = update_mom(dt, mom_arr, force) pos_arr = update_pos(dt, pos_arr, mom_arr, Masses) return pos_arr %timeit main_loop(N_steps, pos, mom) . 3 s Â± 33.1 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) . Conclusion . We have seen that even for relatively modest models the Barnes-Hut algorithm provides fairly efficient computation for the N-body problem. We would expect the improvement to be even more marked for larger models. However Python on its own is not the best place to implement this and a C++ implementation would offer better performance. With some work a Cython approach should be possible and offer performance improvements. The code would require some major re-working if Numba is to be used instead owing to the reliance on classes to generate the quadtree. .",
            "url": "https://www.lewiscoleblog.com/barnes-hut",
            "relUrl": "/barnes-hut",
            "date": " â€¢ Feb 18, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Cython Vs Numba: An Example",
            "content": "Following on from the previous blog post here comparing the relative merits of Cython Vs Numba I thought I&#39;d illustrate this with implementations of a relatively simple model: a vanilla 2d Ising Model. This is a prime target for a performance boost since it is a very &quot;loopy&quot; code. I will not cover what the Ising Model is/how it works (you can check that here!). It is a very interesting model and I may return to it at a later date to look at some of its properties and explore it more deeply (and maybe spin glass models more generally). For now it will just be a test subject to explore performant python type code. . In this blog I will present a few different versions, I won&#39;t throw the kitchen sink at them to get the absolute best performance but will adopt an 80/20 principle. I will not try any parallelisation or clever memory management outside of what comes pre-canned. . Python . This is a basic python implementation using a lot of looping. Even for a relatively small models this code will likely be fairl slow due to the looping. . import numpy as np # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours def neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res def dE(i, j, spin_array): return 2*spin_array[i, j]*neighbour_sum(i, j, spin_array) def update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array def _main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = update(spin_tmp) return spin_tmp . Numba . For this code we will just take the above code and use the njit decorator (forcing the use of LLVM - a jit decorator will fall back to Python object mode if it cannot work out how to use LLVM). This is literally a few seconds of coding updates. . import numpy as np from numba import njit # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @njit def nb_neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res @njit def nb_dE(i, j, spin_array): return 2*spin_array[i, j]*nb_neighbour_sum(i, j, spin_array) @njit def nb_update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = nb_dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array @njit def nb_main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = nb_update(spin_tmp) return spin_tmp . Cython . Again we will modify the python code. We will see that while not too onerous it does require a little more work than the Numba example. The code is largely boilerplate but requires a little more thinking than the Numba example (e.g. in implementing this I initially forgot a static type definition of 1 variable which was causing a 300% increase in runtime). . %load_ext Cython . %%cython cimport cython import numpy as np cimport numpy as cnp from libc.math cimport exp from libc.stdlib cimport rand cdef extern from &quot;limits.h&quot;: int RAND_MAX # Grid size: N (int) NxN square latice cdef int N = 1000 # Fix Temp cdef float kT = 2 / np.log(1 + np.sqrt(2)) cdef float kTinv = 1 / kT # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_neighbour_sum(int i, int j, cnp.int32_t[:, :] spin_array): cdef int north = 0 cdef int south = 0 cdef int east = 0 cdef int west = 0 cdef int max_height = spin_array.shape[0] cdef int max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] cdef int res = north + south + east + west return res @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_dE(int i, int j, cnp.int32_t[:, :] spin_array): return 2*spin_array[i, j]*cy_neighbour_sum(i, j, spin_array) @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_update(cnp.int32_t[:, :] spin_array): cdef int height = spin_array.shape[0] cdef int width = spin_array.shape[1] cdef int y_offset, x_offset cdef int i, j cdef int dEtmp for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = cy_dE(i, j, spin_array) if dEtmp &lt;= 0: spin_array[i, j] *= -1 elif exp(-dEtmp * kTinv) * RAND_MAX &gt; rand(): spin_array[i, j] *= -1 return spin_array @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_main_code(int M, cnp.int32_t[:, :] spin_array): cdef int x for x in range(M): spin_array = cy_update(spin_array) return spin_array @cython.boundscheck(False) @cython.wraparound(False) cpdef cnp.int32_t[:, :] cpy_main_code(int M, cnp.int32_t[:, :] spin_array): return cy_main_code(M, spin_array) . Timing Results . # Python %timeit _main_code(10, spins) # Numba %timeit nb_main_code(10, spins) # Cython %timeit cpy_main_code(10, spins) . 52 s Â± 2.53 s per loop (mean Â± std. dev. of 7 runs, 1 loop each) 244 ms Â± 4.48 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) 395 ms Â± 6.92 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) . For our purposes the %timeout magic will be good enough a proxy for performance. We can see that the intial python implementation is very slow. For a 1000x1000 lattice and looping over 10 times, for my machine the python interation takes 50-55s. This would be fairly problematic if we wanted to sweep over the parameter space, find critical temperatures, perform random seed analyses etc. . In contrast Numba only takes 240-250ms, an impressive 2000% speed up. Running the code multiple times now seems much less onerous. . Cython is not quite as quick as the Numba implementation taking 390-400ms but still represents a significant speedup compared to Python. For practical applications the difference between Numba and Cython in this case may be insignificant. . It is worth noting that these times are from my machine on at a specific time. The times achieved on your machine might be slightly different. Similarly changing the size of the lattice may change the ordering of which option is &quot;quickest&quot; - as always it&#39;s worth checking the code how it will be used rather than performing a benchmark like this for determining which option to use. . Conclusion . From the results above it may be tempting to claim Numba is the obvious choice given it is not only easier to implement than cython but also offers faster speeds. However I selected the 2d Ising model as an example since I knew the code would work well in Numba (in a sense I have been p-value hacking the experiment!) In certain situations (e.g. a code relying very heavily on class structures) Numba is either unusable or requires a complete code overhaul whereas cython can require only a few lines of boilerplate code. . In other examples you can also see that Cython can severely outperform Numba, I am not sure why this is and the only real way to determine which will perform better is to perform testing (if somebody has an explanation/heuristic I&#39;d love to hear it). It is also possible to interface numba and cython which has been useful to me in the past. For a quick example suppose we want to perform an inverse transform of a Beta(2,0.5) distribution: . from scipy.stats import beta x = beta.ppf(0.1, 2, 0.5) x . 0.4681225665264196 . This cannot be optimised in Numba as it is (beta.ppf is currently not supported functionality - this may change by the time you read this). However we can take the address of the cython special function that this calls. We can then build the function in such a way as it can be seen by Numba: . import ctypes from numba import types, njit from numba.extending import get_cython_function_address betaaddr = get_cython_function_address(&quot;scipy.special.cython_special&quot;, &quot;btdtri&quot;) functype3d = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_double) beta_fn = functype3d(betaaddr) @njit def nb_beta_ppf(p, a, b): return beta_fn(a, b, p) x = nb_beta_ppf(0.1, 2.0, 0.5) x . 0.4681225665264196 . As we can see this is a little ugly but it works. . As a result of everything covered in these blog posts I do not believe that one option offers a significant advantage over the other and both offer valid tools for improving runtime. The choise of which to use in a given situation will depend on many factors and requires careful thought as to what is needed in a given setting. .",
            "url": "https://www.lewiscoleblog.com/cython-numba-2",
            "relUrl": "/cython-numba-2",
            "date": " â€¢ Feb 11, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Cython Vs Numba",
            "content": "Outline of the Problem . Python at its core is slow for certain things. By being a dynamically typed and interpreted language you incur certain runtime overheads. In some cases these are not much of an issue. At other times they can be critical. . Typically we will try and &quot;vectorize&quot; the code as much as possible (avoiding extraneous loops) and force as much code as we can into NumPy array operations which are typically &quot;quick&quot; (compiled C code). This is fine when it works, but it is not always possible to vectorize the code or, in some cases, the vectorization leads to code that is very hard to read/understand. . Both Numba and Cython (not to be confused with CPython) aim to provide tools to deal with such situations. . Outline of Cython . Cython is a programming language that is part Python and part C/C++, it can be compiled into a python extension and/or an executable. If you are familiar with Python it is reasonably easy to understand Cython code, it largely just has a few &quot;boiler-plate&quot; code blocks along with a few static type declarations (familiar to those who know C/C++/related languages). . Since there are no dynamic types (in well written Cython code) and it is compiled typically the resulting code is orders of magnitude faster than Python. Compared to vectorised NumPy there may not be a significant improvement but this depends on the exact implementation. . Cython itself is very flexible, if you can express the code in Python it is unlikely you will not be able to express it in Cython. Any arbitrary class structure can work within Cython, as a result it is used for many &quot;high performance&quot; Python packages (e.g. SciPy). . It is possible to parallelize the code or utilise GPU computation using Cython. This normally requires a bit of work but typically does not require nearly as much work as using Cuda in C++ (for example). As usual the normal caveats relating to multi-thread applications also apply to Cython code. . You can read the Cython documentation here! . Outline of Numba . Numba is a slightly different beast. It uses the concept of a &quot;just in time&quot; compiler (JIT). Essentially this means that code is compiled &quot;on the fly&quot; during runtime instead of requiring compilation prior to execution. Numba compiles the python code using a LLVM compiler. . The syntax is very simple and most of the time just requires a simple decorator on a Python function. It also allows for parallelisation and GPU computation very simply (typically just a &quot;target = &#39;cuda&#39;&quot; type statement in a decorator). In my experience a lot less thinking is required to set this up compared to Cython. . The downside of Numba (at least for me) is that it is a (comparatively) new package and as such does not have support for absolutely everything you would want, unlike with Cython it is possible to just &quot;hit the wall&quot; where you simply cannot use Numba without a major re-writing of the code. (One such example being you cannot call @guvectorize functions inside the @njit decorator). It is worth checking the github issues log regularly as often these issues are on the docket to be corrected in future releases. . Another downside of Numba is the lack of useful traceback, typically you need to &quot;switch off&quot; Numba and run in regular python to track down an error. This is typically only a minor inconvenience but if the code is particularly slow it can get frustrating trying to find an error without the Numba speed up. . You can read more about Numba here! . Which is better? . From a raw performance perspective I do not see either Cython nor Numba consistently beating the other in all situations. Typically the performance will be comparable and you will rarely find one being many orders of magnitude quicker (assuming you&#39;re using both correctly). . The choice of which to use, in my opinion, comes down to other factors. Convenience being a big one, I typically find Numba easier and quicker to implement when it works. As noted above however it doesn&#39;t always work (e.g. if using class structures, custom data types, etc.) With familiarity you do get an instinct as to whether a code will work or not. Cython on the other hand offers much more flexibility. . There is also the issue of how the code will be used. Cython is well established for creating efficient extension modules that sit nicely within the Python eco-system. Numba can be used in a similar way but I have found it a bit more finnicky to deal with (for example through Numba itself changing its API fairly regularly since it&#39;s a relatively new module, some code from previous iterations of Numba simply does not work at all with the later versions). . What is the catch? . Unfortunately things are not perfect, typically we will still be interfacing Cython/Numba functions via Python and so using repeated calls to these functions we will still incur overheads (typically through the conversion to Python types). This can mean that certain code is still significantly slower than C/C++ equivalents. These packages are therefore most useful for when you have profiled your code and can see that a handful of functions/operations are the real bottleneck. . These packages may not help if your code is particularly memory intensive, in which case it is better to spend time thinking about memory management instead. In some cases these packages provide some help in that respect also (e.g. NumPy is prone to creating many cached variables for simple operations, if the variables are large arrays this can become a pain.) . What about PyPy/etc? . Another option for performant Python code is to use PyPy instead of CPython. I have not used this very much yet, if I get the time to really kick the tyres I may write another blog on my findings. There are some features that appear useful but the eco-system is not as well supported (yet?) and so may require some additional work to recreate some high level functionality. . You can read more about PyPy here! . Why not just C/C++? . Ultimately if you require peak performance at all costs these options are still no substitute for well written C/C++. However as I often warn people: computation time is generally cheaper than human time - it is often better to use a slightly sub-optimal (but still respectable) code than devote months to R&amp;D and slow down the development cycle. Since Numba/Cython are so similar to Python (and it is possible to just &quot;tack on&quot; some Python to the end of these codes) you can prototype much more quickly in my experience. All these factors (along with many others such as where the code is to be deployed, what other tools are being used, etc.) need to be weighed up. . What about Julia? . Some readers may be familiar with the Julia language as an option for high performance scientific computing. I have a little experience (but am far from an expert). As I understand Julia is based around JIT (as with Numba), however being a language to itself it never needs to interface with Python and its limitations. It can therefore create more efficient code for larger scale projects since they never have to worry about Python overheads. Typically the benchmarks seen online are for smaller &quot;toy&quot; problems and so the performance does not appear to be too different from Cython/Numba. . I have not switched to Julia for a few reasons, firstly the popularity of Python - it is typically fairly easy to learn Cython/Numba for somebody who understands Python/NumPy making collaboration easier. Secondly the Python eco-system is well developed there is typically a package available to do almost anything you would want. Thirdly at this point it is fairly easy to get Python to &quot;speak&quot; with other systems if you need to turn something from a prototype to production. These concerns are ultimately just related to uptake however, as more people use Julia I see this becoming less of a concern. . You can read more about Julia here! . Conclusion . Hopefully now we can see that Cython/Numba provide useful tools for bridging the gap between Python and C/C++ runtimes. As the old saying goes &quot;you cannot have your cake and eat it too&quot; and so it may not be possible to get performance as quick using these options. However we can often get performance that is &quot;good enough&quot; in practical terms. .",
            "url": "https://www.lewiscoleblog.com/cython-numba",
            "relUrl": "/cython-numba",
            "date": " â€¢ Feb 4, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Jackknife Methods",
            "content": "In this blog post we are concerned with a specific problem: we have a Monte-Carlo type model that produces some simulated output. With this we want to estimate an arbitrary statistic (for example percentiles, expected shortfall or more complicated statistics relating to many variables). We know however that calculated in this way the calculated statistic is just one realisation of a distribution of outcomes. We would like to be able to say something about this distribution, in particular we would like to have some idea of the variability in the statistic. . The &quot;obvious&quot; (and most accurate) way to do this would be to re-run the model many times with different random seeds and create an empirical distribution of the statistic. However if the model is particularly complex this might mean a lot of compute time. Instead we would like to find an approximate method that does not require us to re-run the model at all. To do this in a general way we will introduce the Jackknife method. . It is worth noting that in certain situations other methods can be easier to implement/more accurate, for example if we only wanted to estimate the 90th percentile we can construct an argument using a binomial distribution (and normal approximation thereof) - however this method will not generalise to (for example) expected shortfall or even more complicated statistics. . Justification of the Method . We begin by supposing we have $N$ un-ordered observations $ { X_i }_{i=1}^{N}$ from our Monte-Carlo model. We use these observations to create an estimate of a statistic $ hat{Q}$. We denote the estimate from the model $ hat{Q}_{ {1:N }}$, which itself is a random variable. Through a Taylor expansion we can note: $$ mathbb{E} left( hat{Q}_{ {1:N }} right) = hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} + ...$$ For some constants $a_x$. . If we now consider partitioning the $N$ observations as: $N = mk$ for integers $m$ and $k$ - that is we create $m$ collections of $k$ obervations ($k gg m$). We denote a set: $A_i = { X_j | quad j &lt; (i-1) k , quad j geq i k }$ to contain all observations bar $k$, each set removes a different set of $k$ observations. Each has $|A_i| = (m-1)k$. We can then write: $$ mathbb{E} left( hat{Q}_{A_i} right) approx hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} $$ Via a second order approximation. . If we then define a new variable: $ hat{q}_i = m hat{Q}_{ {1:N }} - (m-1) hat{Q}_{A_i}$. Then via a second-order approximation we have: $$ mathbb{E} left( hat{q}_i right) approx m left( hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} right) - (m-1) left( hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} right)$$ Through some simplification this becomes: $$ mathbb{E} left( hat{q}_i right) approx hat{Q} - frac{a_2}{m(m-1)k^2} $$ Asymptotically this has bias $O(N^{-2})$. If the estimator only has bias $O(N^{-1})$ (i.e. $a_i =0$ for $i geq 2$) then the approximation is unbiased. . We can now define two new variables: $ hat{ hat{Q}}_N = frac{1}{m} sum_{i=1}^{m} hat{q}_i$ $ hat{ hat{V}}_N = frac{1}{m-1} sum_{i=1}^{m} left( hat{q}_i - hat{ hat{Q}}_N right)^2$ Then via CLT we have that $ hat{ hat{Q}}_N sim mathcal{N}( hat{Q}, hat{V})$ for some unknown variance $ hat{V}$. We can thus create an $X %$ confidence interval as: $ hat{Q} in left[ hat{ hat{Q}}_N - t sqrt{ frac{ hat{ hat{V}}_N}{m}}, hat{ hat{Q}}_N + t sqrt{ frac{ hat{ hat{V}}_N}{m}} right] $ With $t$ as the $(1-X) %$ point of a double-tail student-t distribution with $(m-1)$ degrees of freedom. . We can see from the construction of this confidence interval there has been no restriction on the type of statistic $ hat{Q}$ used, in this sense this is a generic method. . (Note this is strongly related to a bootstrap method, in fact it is a first order approximation to a bootstrap) . An Example . The explanation above is quite notation dense, it will be easier to look at an example. In this case we will take one of the simplest examples of a Monte-Carlo model: estimating the value of $ pi$. To do this we will take a unit square and a unit quarter circle inside it: . . We will simulate random points within the square and calculate the proportion $p$ of points landing within the quarter circle (red). If we simulate $N$ points we get an estimate $ pi approx frac{4p}{N}$. A simple vectorised numpy for this can be seen below: . # Estimating pi using Monte-Carlo import numpy as np def points_in_circle(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) y = np.random.random(N) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_circle &quot;&quot;&quot; return arr.sum() / arr.shape[0] * 4 SIMS = 10000 pts = points_in_circle(10000) print(&quot;Estimate of pi:&quot;, est_pi(pts)) . Estimate of pi: 3.1456 . We can use the jackknife method to now construct a confidence interval. (Obviously in this case we have the sample estimator of an average and so CLT applies, in practice we wouldn&#39;t use a jackknife here. But for this example I wanted something simple as to not distract attention from the jackknife method itself.) We know what the result &quot;should&quot; be in this example, however we shall pretend we don&#39;t have access to np.pi (or similar). . We can code up an implementation of the jackknife method as: . from scipy.stats import t def jackknife_pi(arr, pi_fn, m): &quot;&quot;&quot; This function implements the jackknife method outlined above The function takes an array (arr) and an estimate function (pi_fn) and a number of discrete buckets (m) - in this implementation m needs to divide size(arr) exactly The function returns Q-double hat, V-double hat, (m-1) &quot;&quot;&quot; Qn = pi_fn(arr) N = arr.shape[0] itr = np.arange(N) k = N / m q = np.zeros(m) for i in range(m): ID = (itr &lt; i*k) | (itr &gt;= (i+1)*k) temp = arr[ID] q[i] = m*Qn - (m-1)*pi_fn(temp) Qjk = q.sum() / m v = (q - Qjk)**2 Vjk = v.sum() / (m - 1) return Qjk, Vjk, (m-1) def conf_int(Q, V, X, dof): tpt = t.ppf(1-(1-X)/2, dof) up = Q + tpt*np.sqrt(V/(dof+1)) down = Q - tpt*np.sqrt(V/(dof+1)) print(round(X*100),&quot;% confidence interval: [&quot;,down,&quot;,&quot;,up,&quot;]&quot;) jk_pi = jackknife_pi(pts, est_pi, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . 95 % confidence interval: [ 3.103649740420917 , 3.187550259579082 ] . We can see that the 95% confidence interval range is fairly large (around 3.10 to 3.19) in this case. We will now show that by using a &quot;better&quot; method we can reduce this range. We start by reconsidering the square-quarter-circle: we notice that we can add 2 additional squares to this setup: . . Apart from looking like a Mondrian painting we can notice that all points generated in the yellow area will add &quot;1&quot; to the estimator array and all points in the black area will add &quot;0&quot;. The only areas of &quot;contention&quot; are the blue/red rectangles, if we focus only in generating points in these areas we will increase the accuracy of the estimator. By symmetry these 2 rectangles are identical, we only need to generate points within the rectangle: $ left { left(1, frac{1}{ sqrt{2}} right), left( 1,0 right), left( frac{1}{ sqrt{2}},0 right), left( frac{1}{ sqrt{2}}, frac{1}{ sqrt{2}} right) right }$. This has the area: $ frac{ sqrt{2}-1}{2}$ or both rectangles together having total area: $ sqrt{2}-1$. Therefore generating $N$ points within these rectangles is equivalent to generating: $ frac{N}{ sqrt{2}-1}$ points in the original scheme (approximately 2.5 times as many). This should reduce the standard deviation of the estimate by about a third (by CLT). We can code this estimator up in a similar way to before: . def points_in_rect(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise This uses the &quot;imporved&quot; method N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) * (1 - 1 / np.sqrt(2)) + (1/np.sqrt(2)) y = np.random.random(N) / np.sqrt(2) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi_2(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_rect This applies a correction since points are only simulated in smaller rectangles &quot;&quot;&quot; pct = arr.sum() / arr.shape[0] approx_pi = (pct*(np.sqrt(2)-1) + 0.5)*4 return approx_pi SIMS = 10000 pts = points_in_rect(10000) print(&quot;Estimate of pi:&quot;, est_pi_2(pts)) jk_pi = jackknife_pi(pts, est_pi_2, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . Estimate of pi: 3.144554915549336 95 % confidence interval: [ 3.1247314678035196 , 3.164378363295143 ] . As expected we can see the confidence interval has decreased significantly through an improved estimator. . Notes on Implementation . As we have seen the jackknife is a general tool. In certain situations other tools exist. There are a couple of points we have to keep in mind when implementing these methods: . The selection of m: this is fairly arbitrary, if the statistic being analysed is very cumbersome to calculate then a smaller choice of m is helpful (or if we wish to run this method over very many statistics). | Properties of the statistic: in some instances we know the statistic must be bounded (e.g. a correlation coefficient must be between $[-1,1]$) This additional information can and should be used to improve the confidence interval. It often becomes more art than science when deciding how to present the results of this method. | . Conclusion . In this post we have seen what a jackknife method is, why it works and a basic implementation. Hopefully now it is obvious the power these methods hold for reporting on the results of a Monte-Carlo simulation. In more sophisticated situations it can really give insight into which parts of a model suffer most from simulation error and also how confident we should be with an estimate it produces. .",
            "url": "https://www.lewiscoleblog.com/jackknife",
            "relUrl": "/jackknife",
            "date": " â€¢ Jan 28, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Neuron Models 3: Ensembles",
            "content": ". This is the third blog post in a series - you can find the previous blog post here . . Justifying Gaussian White Noise . We first begin with a small diversion, in the previous HH and EIF neuron firing examples we have assumed some sort of Gaussian white noise as an input signal. We briefly mentioned that this is a reasonable assumption but we will justify this in a bit more detail here. First we note that each neuron will typically take input signal from the order of 10,000 neurons. As such even in a low firing rate scheme a neuron will likely receive relatively large amount of input spikes. We can express this signal as: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k)$$ Where: $N$ is the number of connected neurons $J_{i}$ is the synaptic connection strength from neuron $i$ $t_i^k$ is the time of the kth spike recieved from neuron i . If we assume that these spikes arrive in an uncorrelated, memoryless fashion in the form of a Poisson process and that the connection strengths are suitably small: $ langle J_i rangle ll V_{Th} - V_{Re}$ (where angle brackets denote population average). Then we can apply a diffusion approximation: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k) approx mu + sigma xi(t)$$ Where: $ mu = langle J_i rangle N nu $ $ sigma = langle J_i^2 rangle N nu $ $ nu$ is the mean firing rate over all connected neurons $ xi(t)$ is a Gaussian white noise process . Of course as with all approximations this is subject to &quot;small sample size&quot; and $N$ needs to be suitably large. . Fokker-Planck . Recall that we specified the EIF model with Gaussian white noise as having dynamics: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . This is nothing more than an Ito process of the form: $$dX_t = mu(X_t,t)dt + sigma(X_t, t)dW_t $$ With standard Wiener process $W_t$. The Fokker-Planck equation gives us a probability distribution of this process $p(x,t)$ through the PDE: $$ frac{ partial}{ partial t} p(x,t) = - frac{ partial}{ partial x} left[ mu(x,t)p(x,t) right] + frac{ partial^2}{ partial x^2} left[ frac{1}{2} sigma^2(x,t)p(x,t) right] $$ This formula can also be extended to higher dimensions in an obvious way. The derivation of this formula is fairly involved so not included in this blog post, most good textbooks on stochastic analysis should have a derivation for the interested reader. . In the case of the EIF model we can thus write down: $$ frac{ partial p}{ partial t} = frac{ sigma^2}{ tau} frac{ partial^2p}{ partial V_m^2} + frac{ partial}{ partial V_m} left[ frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) right] $$ With $ psi(V_m)$ represnting the exponential firing term. . By the continuity equation we can write: $$ frac{ partial p}{ partial t} = - frac{ partial J}{ partial V_m} $$ . Where $J$ represents the flux. By using this relation in the Fokker-Planck equation and integrating over voltage we get: $$ J(V_m, t) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . We can also note that: $$J(V_{Re}^+,t) = J(V_{Re}^-, t) + r(t)$$ . Where $V_{Re}^ pm$ represents the limit from above (+) or below (-) the reset voltage. the function $r(t)$ represents the average neuron firing rate. This is due to the implementaion of the voltage reset mechanism post spike. We can also note that for $V_m &lt; V_{Re}$ we have $J(V_m, t) = 0$ and for $V_m &gt; V_{Re}$ we have $J(V_m, t) = - r(t)$. We can then solve the flux equation to give: $$P(V_m, t) = frac{r(t) tau}{ sigma^2} int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du $$ . Since the probability measure needs to integrate to 1, we can then write: $$r(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m right)^{-1} $$ . (Note under the scheme presented there is no time dependence to any of these equations. Under time dependent signals we would have to be more careful and typically further approximations are made.) . So far we have not allowed for the refractory period, we have assumed that after reset the voltage trajectories continue as normal. Given we have chosen a deterministic refractory period we can just add this to the euqation above: $$r_{ref}(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m + T_{Ref} right)^{-1} $$ . We can see that this integral will not give rise to an analytic solution in the case of EIF neurons. The forward Euler scheme we relied upon in the past will not perform well here. Instead we will use a slightly different numerical scheme. . Numerical Integration . (This is taken from Richardson [2007] - see references for further details) Presented now is a numerical scheme for calculating the firing rate. Recall from above: $$ J(V_m, t) = - r(t) Theta(V - V_{Re}) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . Where $ Theta(V)$ is the Heaviside step-function. Re-arranged this gives: $$- frac{ partial p}{ partial V_m} = - frac { tau}{ sigma^2} r(t) Theta(V - V_{Re}) + sigma^{-2}(V_m - V_L - psi(V_m)) p(V_m,t) $$ . Which is of the form: $$ frac{ partial p}{ partial V_m} = G(V_m)p(V_m) + H(V_m) $$ . By applying a voltage discretization scheme: $V_k = V_{Lb} + k Delta_V $ with $V_n = V_{Th}$ we can write down: $$ p(V_{k-1}) = p(V_k) e^{ int^{V_k}_{V_{k-1}} G(V)dV} + int^{V_k}_{V_{k-1}} H(V) e^{ int^V_{V_{k-1}}G(U)dU} $$ . We can approximate this as: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V G(V_k)} + Delta_V H(V_k) left( frac{e^{ Delta_V G(V_k)} - 1}{ Delta_V G(V_k)} right) $$ . Substituting back in the necessary formulae for $G$ and $H$ gives: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2}r(t) Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right) $$ . However this still has unknown $r(t)$ in it. If we apply a transform: $q(V,t) = frac{p(V,t)}{r(t)}$ then: $ sum q(V_k) = (r(t))^{-1}$ and: $$ q(V_{k-1}) = q(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2} Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right)$$ . To simplify this expression we define functions $A$ and $B$ so that: $$ q(V_{k-1}) = q(V_k) A(V_k) + Theta(V_k - V_{Re}) B(V_k) $$ . And so we can calculate the firing rate. This scheme has a much better performance than an Euler scheme. We instantiate the scheme with $q(V_n) = 0$, we also select a value $V_{Lb}$ as a cut-off to stop iterating. An implementation of this method can be seen below: . # Evaluating the solution to the Fokker-Planck Equation to calculate the firing rate of an EIF neuron subject to Gaussian white noise import numpy as np # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up additional parameters for solving Fokker-Planck. DelV (mV) and VLb (mV) DelV = 0.001 VLb = -100 Steps = int(np.ceil((Vth - VLb) / DelV)) q = np.zeros(Steps) V = np.arange(Steps)*DelV + VLb # For ease define function psi def psi(V): return DelT * np.exp((V - VT) / DelT) def A(V): return np.exp(DelV * sig**-2 *(V - VL - psi(V))) def B(V): if A(V) == 1.0: return DelV * tau * sig**-2 else: return DelV * tau * sig**-2 * (A(V) - 1) / np.log(A(V)) # Shut off numpy divide errors np.seterr(divide=&#39;ignore&#39;) for i in range(Steps -1, 0, -1): if V[i] &gt; Vr: q[i-1] = q[i]*A(V[i]) + B(V[i]) else: q[i-1] = q[i]*A(V[i]) r = 1/(q.sum()/1000000 + Tref/1000) print(&quot;Firing rate:&quot;, round(r,1), &quot;Hz&quot;) . Firing rate: 21.6 Hz . We can modify the previous EIF firing code to estimate the firing rate, the results should be similar (note: for this I used 10m time steps, it is a slow running code!): . #collapse # Implementation of a noisy EIF neuron using a forward Euler scheme # Reduce N for quicker running code import numpy as np # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 10000000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up voltage Vold (mV) and spike count Sp Vold = Vr Sp = 0 # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: Vnew = Vr Tc -= 1 else: Vtemp = Vold + dt/tau*(VL - Vold) + DelT*dt/tau*np.exp((Vold - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: Vnew = Vr Tc = np.ceil(Tref/dt) Sp += 1 else: Vnew = Vtemp Vold = Vnew print(&quot;Estimated firing rate:&quot;, round(Sp/(N*dt/1000),1), &quot;Hz&quot;) . . Estimated firing rate: 20.2 Hz . Which we can see is similar to the solution of the Fokker-Planck equation. In the limit $N to infty$ and decreasing the lattice sizes these approximations should become much closer. . Conclusion . We have seen that by using the Fokker-Planck framework we are able to calculate the mean firing rate of the EIF neuron. We can also notice that the numerical scheme to integrate the Fokker-Planck runs significantly faster than taking a Monte-Carlo approximation by simulating the EIF directly. We can also notice that the Fokker-Planck framework is easy to extend (e.g. to modulated noise or other applied signals) and further we can extend this to allow for connected networks of neurons (I may write an additional blog post on this in the future but will likely end up being quite similar to this one). . References . https://neuronaldynamics.epfl.ch/online/Ch13.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | How Spike Generation Mechanisms Determine the Neuronal Response to Fluctuating Inputs - Nicolas Fourcaud-TrocmeÂ´, David Hansel, Carl van Vreeswijk, and Nicolas Brunel [2003] | Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based synaptic drive - Magnus J Richardson [2007] | .",
            "url": "https://www.lewiscoleblog.com/neuron-models-3",
            "relUrl": "/neuron-models-3",
            "date": " â€¢ Jan 21, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Neuron Models 2: Exponential Integrate and Fire Model",
            "content": ". This is the second blog post in a series - you can find the previous blog post here . . Integrate and Fire Models . Throughout this blog post we will focus on integrate and fire models. This class of model has been around for a long time, in fact longer than the Hodgkin-Huxley model. The first model was presented by Lapicque in 1907. Since then many alternative formulations have been presented. We can express the models in the form: $$ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $$ . Where: $ tau$ represents membrane time constant ($C_m / g_L$ in notation used previously) $E$ represents the rest potential $ psi(V)$ is the spike generating current term $R_m$ represents membrane resistance ($1/g_L$) $I(t)$ is a function representing an applied current (in the Hodgkin-Huxley example we took a Gaussian white noise) . With integrate and fire models we have the issue that (typically) the action potential will shoot off to infinity. In order to stop this we implement a threshold ($V_{Th}$), when the process reaches this value it is reset (to $V_{Re}$) and the dynamics start again. This is not a big issue because for our purposes we are noly interested in the dynamics of an onset of an action potential, the mechanism of returning to normal levels is not of (much) interest. When modelling we may wish to hold the voltage at $V_{Re}$ following a spike for a short time ($ Delta_{T_{Rf}}$) to reflect the refractory period of a neuron. The refractory period can be modelled stochastically but usually a static value is succficient. . Integrate and fire models are thus defined by the form of the function $ psi(V)$ - this function may be linear or non-linear. Examples include the leaky-integrate and fire model (linear), Fitzhugh-Nagumo (polynomial) and the exponential integrate and fire (non-linear). . From Hodgkin-Huxley to Integrate and Fire . We now take a quick de-tour to justify the use of the integrate and fire model as an approximation to the Hodgkin-Huxley dynamics. . First we notice that gate m operates on a much faster time scale than gates n or h (and similarly much faster than the leak channel which controls the potential dynamics with all gates closed.) Given it is so much faster we can apply an instantaneous approximation, namely: $m(t) = hat{m}(V_m(t))$ that is: the dynamics are defined by the membrane voltage. From plotting gate dynamics we can also observe that gates n and h are approximately translated reflections of each other. As an approximation we can create an adaptation variable $w$ with $n = aw$ and $h = b - w$ for constants $a, b$. We can then write down the equation: $$ C_m frac{dV_m}{dt} = I_p - overline{g_K}(aw)^4(V_m-V_K) - overline{g_{Na}}( hat{m}(V_m))^3(b - w)(V_m-V_{Na}) - overline{g_L}(V_m-V_L) $$ . There is a corresponding equation for the adaptation variable $w$ which we shall not concern ourselves with. We are only interested in the onset of spiking not the refractory period dynamics so we will take $w = w_{rest}$ to be a fixed value. We can therefore express the voltage dynamics as: $$ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) - lambda ( hat{m}(V_m))^3(V_m-V_{Na}) $$ . By collecting terms and some re-arrangement. $g_{eff}, V_{eff}$ and $ lambda$ are all constant values. Therefore via the approximations outlined above we are left with an equation of the form: $$ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $$ . Namely an integrate and fire model. . Exponential Integrate and Fire . Continuing with the line of reasoning above we shall consider the function: $ hat{m}(V_m)$. We assume the dynamics are so rapid that they are essentially in equilibrium: $$ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m = 0 $$ So: $$ hat{m}(V_m) = frac{ alpha_m(V_m)}{ alpha_m(V_m) + beta_m(V_m)}$$ . Since Hodgkin-Huxley suggested the following forms of these equations: $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ . We can approximate $ hat{m}(V_m)$ with a logistic function: $$ hat{m}(V_m) approx (1 + e^{- beta(V_m - theta)})^{-1}$$ We can then express the Taylor expansion of this as: $$ hat{m}(V_m) = sum (-1)^k e^{ beta(V_m - theta)(1+k)}$$ Which we can see from the expansion of $1/(1+y)$ with $y = e^{- beta(V_m - theta)}$. So a first order approximation is: $$ hat{m}(V_m) approx e^{ beta(V_m - theta)}$$ . Then the current corresponding to the sodium channel can be expressed approximately: $$I_{Na} = g_{Na}(b - w_{rest})(V_m - V_{Na}) e^{3 beta(V_m - theta)} $$ . If we take $(V_m - V_{Na}) approx (V_{rest} - V_{Na}) &lt; 0$ as an approximation we get approximate voltage dynamics as: $$ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) + hat{ lambda} e^{ beta (V_m - theta)} $$ . This is known as the exponential integrate and fire (EIF) model.This model has been shown to fit experimental data (and the Hodgkin-Huxley model) very well in practice. Typically we use a parameterization of the spiking term: $$ psi(V) = Delta_T e^{ left( frac{V - V_T}{ Delta_T} right)} $$ We can see this model is highly non-linear and without applying the threshold mechanics the membrane potential would shoot to infinity. The 2 new parameters in this model are: $V_T$ which represents the voltage scale at which the exponential term becomes significant in the dynamics $ Delta_T$ representing the sharpness of the spike . With a Gaussian white noise term $ xi_t$ we can fully specify the dynamics using: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . As before we can solve this using a foward Euler scheme (although the forcing term is non-linear this still provides a reasonable solution for small enough time steps.) An implementation of this can be seen below: . # Implementation of a noisy EIF neuron using a forward Euler scheme import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 50000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up arrays for time T (ms) and voltage V (mV) T = np.arange(N) * dt V = np.zeros(N) V[0] = Vr # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: V[i] = Vr Tc -= 1 else: Vtemp = V[i-1] + dt/tau*(VL - V[i-1]) + DelT*dt/tau*np.exp((V[i-1] - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: V[i] = Vr Tc = np.ceil(Tref/dt) else: V[i] = Vtemp # Plot voltage trajectory plt.plot(T, V) plt.xlabel(&quot;Time (ms)&quot;) plt.ylabel(&quot;Membrane Potential (mV)&quot;) plt.title(&quot;Membrane Voltage Trajectory&quot;) plt.show() . Conclusion . The voltage trajectory displays realistic neuron dynamics for the onset of spiking. However as expected through the use of the refractory period implementation the depolarizing phase is not captured well. This is ok since we consider the information to be carried by the spike itself not the behaviour shortly afterwards. We can also see that this implementation is considerably simpler than that of Hodgkin-Huxely since there is only one ODE. This is of benefit when modelling large networks of neurons where time/computational constraints become a consideration. . It has been shown that the EIF model can predict spiking behaviour very well in practice, despite the somewhat cavalier assumptions made during its derivation from the Hodgkin-Huxley. . In a future blog post we will make use of the mathematical tractability of the EIF model to analyse neurons in more depth. . References . https://neuronaldynamics.epfl.ch/online/Ch5.S2.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | . . This blog post is the second part of a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/neuron-models-2",
            "relUrl": "/neuron-models-2",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Neuron Firing Models: Hodgkin-Huxley Model",
            "content": "Some (very basic) Biology . We start with a very limited description of what a neuron is and the mechanism by which it fires. For sake of completeness by neuron we will refer to pyramidal neurons, they make up a large proportion of neurons within the cortex of mammals. Other types of neurons are specialised for different functions. This background will be very brief and not cover the biology in any great detail. The literature and data on this topic is vast and I will not do it justice so any interested readers should look towards biology textbooks for more detailed descriptions. . The neuron is made up of 3 main components: a soma (cell body), an axon and many dendrites. In laymans terms the axon is the &quot;output&quot; of the neuron while the dendrites are &quot;inputs&quot; to the soma. The axon is covered in a myelin sheath which acts &quot;insulation&quot; which can &quot;speed up&quot; signal flow. Dendrites can futher be broken down into the apical dendrite, basal dendrites and dendritic spines. Both dendrites and axons are highly branched and a single neuron can be linked to many 1000s of others. Signals are passed through synapses. Synaptic inputs can be either excitatory (making the target neuron more likely to fire) or inhibitory (less likely). . . The details of axons and dendrites are not that important for our purposes right now. Instead we are interested in the soma: essentially where the signal is generated. The cell wall contains many voltage gated ion channels, most notably: sodium (Na+) and potassium (K+) channels. There are 2 forces acting on ions inside/outside of the soma: namely the electrical potential within the body and the concentration gradient. Resting membrane potential is around -40mv to -90mv, in this state there is a higher concentration of potassium ions inside the cell than outside and the reverse for sodium ions. The channel for potassium is highly permeable and so potassium ions flow into the soma, the sodium channel is semi-permeable so sodium slowly flow out of the soma, to maintain a constant negative potential the cell &quot;pumps&quot; ions in the reverse direction to maintain equilibrium. . An action potential (neuron spike) is a shift away from the negative equilibrium membrane potential to a positive potential. This is as a result of ion flows due to voltage controlled gates. There are 3 gates of interest here: . Sodium activation gate - normally closed but opens with positive potential | Sodium inactivation gate - normally open but closes with positive potential | Potassium inactivation gate - normally closed but opens with large positive potential | We can break down the events that cause the action potential then as: . Depolarisation - A depolarisation event occurs (e.g. a signal from the dendrites) which brings the cell&#39;s membrane potential to 0mv. This is as a result of positive charged ions flowing into the cell. As the potential increases to some threshold the sodium activation gate is opened which allows more positively charged sodium ions to enter. The potential then becomes positive. | Repolarisation - The positive potential causes the sodium inactivation gate to close preventing more sodium ions entering. Meanwhile the potassium inactivation gate opens, since the concentration of potassium ions inside the cell is much greater than outside this leads to an outflow of potassium. The removal of positively charged ions moves the cell back towards equilibrium. | Refractory Period - The potassium channel stays open slightly past the equilibrium point and the membrane potential becomes too negative (hyperpolarises). As the potassium channel closes the potential tends back to equilibrium. There is a short period after an action potential where the neuron is unable to fire again. | (Source: https://teachmephysiology.com/wp-content/uploads/2018/08/action-potential.png) . In the rest of this blog we will look at models of action potentials and will try and keep this real world description of action potentials in mind. . Hodgkin Huxley Model . We now consider a spiking neuron model as presented by Alan Hodgkin and Andrew Huxley in 1952, this model won them the Nobel prize for physiology in 1963. The model was developed by studying the axon of a giant squid neuron. The model itself tries to mimic the gates and ionic channels described above. Diagramatically we can represent the model as: . The voltage controlled ionic gates conductances are represented in the diagram by gn (only one represented in the diagram) and there is a leak conductance represented by gl. Cm represents the membrane capacitance and there is an external stimulus Ip which represents inputs from other neurons (or a test current applied by a probe in experiment). We can then represent the model as a set of 4 interacting PDEs: $ frac{dV_m}{dt} = frac{I_p}{C_m} - frac{ overline{g_K}n^4}{C_m}(V_m-V_K) - frac{ overline{g_{Na}}m^3h}{C_m}(V_m-V_{Na}) - frac{ overline{g_L}}{C_m}(V_m-V_L) $ $ frac{dn}{dt} = alpha_n(V_m)(1-n) - beta_n(V_m)n $ $ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m $ $ frac{dh}{dt} = alpha_h(V_m)(1-h) - beta_h(V_m)h $ . Where the functions $ alpha_x , beta_x$ as suggested by Hodgkin and Huxley are: $ alpha_n(V_m) = frac{0.01(10-V_m)}{e^{(1.0-0.1V_m)}-1} $ $ beta_n(V_m) = 0.125 e^{-V_m / 80} $ $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ $ alpha_h(V_m) = 0.07 e^{-V_m / 20} $ $ beta_h(V_m) = frac{1}{e^{(3.0-0.1V_m)}+1} $ . The following values are suggested for the model constants: $C_m = 1 mu F /cm^2$ - Capacitance per unit surface area of neuron membrane $ overline{g_{Na}} = 120 mu S / cm^2$ - Voltage controlled sodium conductance per unit surface area $ overline{g_{K}} = 36 mu S / cm^2$ - Voltage controlled potassium conductance per unit surface area $ overline{g_{L}} = 0.336 mu S / cm^2$ - Voltage controlled leak conductance per unit surface area $V_{Na} = 115mV$ - Sodium voltage gradient $V_{K} = -12mV$ - Potassium voltage gradient $V_{L} = 10.613mV$ - Leak current voltage gradient . This is a non-linear system of differential equations and as such it is not possible to study analytically. However we can simulate this numerically. For this example we will assume that the external stimulus follows a white noise (Brownian motion). Under certain conditions this can be a reasonable assumption. We will define this as follows: $I_p(t) = sigma W_t$ for some positive constant $ sigma$ with units $ mu A / cm^2$ . By introducing stochastic noise such as this we unfortunately are unable to use any of the Python pre-made ODE integrators (e.g. scipy.integrate.ode) instead we will rely on a forward Euler scheme which will perform well enough for our purposes assuming the discrete time steps are selected to be sufficiently small. An example implementation of this can be seen below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed np.random.seed(123) # Set time step dt (ms) and number of steps for simulation N dt = 0.001 N = 50000 # Set up time array T T = np.arange(N) * dt # Set model inputs, using nomenclature: K - Potassium, Na - Sodium, L - Leak # Set membrane capacitance per unit area (uF/cm^2) Cm = 1.0 # Set applied current density volatility (uA/cm^2) sigma = 2 # Set channel conductance per unit area (mS/cm^2) gK = 36.0 gNa = 120.0 gL = 0.3 # Set voltage gradients (mV) VK = -12.0 VNa = 115.0 VL = 10.613 # Define ion channel rate functions def alpha_n(Vm): return (0.01 * (10.0 - Vm)) / (np.exp(1.0 - (0.1 * Vm)) - 1.0) def beta_n(Vm): return 0.125 * np.exp(-Vm / 80.0) def alpha_m(Vm): return (0.1 * (25.0 - Vm)) / (np.exp(2.5 - (0.1 * Vm)) - 1.0) def beta_m(Vm): return 4.0 * np.exp(-Vm / 18.0) def alpha_h(Vm): return 0.07 * np.exp(-Vm / 20.0) def beta_h(Vm): return 1.0 / (np.exp(3.0 - (0.1 * Vm)) + 1.0) # Define applied signal function - can be replaced to investigate different signals def Ip(sig, t, V): return np.sqrt(dt) * sig * np.random.normal(0, 1, 1) # Set up arrays for dynamic results Vm = np.zeros(N) n = np.zeros(N) m = np.zeros(N) h = np.zeros(N) Signal = np.zeros(N) # Initialize the system V0 = 0 Vm[0] = V0 n[0] = alpha_n(V0) / (alpha_n(V0) + beta_n(V0)) m[0] = alpha_m(V0) / (alpha_m(V0) + beta_m(V0)) h[0] = alpha_h(V0) / (alpha_h(V0) + beta_h(V0)) # Loop through Euler-Forward scheme for i in range(1, N): Signal[i] = Ip(sigma, i*dt, Vm[i-1])/Cm Vm[i] = Vm[i-1] + Signal[i] - gK*np.power(n[i-1],4)*(Vm[i-1]-VK)*dt/Cm - gNa*np.power(m[i-1],3)*h[i-1]*(Vm[i-1]-VNa)*dt/Cm - gL*(Vm[i-1]-VL)*dt/Cm n[i] = n[i-1] + (alpha_n(Vm[i-1])*(1 - n[i-1]) - beta_n(Vm[i-1])*n[i-1])*dt m[i] = m[i-1] + (alpha_m(Vm[i-1])*(1 - m[i-1]) - beta_m(Vm[i-1])*m[i-1])*dt h[i] = h[i-1] + (alpha_h(Vm[i-1])*(1 - h[i-1]) - beta_h(Vm[i-1])*h[i-1])*dt # Plot sample path plt.plot(T, Vm) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Membrane voltage mV&quot;) plt.title(&quot;Hodgkin-Huxley Spiking&quot;) plt.show() # Plot gate opening over time plt.plot(T, n, label=&quot;n - K&quot;) plt.plot(T, m, label=&quot;m - Na&quot;) plt.plot(T, h, label=&quot;h - Na&quot;) plt.legend(loc=&quot;upper right&quot;) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Gate Proportion&quot;) plt.title(&quot;Gate Dynamics&quot;) plt.show() # Plot Limit trajectories plt.plot(n, Vm, label=&quot;n-Vm&quot;) plt.plot(m, Vm, label=&quot;m-Vm&quot;) plt.plot(h,Vm, label=&quot;h-Vm&quot;) plt.legend() plt.title(&quot;Cycle Trajectories&quot;) plt.xlabel(&quot;Gate Proportion&quot;) plt.ylabel(&quot;Membrane Voltage mV&quot;) plt.show() . Conclusion . In this blog post we have seen some of the basic biolgical and physical processes behind generating an action potential. Through the Hodgkin-Huxley model we have looked at dynamics of 3 voltage controlled ionic gates which control the membrane potential and neuron spiking. We can see that this model is fairly complicated, if we wanted to model many neurons it can become problematic. In a future blog post we will look at ways to simplify this model. . . This is the first blog post in a series - you can find the next blog post here .",
            "url": "https://www.lewiscoleblog.com/neuron-models-1",
            "relUrl": "/neuron-models-1",
            "date": " â€¢ Jan 7, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Insurance Aggregation Model",
            "content": "Note: Thoughts epressed within this workbook are my own and do not represent any prior, current nor future employers or affiliates . Background - An Overview of Modelling in Specialty Insurance . Within the specialty insurance space we are typically insuring economic interests of relatively rare events of high impact (for example: buildings damaged by hurricanes, aircraft crashes, impact of CEO wrong-doing, and so on.) These events are typically broken up into 2 broad classes: . Property | Casualty | Hence why the term &quot;P&amp;C&quot; insurer is sometimes used. Property risks are, as the name suggests, related to property - historically phyiscal property but now can include non-physical property (e.g. data). Owing to the relative simplicity of these risks there is an entire universe of quantitative models that exist for risk management purposes, in particular there are a handful of vendors that create &quot;natural catastrophe&quot; (nat-cat) models. These models are sophisticated and all essentially rely on GIS style modelling: a portfolio of insured risks are placed on a geographical map (using lat-long co-ordinates) then &quot;storm tracks&quot; representing possible hurricane paths are run through the portfolio resulting in a statistical distribution of loss estimates. For other threats such as earthquakes, typhoons and wild-fires similar methods are used. . These nat-cat models allow for fairly detailed risk management procedures. For example it allows insurers to look for &quot;hot spots&quot; of exposure and can then allow for a reduction in exposure growth in these areas. They allow for counter-factual analysis: what would happen if the hurricane from last year took a slightly different track? It allows insurers to consider marginal impacts of certain portfolios, for example: what if we take on a portfolio a competitor is giving up, with our current portfolio will it aggregate or diversify? As a result of this explanatory power natural catastrophe risks are now well understood and for all intents and purposes these risks are now commodified and have allowed insurance linked securities (ILS) to form. &lt;/br&gt; . Before this analytics boom specialty insurers made their money in natural catastrophe and property insurance, as such there has been a massive growth in recent years in the Casualty side of the business. Unfortunately the state of modelling on that side is, to put it politely, not quite at the same level. . As one would expect nat-cat model vendors have tried, and continue to try, to force the casualty business into their existing natural catastrophe models. This is a recipe for disaster as the network structure for something like the economy does not naturally lend itself to a geogprahic spatial representation. There is also a big problem of available data. Physical property risks give rise to data that is easy to cultivate. Casualty data is either hard to find or impossible - why would any corporation want to divulge all the details of their interactions? As such it does not appear that these approaches will become useful tools in this space. . To fill this void there has been an increasing movement of actuaries into casualty risk modelling roles. While this overcomes some of the problems that face the nat-cat models they also introduce a whole new set of issues. Traditional actuarial models relying on statistical curve fitting to macro-level data. Even assuming a suitable distribution function can be constructed it is of limited use for risk management as it only informs them of the &quot;what&quot; but not the &quot;why&quot;, making it hard to orient a portfolio for a specific result. More recently actuaries have slowly began to model individual deals at a micro-level and aggregate them to get a portfolio view. To do this a &quot;correlation matrix&quot; is typically employed, this aproach also has issues: . Methods don&#39;t scale well with size, adding new risks often require the entire model to be recalibrated taking time and effort. | They either require a lot of parameters or unable to capture multi-factor dependency (e.g. a double trigger policy where each trigger has its own sources of accumulation). | It is usually not possible to vary the nature of dependency (e.g. add tail dependence or non-central dependency) | Results are often meaningless in the real world, it is usually impossible to perform counter-factual analysis | To bridge this gap I have developed a modelling framework that allows for the following: . Modelling occurs at an individual insured interest level | Modelling is scalable in the sense that adding new insured interests requires relatively few new parameters and calibrations | Counter-factual analysis is possible and the model can be interpreted in terms of the real world | The framework itself is highly parallelizable, whereas nat-cat models require teams of analysts, large servers and IT infrastructure this framework lends itself to being run by multiple people on regular desktop computers with little additional workflow requirements | A First Step: A Simple Driver Method . We will now look at a very stylised model of aggregation that will form a foundation on which we can build the more sophisticated model framework. We call this method of applying dependence a &quot;driver method&quot;, it is standard practice for applying dependence in banking credit risk models where there can be many thousands of risks modelled within a portfolio. The interpretation is that there is a central &quot;driver&quot;, each individual risk is &quot;driven&quot; by this and since this is common to all risks there is an induced dependence relation between them. . The model relies on the generalised inverse transform method of generating random variates. Stated very simply: if you apply the inverse CDF of a random variable to a random number (U[0,1] variate) you will have samples distributed as that random variable. Therefore in order to apply dependence in a general form we only need to apply dependence between U[0,1] variates. We will also exploit the fact that normal distributions are closed under addition (that is the sum of normals is normal). . We can now express the model as follows: . We sample standard normal (N(0,1)) variates to represent the &quot;driver&quot; variable | For each risk sample an additional set of normal variates | Take a weighted sum of the &quot;driver&quot; and the additional normal variates to give a new (dependent) normal variate | Standardise the result from step 3) and convert to a U[0,1] variable using the standard gaussian CDF | Use an inverse transform to convert the result of step 4) to a variate as specified by the risk model | We can see that this method is completely general, it does not depend on any assumption about the stand-alone risk model distributions (it is a &quot;copula&quot; method). Another observation is that the normal variates here are in some sense &quot;synthetic&quot; and simply a tool for applying the dependence. . For clarity an example is presented below: . # Simple driver method example # We model a central driver Z # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates X1 and X2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate temporary synthetic variable X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Use normal CDF to convert X synthetic variables to uniforms U U1 = norm.cdf(X1) U2 = norm.cdf(X2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.4628059800990357 . The example above shows we have correlated gamma variates with around a 50% correlation coefficient (in this case we could calculate the correlation coefficient analytically but it is not necessary for our purposes, as we create more sophisticated models the analytic solutions become more difficult/impossible). . Even from this example we can see how models of this form provide superior scalability: for each additional variable we only need to specify 1 parameter: the weight given to the central driver. In contrast a &quot;matrix&quot; method requires each pair-wise combination to be specified (and then we require a procedure to convert the matrix to positive semi-definite form in order to apply it). Say our model requires something more sophisticated: say the sum of a correlated gamma and a weibull distribution - the number of parameters in a matrix representation grows very quickly. However it is worth noting we do lose some control, by reducing the number of parameters in this way we lose the ability to express every possible correlation network. However in most cases this is not a big problem as there is insufficient data to estimate the correlation matrix anyway. . It is worth pointing out that the type of dependency applied here is a &quot;rank normal&quot; dependency - this is the same dependency structure as in a multi-variate normal distribution, albeit generalised to any marginal distribution. . An Extension to the Simple Driver Method . We can extend the model above by noticing the following: there is nothing stopping the &quot;synthetic&quot; variables being considered drivers in their own right. Gaussians being closed under addition does not require that each variable needs to be independent, sums of rank correlated normals are still normal! We can thus extend the model to: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Use normal CDF to convert sX synthetic variables to uniforms U U1 = norm.cdf(sX1) U2 = norm.cdf(sX2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7851999480298125 . As before we have ended up with rank-normal correlated gamma variates. This time we have 3 potential &quot;driver&quot; variables Z, X1, X2 - all correlated with each other. It is not hard to see how this procedure can be iterated repeatedly to give arbitrarily many correlated driver variables. Further we can imagine these variables being oriented in a hierarchy, Z being at the bottom layer, X1 and X2 being a layer above, and so on. . What is a Driver? . We should now take a step back and think about the implications for the insurance aggregation problem. As stated previously this method allows us to define dependency with far fewer parameters than using a matrix approach. When you start getting into the realms of 100,000s of modelled variables this becomes increasingly important from a calibration perspective. . However there are other benefits: for example we can look at how the model variables relate to the driver variables. For example we can ask questions such as: &quot;What is the distribution of modelled variables when driver Z is above the 75th percentile&quot; and so on. This is a form of counter-factual analysis that can be performed using the model, with the matrix approaches you get no such ability. For counter-factual analysis to be useful however we require real-world interpretations of the drivers themselves. By limiting ourselves to counter-factual analysis based on driver percentiles (e.g. after the normal cdf is applied to Z, X1, X2 - leading to uniformly distributed driver variables) we make no assumption about the distribution about the driver itself, only its relationship with other drivers. . By not making a distributional assumption a driver can represent any stochastic process. This is an important but subtle point. For example we could create a driver for &quot;global economy&quot; (Z) and by taking weighted sums of these create new drivers &quot;US economy&quot; (X1) and &quot;european economy&quot; (X2). In this example there may be data driven calibrations for suitable weights to select (e.g. using GDP figures) however it is also relatively easy to use expert judgement. In my experience it is actually easier to elicit parameters in this style of model compared to &quot;correlation&quot; parameters given this natural interpretation. . Given this natural interpretation we can quite easily begin to answer questions such as: &quot;What might happen to the insurance portfolio in the case of a european economic downturn?&quot; and so on. Clearly the detail level of the driver structure controls what sort of questions can be answered. . As stated previously we can repeat the mechanics of creating drivers to create new &quot;levels&quot; of drivers (e.g. moving from &quot;european economy&quot; to &quot;French economy&quot;, &quot;UK economy&quot; and so on). We can also create multiple &quot;families&quot; of driver, for example in addition to looking at economies we may consider a family relating to &quot;political unrest&quot;, again this could be broken down into region then country and so on. Other driver families may not have a geographic interpretation - for example commodity prices. In some cases the families may be completely independent of each other, in other cases they can depend on each other (e.g. commodity prices will have some relationship with the economy). . In the examples so far we have presented a &quot;top down&quot; implementation in our examples: we start by modelling a global phenomena and then build &quot;smaller&quot; phenomena out of these. There is nothing special about this, we could have just as easily presented a &quot;bottom up&quot; implementation: take a number of &quot;Z&quot; variables to represent regions and combine these to form an &quot;X&quot; representing a global variable. Neither implementation is necessarily better than another and mathematically they lead to equivalent behaviours (through proper calibration). In practice however I have found the &quot;top down&quot; approach works better, typically you will start with a simple model and through time it can iterate and become more sophisticated. The top down approach makes it easier to create &quot;backward compatability&quot; which is a very useful feature for any modelling framework (e.g. suppose the first iteration of the framework only considers economic regions, next time a model is added which requires country splits - with top down adding new country variables keeps the economic regions identical without requiring any addtional thought.) . The need for more Sophistication: Tail Dependence . Unfortunately the model presented so far is still quite a way from being useful. We may have found a way of calibrating a joint distribution using relatively few (O(N)) parameters and can (in some sense) perform counter-factual analysis, but there is still a big issue. . So far the method only allows for rank-normal joint behaviour. From the analysis of complex systems we know that this is not necessarily a good assumption (please see other blog posts for details). We are particularly interested in &quot;tail dependence&quot;, in layman&#39;s terms: &quot;when things go bad, they go bad together&quot;. Tail dependence can arise for any number of reasons: . Structual changes in the system | Feedback | State space reduction | Multiplicative processes | Herd mentality/other human behaviours | And many others | . Given the framework we are working within we are not particularly interested in how these effects occur, we are just interested in replicating the behaviour. . To do this we will extend the framework to cover a multivariate-student-T dependence structure. To do this we note the following: $$ T_{ nu} sim frac{Z} { sqrt{ frac{ chi^2_{ nu}} { nu}}} $$ Where: $ T_{ nu} $ follows a student-t distribution with $ nu$ degrees of freedom $ Z $ follows a standard normal $N(0,1)$ $ chi^2_{ nu} $ follows Chi-Square with $ nu$ degrees of freedom . Therefore we can easily extend the model to allow for tail dependence. . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu = 3 Chi = chi2.rvs(nu, size=SIMS) sX1 /= np.sqrt(Chi / nu) sX2 /= np.sqrt(Chi / nu) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu) U2 = t.cdf(sX2, df=nu) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7907911109201866 . Adding Flexibility . We can further extend this model by allowing each model variate to have its own tail-dependence. Why is this important one might ask? In the case of this framework we are spanning many different models, selecting a single degree of tail dependence might not be suitable for all variables. We can do this via applying another inverse transform: $$ T_{ nu} sim frac{Z} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ As before but where: $U$ follows a uniform U[0,1] distribution $F^{-1}_{ chi^2_{ nu}}$ is the inverse cdf of $ chi^2_{ nu}$ . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 U = np.random.rand(SIMS) Chi1 = chi2.ppf(U,df=nu1) Chi2 = chi2.ppf(U, df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7703228652641819 . There is a small practical issue relating to multivariate student-t distributions: namely that we lose the ability to assume independence. This is a direct result of allowing for tail dependence. In many situations this is not an issue, however within this framework we have models covering very disperate processes some of which may genuinely exhibit independence. To illustrate this issue we will re-run the existing model with zero driver weights (&quot;attempt to model independence&quot;): . Estimated Pearson Correlation Coefficient: 0.08220534833363176 . As we can see there is a dependence between Y1 and Y2 0 clearly through the chi-square variates. We can overcome this issue by &quot;copying&quot; the driver process. The common uniform distribution is then replaced a number of correlated uniform distributions. We can then allow for independence. An implemntation of this can be seen in the code sample below: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7406745557389065 . To show this allows full independence we repeat the zero-weight example: . Estimated Pearson Correlation Coefficient: -0.01456173215652803 . We can see that this is a much better scatter plot if we are looking for independence! . Non-Centrality . We now extend this model yet further. So far we have allowed for tail dependence however it treats both tails equally. In some instances this can be problematic. For example if we rely on output from the framework to do any kind of risk-reward comparison the upisde and downside behaviour are both important. While it is easy to think of structural changes leading to a downside tail dependence an upside tail dependence is typically harder to justify. We can allow for this with a simple change to the model, namely: $$ T_{ nu, mu} sim frac{Z + mu} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ The addition of the $ mu$ parameter means that $T_{ nu, mu}$ follows non-central student-t distribution with $ nu$ degrees of freedom and non-centrality $ mu$. Details of this distribution can be found on wikipedia. By selecting large positive values of $ mu$ we can create tail dependence in the higher percentiles, large negative values can create tail dependence in the lower percentiles and a zero value leads to a symmetrical dependency. Adjusting the code futher we get: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers # We add non-centrality through an additive scalar import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Specify the non-centrality values nc1 = -2 nc2 = -2 # Use non-central t CDF to convert sX synthetic variables to uniforms U U1 = nct.cdf(sX1+nc1, nc=nc1, df=nu1) U2 = nct.cdf(sX2+nc2, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7100911602838634 . In the code example we have selected a non-centrality of -2 which is a fairly large negative value, we can see the dependency increasing in the lower percentiles (clustering around (0,0) on the plot). . Temporal Considerations . So far we have essentially considered a &quot;static&quot; model, we have modelled a number of drivers which represent values at a specific time period. For the majority of insurance contracts this is sufficient: we are only interested in losses occuring over the time period the contract is active. However in some instances the contracts relate to multiple time periods and it does not make sense to consider losses over the entire lifetime. Moreover it is not ideal to model time periods as independent from one another, to take the US economy example: if in 2020 the US enters recession it is (arguably) more likely that the US will also stay in recession in 2021. Clearly the dynamics of this are very complex and constructing a detailed temporal model is very difficult, however for the sake of creating the drivers we do not need to know the exact workings. Instead we are looking for a simple implementation that gives dynamics that are somewhat justifiable. . Fortunately it is relatively easy to add this functionality to the model framework we have described so far. Essentially we will adopt a Markovian assumption whereby a driver in time period t+1 is a weighted sum of its value at time t and an idiosyncratic component. Of course this is not a perfect description of the temporal behaviour of every possible driver but it shouldn&#39;t be completely unjustifiable in most instances and the trajectories shouldn&#39;t appear to be totally alien (e.g. US economy being in the top 1% one year immediately followed by a bottom 1% performance very frequently). . To illustrate this please see the code example below, for brevity I will change the model code above to a functional definition to avoid repeating blocks of code. . # Creating temporally dependent variables import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Define function to create correlated normal distributions def corr_driver(): # Create driver Z Z = np.random.normal(0, 1, SIMS) # Create drivers X1, X2 X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) return np.array([X1, X2]) # Create drivers variables for time periods t0 and t1 driver_t0 = corr_driver() driver_t1 = 0.5 * driver_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Create copy of drivers for tail process time periods t0 and t1 tail_t0 = corr_driver() tail_t1 = 0.5 * tail_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Define a standardise function def standardise(x): return (x - x.mean()) / x.std() # Create sythetic variables sX1 sX2 for variable 1 and 2 at times t0 and t1 # Note depending on the model idiosyncratic components may also be dependent sX1t0 = standardise(0.25*driver_t0[0] + 0.5*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1t1 = standardise(0.25*driver_t1[0] + 0.5*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t0 = standardise(0.5*driver_t0[0] + 0.25*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t1 = standardise(0.5*driver_t1[0] + 0.25*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Repeat synthetic variable construction for tail process sX1tailt0 = standardise(0.25*tail_t0[0] + 0.5*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1tailt1 = standardise(0.25*tail_t1[0] + 0.5*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt0 = standardise(0.5*tail_t0[0] + 0.25*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt1 = standardise(0.5*tail_t1[0] + 0.25*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Simulate Chi-Square for tail-dependence t0 and t1 nu1 = 2 nu2 = 4 Chi1t0 = chi2.ppf(norm.cdf(sX1tailt0),df=nu1) Chi2t0 = chi2.ppf(norm.cdf(sX2tailt0), df=nu2) sX1t0 /= np.sqrt(Chi1t0 / nu1) sX2t0 /= np.sqrt(Chi2t0 / nu2) Chi1t1 = chi2.ppf(norm.cdf(sX1tailt1),df=nu1) Chi2t1 = chi2.ppf(norm.cdf(sX2tailt1), df=nu2) sX1t1 /= np.sqrt(Chi1t1 / nu1) sX2t1 /= np.sqrt(Chi2t1 / nu2) # Specify the non-centrality values nc1 = 2 nc2 = 2 # Use non-central t CDF to convert sX synthetic variables to uniforms U for t0 and t1 U1t0 = nct.cdf(sX1t0+nc1, nc=nc1, df=nu1) U2t0 = nct.cdf(sX2t0+nc2, nc=nc2, df=nu2) U1t1 = nct.cdf(sX1t1+nc1, nc=nc1, df=nu1) U2t1 = nct.cdf(sX2t1+nc1, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 at t0 and t1 Y1t0 = gamma.ppf(U1t0, 2) Y2t0 = gamma.ppf(U2t0, 3) Y1t1 = gamma.ppf(U1t1, 2) Y2t1 = gamma.ppf(U2t1, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1t0, Y1t1) plt.xlabel(&#39;Y1(t=t0)&#39;) plt.ylabel(&#39;Y1(t=t1)&#39;) plt.show() correl = np.corrcoef(Y1t0, Y1t1) print(&quot;Estimated Pearson Auto-Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Auto-Correlation Coefficient: 0.37600307233845764 . In this code example we created to variables Y1 and Y2, each one taking a value from a Gamma distribution at times t0 and t1. Y1 and Y2 have a dependency between eachother but also temporally. . As with any temporal model the time period chosen is very important, typically for insurance contracts yearly time periods make sense. However in one particular model I developed there was a need for monthly simulations, rather than re-parameterising the entire central driver structure to work on a monthly basis (creating lots of extra data that will not be used by the vast majority of the models) I applied a &quot;Brownian Bridge&quot; type argument to interpolate driver simulations for each month. . Notes on Implementation . In this blog post I have not included the code exactly as it is implemented in production since this is my employer&#39;s IP. The implementation presented here is not very efficient and trying to run large portfolios in this way will be troublesome. In the full production implementation I used the following: . Strict memory management as the this is a memory hungry program | Certain aspects of the implementation are slow in pure python (and even Numpy) Cython and Numba are used for performance | The Scipy stats module is convenient but restrictive, it is better to either use the Cython address for Scipy special functions or implement functions from scratch. By implementing extended forms of some of the distribution functions one is also able to allow for non-integer degrees of freedom which is useful | The model naturally lends itself to arrays (vectors, matrices, tensors) however these tend to be sparse in nature, it is often better to construct &quot;sparse multiply&quot; type operations rather than inbuilt functions like np.dot | Conclusion . This blog posts represents the current iteration of the aggregation framework I have developed. It is considered a &quot;version 0.1&quot; implementation and is expected to develop as we use it more extensively and uncover further properties or issues. For example it is clear regardless of parameters selected the joint behaviour will always be (approximately) elliptical, as presented it is not possible to implement non-linearities (e.g. the price of some asset will only attain a maximum/minimum value dependent on some other driver indicator). It is not difficult to implement ideas like this when the need arises, the difficulty becomes more around how to implement the idea in a seamless way. . There are a couple of additional benefits to this framework which we have not mentioned, I will outline these here briefly: . It is possible to parallelise this process quite effectively as there are minimal bottlenecks/race conditions | The driver variables can be generated centrally and models can link to this central variable repository. From a work-flow perspective this means that individual underwriting teams can run models independently (quickly) leaving the risk teams to collate an analyse the results. (Sometimes called a federated workflow.) | The federated workflow means no specialist hardware is required, even very large portfolios can be run on standard desktops/laptops. | The current production version of this framework has around 5-10,000 driver variables (&quot;X1, X2&quot;) over 5 different hierarchical layers. These influence dependence between around 500,000 individual modelled variables (&quot;Y1, Y2&quot;) with 20 time periods (&quot;t0, t1&quot;). The quality of risk management analysis and reporting has increased dramatically as a result. . There are still some things left to do in relation to this framework and the work is on-going. These include: . Work relating to calibration and how to do this as efficiently as possible | Further work on increasing code efficiency | Further mathematical study of the framework&#39;s parameters | Study of the implied network behaviour: since we&#39;re placing risks on a network (driver structure) can we gain additional insight by considering contagion, critical nodes, etc.? | Further improvements to the workflow, how the model data is stored/collated etc. |",
            "url": "https://www.lewiscoleblog.com/insurance-aggregation-model",
            "relUrl": "/insurance-aggregation-model",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Welcome",
          "content": "Lewis Cole 2020 . Hello, and welcome to my new blog. I have never written a blog before and so this is likely to be a bit of a work in progress slowly evolving in time. . Within this blog I aim to talk about some ideas that are of interest to me. My interests are quite broad and so there will likely be a wide variety of topics discussed. I suppose most, if not all, of these interests could be placed under the broad umbrella of â€œmodelsâ€ or â€œsimulationâ€. Some particular areas or topics I wish to write about include: . Non-linear dynamics | Systems out of equilibrium | Fat-tail and extreme value statistics | Non-ergodicity | Path dependence | Individual vs Collective phenomena | Agent based modelling | Networks | Machine Learning / Data Analysis | Mathematics, Probability, Computer Science generally | Inter-disciplinary study | . Many of these areas could be considered â€œcomplexâ€ or â€œcomplex systemsâ€ although I am not a big fan of the term due to lack of a consistent definition. In many cases we must rely on computational methods since traditional analytic methods tend to fall down. As a result most blog posts will contain sample code, I will write this in Python (with a variety of libraries/packages) owing to ease of understanding and itâ€™s ubiquity. As such I will also write about more computational considerations such as: . Python packages | Other languages | Writing performant python | Optimization techniques | and so on | . From time to time I might also include more â€œthought piecesâ€ on news/recent research or book reviews or similar. . Where applicable I will try and assume no specific knowledge of a particular subject and try and build up to a somewhat sophisticated level of understanding. However I will be forced to assume a certain level of mathematical/statistical/programming understanding as I would rather not write articles on the fundamentals, where possible I will try and mention the name of techniques used so if they appear unfamiliar it will at least be possible to search for resources online. . To create this blog I used the fast.ai fastpages which has made the job much easier. I have a great deal of gratitude to Jeremy Howard, Hamel Husain and the folks at fast.ai for making this simple as possible so I can focus on creating content instead of worrying about the technicalities of creating the blog. . Thank you for your time, I hope you enjoy my blog posts. . If you would like to get in contact with me you can via my twitter account or via this blogâ€™s email: .",
          "url": "https://www.lewiscoleblog.com/welcome/",
          "relUrl": "/welcome/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

}