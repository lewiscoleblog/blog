{
  
    
        "post0": {
            "title": "",
            "content": "Outline of the Problem . Python at its core is slow for certain things. By being a dynamically typed and interpreted language you incur certain runtime overheads. In some cases these are not much of an issue. At other times they can be critical. . Typically we will try and &quot;vectorize&quot; the code as much as possible (avoiding extraneous loops) and force as much code as we can into NumPy array operations which are typically &quot;quick&quot; (compiled C code). This is fine when it works, but it is not always possible to vectorize the code or, in some cases, the vectorization leads to code that is very hard to read/understand. . Both Numba and Cython (not to be confused with CPython) aim to provide tools to deal with such situations. . Outline of Cython . Cython is a programming language that is part Python and part C/C++, it can be compiled into a python extension and/or an executable. If you are familiar with Python it is reasonably easy to understand Cython code, it largely just has a few &quot;boiler-plate&quot; code blocks along with a few static type declarations (familiar to those who know C/C++/related languages). . Since there are no dynamic types (in well written Cython code) and it is compiled typically the resulting code is orders of magnitude faster than Python. Compared to vectorised NumPy there may not be a significant improvement but this depends on the exact implementation. . Cython itself is very flexible, if you can express the code in Python it is unlikely you will not be able to express it in Cython. Any arbitrary class structure can work within Cython, as a result it is used for many &quot;high performance&quot; Python packages (e.g. SciPy). . It is possible to parallelize the code or utilise GPU computation using Cython. This normally requires a bit of work but typically does not require nearly as much work as using Cuda in C++ (for example). As usual the normal caveats relating to multi-thread applications also apply to Cython code. . You can read the Cython documentation here! . Outline of Numba . Numba is a slightly different beast. It uses the concept of a &quot;just in time&quot; compiler (JIT). Essentially this means that code is compiled &quot;on the fly&quot; during runtime instead of requiring compilation prior to execution. Numba compiles the python code using a LLVM compiler. . The syntax is very simple and most of the time just requires a simple decorator on a Python function. It also allows for parallelisation and GPU computation very simply (typically just a &quot;target = &#39;cuda&#39;&quot; type statement in a decorator). In my experience a lot less thinking is required to set this up compared to Cython. . The downside of Numba (at least for me) is that it is a (comparatively) new package and as such does not have support for absolutely everything you would want, unlike with Cython it is possible to just &quot;hit the wall&quot; where you simply cannot use Numba without a major re-writing of the code. (One such example being you cannot call @guvectorize functions inside the @njit decorator). It is worth checking the github issues log regularly as often these issues are on the docket to be corrected in future releases. . Another downside of Numba is the lack of useful traceback, typically you need to &quot;switch off&quot; Numba and run in regular python to track down an error. This is typically only a minor inconvenience but if the code is particularly slow it can get frustrating trying to find an error without the Numba speed up. . You can read more about Numba here! . Which is better? . From a raw performance perspective I do not see either Cython nor Numba consistently beating the other in all situations. Typically the performance will be comparable and you will rarely find one being many orders of magnitude quicker (assuming you&#39;re using both correctly). . The choice of which to use, in my opinion, comes down to other factors. Convenience being a big one, I typically find Numba easier and quicker to implement when it works. As noted above however it doesn&#39;t always work (e.g. if using class structures, custom data types, etc.) With familiarity you do get an instinct as to whether a code will work or not. Cython on the other hand offers much more flexibility. . There is also the issue of how the code will be used. Cython is well established for creating efficient extension modules that sit nicely within the Python eco-system. Numba can be used in a similar way but I have found it a bit more finnicky to deal with (for example through Numba itself changing its API fairly regularly since it&#39;s a relatively new module, some code from previous iterations of Numba simply does not work at all with the later versions). . What is the catch? . Unfortunately things are not perfect, typically we will still be interfacing Cython/Numba functions via Python and so using repeated calls to these functions we will still incur overheads (typically through the conversion to Python types). This can mean that certain code is still significantly slower than C/C++ equivalents. These packages are therefore most useful for when you have profiled your code and can see that a handful of functions/operations are the real bottleneck. . These packages may not help if your code is particularly memory intensive, in which case it is better to spend time thinking about memory management instead. In some cases these packages provide some help in that respect also (e.g. NumPy is prone to creating many cached variables for simple operations, if the variables are large arrays this can become a pain.) . What about PyPy/etc? . Another option for performant Python code is to use PyPy instead of CPython. I have not used this very much yet, if I get the time to really kick the tyres I may write another blog on my findings. There are some features that appear useful but the eco-system is not as well supported (yet?) and so may require some additional work to recreate some high level functionality. . You can read more about PyPy here! . Why not just C/C++? . Ultimately if you require peak performance at all costs these options are still no substitute for well written C/C++. However as I often warn people: computation time is generally cheaper than human time - it is often better to use a slightly sub-optimal (but still respectable) code than devote months to R&amp;D and slow down the development cycle. Since Numba/Cython are so similar to Python (and it is possible to just &quot;tack on&quot; some Python to the end of these codes) you can prototype much more quickly in my experience. All these factors (along with many others such as where the code is to be deployed, what other tools are being used, etc.) need to be weighed up. . What about Julia? . Some readers may be familiar with the Julia language as an option for high performance scientific computing. I have a little experience (but am far from an expert). As I understand Julia is based around JIT (as with Numba), however being a language to itself it never needs to interface with Python and its limitations. It can therefore create more efficient code for larger scale projects since they never have to worry about Python overheads. Typically the benchmarks seen online are for smaller &quot;toy&quot; problems and so the performance does not appear to be too different from Cython/Numba. . I have not switched to Julia for a few reasons, firstly the popularity of Python - it is typically fairly easy to learn Cython/Numba for somebody who understands Python/NumPy making collaboration easier. Secondly the Python eco-system is well developed there is typically a package available to do almost anything you would want. Thirdly at this point it is fairly easy to get Python to &quot;speak&quot; with other systems if you need to turn something from a prototype to production. These concerns are ultimately just related to uptake however, as more people use Julia I see this becoming less of a concern. . You can read more about Julia here! . Conclusion . Hopefully now we can see that Cython/Numba provide useful tools for bridging the gap between Python and C/C++ runtimes. As the old saying goes &quot;you cannot have your cake and eat it too&quot; and so it may not be possible to get performance as quick using these options. However we can often get performance that is &quot;good enough&quot; in practical terms. .",
            "url": "https://www.lewiscoleblog.com/2020/04/04/2020-02-04-Cython-Numba.html",
            "relUrl": "/2020/04/04/2020-02-04-Cython-Numba.html",
            "date": " • Apr 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Cython Vs Numba: An Example",
            "content": "Following on from the previous blog post comparing the relative merits of Cython Vs Numba I thought I&#39;d illustrate this with implementations of a relatively simple model: a vanilla 2d Ising Model. This is a prime target for a performance boost since it is a very &quot;loopy&quot; code. I will not cover what the Ising Model is/how it works (you can check that here!). It is a very interesting model and I may return to it at a later date to look at some of its properties and explore it more deeply (and maybe spin glass models more generally). For now it will just be a test subject to explore performant python type code. . In this blog I will present a few different versions, I won&#39;t throw the kitchen sink at them to get the absolute best performance but will adopt an 80/20 principle. I will not try any parallelisation or clever memory management outside of what comes pre-canned. . Python . This is a basic python implementation using a lot of looping. Even for a relatively small models this code will likely be fairl slow due to the looping. . import numpy as np # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours def neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res def dE(i, j, spin_array): return 2*spin_array[i, j]*neighbour_sum(i, j, spin_array) def update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array def _main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = update(spin_tmp) return spin_tmp . Numba . For this code we will just take the above code and use the njit decorator (forcing the use of LLVM - a jit decorator will fall back to Python object mode if it cannot work out how to use LLVM). This is literally a few seconds of coding updates. . import numpy as np from numba import njit # Grid size: N (int) NxN square latice N = 1000 # Fix Temp kT = 2 / np.log(1 + np.sqrt(2)) # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @njit def nb_neighbour_sum(i, j, spin_array): north, south, east, west = 0, 0, 0, 0 max_height = spin_array.shape[0] max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] res = north + south + east + west return res @njit def nb_dE(i, j, spin_array): return 2*spin_array[i, j]*nb_neighbour_sum(i, j, spin_array) @njit def nb_update(spin_array): height = spin_array.shape[0] width = spin_array.shape[1] for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = nb_dE(i, j, spin_array) if dEtmp &lt;= 0 or np.exp(-dEtmp / kT) &gt; np.random.random(): spin_array[i, j] *= -1 return spin_array @njit def nb_main_code(M, spin_array): spin_tmp = spin_array for x in range(M): spin_tmp = nb_update(spin_tmp) return spin_tmp . Cython . Again we will modify the python code. We will see that while not too onerous it does require a little more work than the Numba example. The code is largely boilerplate but requires a little more thinking than the Numba example (e.g. in implementing this I initially forgot a static type definition of 1 variable which was causing a 300% increase in runtime). . %load_ext Cython . %%cython cimport cython import numpy as np cimport numpy as cnp from libc.math cimport exp from libc.stdlib cimport rand cdef extern from &quot;limits.h&quot;: int RAND_MAX # Grid size: N (int) NxN square latice cdef int N = 1000 # Fix Temp cdef float kT = 2 / np.log(1 + np.sqrt(2)) cdef float kTinv = 1 / kT # Fix seed np.random.seed(123) # Random Initialize spins = 2*np.random.randint(2, size=(N, N))-1 # Get sum of neighbours @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_neighbour_sum(int i, int j, cnp.int32_t[:, :] spin_array): cdef int north = 0 cdef int south = 0 cdef int east = 0 cdef int west = 0 cdef int max_height = spin_array.shape[0] cdef int max_width = spin_array.shape[1] if i &gt; 0: north = spin_array[i-1, j] if i &lt; max_height-1: south = spin_array[i+1, j] if j &gt; 0: west = spin_array[i, j-1] if j &lt; max_width-1: east = spin_array[i, j+1] cdef int res = north + south + east + west return res @cython.boundscheck(False) @cython.wraparound(False) cdef int cy_dE(int i, int j, cnp.int32_t[:, :] spin_array): return 2*spin_array[i, j]*cy_neighbour_sum(i, j, spin_array) @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_update(cnp.int32_t[:, :] spin_array): cdef int height = spin_array.shape[0] cdef int width = spin_array.shape[1] cdef int y_offset, x_offset cdef int i, j cdef int dEtmp for y_offset in range(2): for x_offset in range(2): for i in range(y_offset, height, 2): for j in range(x_offset, width, 2): dEtmp = cy_dE(i, j, spin_array) if dEtmp &lt;= 0: spin_array[i, j] *= -1 elif exp(-dEtmp * kTinv) * RAND_MAX &gt; rand(): spin_array[i, j] *= -1 return spin_array @cython.boundscheck(False) @cython.wraparound(False) cdef cnp.int32_t[:, :] cy_main_code(int M, cnp.int32_t[:, :] spin_array): cdef int x for x in range(M): spin_array = cy_update(spin_array) return spin_array @cython.boundscheck(False) @cython.wraparound(False) cpdef cnp.int32_t[:, :] cpy_main_code(int M, cnp.int32_t[:, :] spin_array): return cy_main_code(M, spin_array) . Timing Results . # Python %timeit _main_code(10, spins) # Numba %timeit nb_main_code(10, spins) # Cython %timeit cpy_main_code(10, spins) . 52 s ± 2.53 s per loop (mean ± std. dev. of 7 runs, 1 loop each) 244 ms ± 4.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 395 ms ± 6.92 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . For our purposes the %timeout magic will be good enough a proxy for performance. We can see that the intial python implementation is very slow. For a 1000x1000 lattice and looping over 10 times, for my machine the python interation takes 50-55s. This would be fairly problematic if we wanted to sweep over the parameter space, find critical temperatures, perform random seed analyses etc. . In contrast Numba only takes 240-250ms, an impressive 2000% speed up. Running the code multiple times now seems much less onerous. . Cython is not quite as quick as the Numba implementation taking 390-400ms but still represents a significant speedup compared to Python. For practical applications the difference between Numba and Cython in this case may be insignificant. . It is worth noting that these times are from my machine on at a specific time. The times achieved on your machine might be slightly different. Similarly changing the size of the lattice may change the ordering of which option is &quot;quickest&quot; - as always it&#39;s worth checking the code how it will be used rather than performing a benchmark like this for determining which option to use. . Conclusion . From the results above it may be tempting to claim Numba is the obvious choice given it is not only easier to implement than cython but also offers faster speeds. However I selected the 2d Ising model as an example since I knew the code would work well in Numba (in a sense I have been p-value hacking the experiment!) In certain situations (e.g. a code relying very heavily on class structures) Numba is either unusable or requires a complete code overhaul whereas cython can require only a few lines of boilerplate code. . In other examples you can also see that Cython can severely outperform Numba, I am not sure why this is and the only real way to determine which will perform better is to perform testing (if somebody has an explanation/heuristic I&#39;d love to hear it). It is also possible to interface numba and cython which has been useful to me in the past. For a quick example suppose we want to perform an inverse transform of a Beta(2,0.5) distribution: . from scipy.stats import beta x = beta.ppf(0.1, 2, 0.5) x . 0.4681225665264196 . This cannot be optimised in Numba as it is (beta.ppf is currently not supported functionality - this may change by the time you read this). However we can take the address of the cython special function that this calls. We can then build the function in such a way as it can be seen by Numba: . import ctypes from numba import types, njit from numba.extending import get_cython_function_address betaaddr = get_cython_function_address(&quot;scipy.special.cython_special&quot;, &quot;btdtri&quot;) functype3d = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_double) beta_fn = functype3d(betaaddr) @njit def nb_beta_ppf(p, a, b): return beta_fn(a, b, p) x = nb_beta_ppf(0.1, 2.0, 0.5) x . 0.4681225665264196 . As we can see this is a little ugly but it works. . As a result of everything covered in these blog posts I do not believe that one option offers a significant advantage over the other and both offer valid tools for improving runtime. The choise of which to use in a given situation will depend on many factors and requires careful thought as to what is needed in a given setting. .",
            "url": "https://www.lewiscoleblog.com/performant-python/computation/cython/numba/ising-model/2020/02/11/Cython-Numba-2.html",
            "relUrl": "/performant-python/computation/cython/numba/ising-model/2020/02/11/Cython-Numba-2.html",
            "date": " • Feb 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Jackknife Methods",
            "content": "In this blog post we are concerned with a specific problem: we have a Monte-Carlo type model that produces some simulated output. With this we want to estimate an arbitrary statistic (for example percentiles, expected shortfall or more complicated statistics relating to many variables). We know however that calculated in this way the calculated statistic is just one realisation of a distribution of outcomes. We would like to be able to say something about this distribution, in particular we would like to have some idea of the variability in the statistic. . The &quot;obvious&quot; (and most accurate) way to do this would be to re-run the model many times with different random seeds and create an empirical distribution of the statistic. However if the model is particularly complex this might mean a lot of compute time. Instead we would like to find an approximate method that does not require us to re-run the model at all. To do this in a general way we will introduce the Jackknife method. . It is worth noting that in certain situations other methods can be easier to implement/more accurate, for example if we only wanted to estimate the 90th percentile we can construct an argument using a binomial distribution (and normal approximation thereof) - however this method will not generalise to (for example) expected shortfall or even more complicated statistics. . Justification of the Method . We begin by supposing we have $N$ un-ordered observations $ { X_i }_{i=1}^{N}$ from our Monte-Carlo model. We use these observations to create an estimate of a statistic $ hat{Q}$. We denote the estimate from the model $ hat{Q}_{ {1:N }}$, which itself is a random variable. Through a Taylor expansion we can note: $$ mathbb{E} left( hat{Q}_{ {1:N }} right) = hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} + ...$$ For some constants $a_x$. . If we now consider partitioning the $N$ observations as: $N = mk$ for integers $m$ and $k$ - that is we create $m$ collections of $k$ obervations ($k gg m$). We denote a set: $A_i = { X_j | quad j &lt; (i-1) k , quad j geq i k }$ to contain all observations bar $k$, each set removes a different set of $k$ observations. Each has $|A_i| = (m-1)k$. We can then write: $$ mathbb{E} left( hat{Q}_{A_i} right) approx hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} $$ Via a second order approximation. . If we then define a new variable: $ hat{q}_i = m hat{Q}_N - (m-1) hat{Q}_{A_i}$. Then via a second-order approximation we have: $$ mathbb{E} left( hat{q}_i right) approx m left( hat{Q} + frac{a_1}{N} + frac{a_2}{N^2} right) - (m-1) left( hat{Q} + frac{a_1}{(m-1)k} + frac{a_2}{(m-1)^2k^2} right)$$ Through some simplification this becomes: $$ mathbb{E} left( hat{q}_i right) approx hat{Q} - frac{a_2}{m(m-1)k^2} $$ Asymptotically this has bias $O(N^{-2})$. If the estimator only has bias $O(N_{-1})$ (i.e. $a_i =0$ for $i geq 2$) then the approximation is unbiased. . We can now define two new variables: $ hat{ hat{Q}}_N = frac{1}{m} sum_{i=1}^{m} hat{q}_i$ $ hat{ hat{V}}_N = frac{1}{m-1} sum_{i=1}^{m} left( hat{q}_i - hat{ hat{Q}}_N right)^2$ Then via CLT we have that $ hat{ hat{Q}}_N sim mathcal{N}( hat{Q}, hat{V})$ for some unknown variance $ hat{V}$. We can thus create an $X %$ confidence interval as: $ hat{Q} in left[ hat{ hat{Q}}_N - t sqrt{ frac{ hat{ hat{V}}_N}{m}}, hat{ hat{Q}}_N + t sqrt{ frac{ hat{ hat{V}}_N}{m}} right] $ With $t$ as the $(1-X) %$ point of a double-tail student-t distribution with $(m-1)$ degrees of freedom. . We can see from the construction of this confidence interval there has been no restriction on the type of statistic $ hat{Q}$ used, in this sense this is a generic method. . (Note this is strongly related to a bootstrap method, in fact it is a first order approximation to a bootstrap) . An Example . The explanation above is quite notation dense, it will be easier to look at an example. In this case we will take one of the simplest examples of a Monte-Carlo model: estimating the value of $ pi$. To do this we will take a unit square and a unit quarter circle inside it: . . We will simulate random points within the square and calculate the proportion $p$ of points landing within the quarter circle (red). If we simulate $N$ points we get an estimate $ pi approx frac{4p}{N}$. A simple vectorised numpy for this can be seen below: . # Estimating pi using Monte-Carlo import numpy as np def points_in_circle(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) y = np.random.random(N) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_circle &quot;&quot;&quot; return arr.sum() / arr.shape[0] * 4 SIMS = 10000 pts = points_in_circle(10000) print(&quot;Estimate of pi:&quot;, est_pi(pts)) . Estimate of pi: 3.1456 . We can use the jackknife method to now construct a confidence interval. (Obviously in this case we have the sample estimator of an average and so CLT applies, in practice we wouldn&#39;t use a jackknife here. But for this example I wanted something simple as to not distract attention from the jackknife method itself.) We know what the result &quot;should&quot; be in this example, however we shall pretend we don&#39;t have access to np.pi (or similar). . We can code up an implementation of the jackknife method as: . from scipy.stats import t def jackknife_pi(arr, pi_fn, m): &quot;&quot;&quot; This function implements the jackknife method outlined above The function takes an array (arr) and an estimate function (pi_fn) and a number of discrete buckets (m) - in this implementation m needs to divide size(arr) exactly The function returns Q-double hat, V-double hat, (m-1) &quot;&quot;&quot; Qn = pi_fn(arr) N = arr.shape[0] itr = np.arange(N) k = N / m q = np.zeros(m) for i in range(m): ID = (itr &lt; i*k) | (itr &gt;= (i+1)*k) temp = arr[ID] q[i] = m*Qn - (m-1)*pi_fn(temp) Qjk = q.sum() / m v = (q - Qjk)**2 Vjk = v.sum() / (m - 1) return Qjk, Vjk, (m-1) def conf_int(Q, V, X, dof): tpt = t.ppf(1-(1-X)/2, dof) up = Q + tpt*np.sqrt(V/(dof+1)) down = Q - tpt*np.sqrt(V/(dof+1)) print(round(X*100),&quot;% confidence interval: [&quot;,down,&quot;,&quot;,up,&quot;]&quot;) jk_pi = jackknife_pi(pts, est_pi, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . 95 % confidence interval: [ 3.103649740420917 , 3.187550259579082 ] . We can see that the 95% confidence interval range is fairly large (around 3.10 to 3.19) in this case. We will now show that by using a &quot;better&quot; method we can reduce this range. We start by reconsidering the square-quarter-circle: we notice that we can add 2 additional squares to this setup: . . Apart from looking like a Mondrian painting we can notice that all points generated in the yellow area will add &quot;1&quot; to the estimator array and all points in the black area will add &quot;0&quot;. The only areas of &quot;contention&quot; are the blue/red rectangles, if we focus only in generating points in these areas we will increase the accuracy of the estimator. By symmetry these 2 rectangles are identical, we only need to generate points within the rectangle: $ left { left(1, frac{1}{ sqrt{2}} right), left( 1,0 right), left( frac{1}{ sqrt{2}},0 right), left( frac{1}{ sqrt{2}}, frac{1}{ sqrt{2}} right) right }$. This has the area: $ frac{ sqrt{2}-1}{2}$ or both rectangles together having total area: $ sqrt{2}-1$. Therefore generating $N$ points within these rectangles is equivalent to generating: $ frac{N}{ sqrt{2}-1}$ points in the original scheme (approximately 2.5 times as many). This should reduce the standard deviation of the estimate by about a third (by CLT). We can code this estimator up in a similar way to before: . def points_in_rect(N): &quot;&quot;&quot; Returns an array that contains 1 if a random point (x,y) is within the unit circle of 0 otherwise This uses the &quot;imporved&quot; method N: Number of simulations (int) Random seed fixed for repeatability &quot;&quot;&quot; np.random.seed(123) x = np.random.random(N) * (1 - 1 / np.sqrt(2)) + (1/np.sqrt(2)) y = np.random.random(N) / np.sqrt(2) r = np.sqrt(x**2 + y**2) p = r &lt; 1 return p*1 def est_pi_2(arr): &quot;&quot;&quot; Return an estimate of pi using the output of points_in_rect This applies a correction since points are only simulated in smaller rectangles &quot;&quot;&quot; pct = arr.sum() / arr.shape[0] approx_pi = (pct*(np.sqrt(2)-1) + 0.5)*4 return approx_pi SIMS = 10000 pts = points_in_rect(10000) print(&quot;Estimate of pi:&quot;, est_pi_2(pts)) jk_pi = jackknife_pi(pts, est_pi_2, 10) Qtest = jk_pi[0] Vtest = jk_pi[1] pct = 0.95 dof = jk_pi[2] conf_int(Qtest, Vtest, 0.95, dof) . Estimate of pi: 3.144554915549336 95 % confidence interval: [ 3.1247314678035196 , 3.164378363295143 ] . As expected we can see the confidence interval has decreased significantly through an improved estimator. . Notes on Implementation . As we have seen the jackknife is a general tool. In certain situations other tools exist. There are a couple of points we have to keep in mind when implementing these methods: . The selection of m: this is fairly arbitrary, if the statistic being analysed is very cumbersome to calculate then a smaller choice of m is helpful (or if we wish to run this method over very many statistics). | Properties of the statistic: in some instances we know the statistic must be bounded (e.g. a correlation coefficient must be between $[-1,1]$) This additional information can and should be used to improve the confidence interval. It often becomes more art than science when deciding how to present the results of this method. | . Conclusion . In this post we have seen what a jackknife method is, why it works and a basic implementation. Hopefully now it is obvious the power these methods hold for reporting on the results of a Monte-Carlo simulation. In more sophisticated situations it can really give insight into which parts of a model suffer most from simulation error and also how confident we should be with an estimate it produces. .",
            "url": "https://www.lewiscoleblog.com/computational-statistics/computation/confidence-interval/2020/01/28/Jackknife.html",
            "relUrl": "/computational-statistics/computation/confidence-interval/2020/01/28/Jackknife.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Neuron Models 3: Ensembles",
            "content": "Justifying Gaussian White Noise . We first begin with a small diversion, in the previous HH and EIF neuron firing examples we have assumed some sort of Gaussian white noise as an input signal. We briefly mentioned that this is a reasonable assumption but we will justify this in a bit more detail here. First we note that each neuron will typically take input signal from the order of 10,000 neurons. As such even in a low firing rate scheme a neuron will likely receive relatively large amount of input spikes. We can express this signal as: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k)$$ Where: $N$ is the number of connected neurons $J_{i}$ is the synaptic connection strength from neuron $i$ $t_i^k$ is the time of the kth spike recieved from neuron i . If we assume that these spikes arrive in an uncorrelated, memoryless fashion in the form of a Poisson process and that the connection strengths are suitably small: $ langle J_i rangle ll V_{Th} - V_{Re}$ (where angle brackets denote population average). Then we can apply a diffusion approximation: $$I_p(t) = sum^{N}_{i=0} J_{i} sum_k delta(t-t_i^k) approx mu + sigma xi(t)$$ Where: $ mu = langle J_i rangle N nu $ $ sigma = langle J_i^2 rangle N nu $ $ nu$ is the mean firing rate over all connected neurons $ xi(t)$ is a Gaussian white noise process . Of course as with all approximations this is subject to &quot;small sample size&quot; and $N$ needs to be suitably large. . Fokker-Planck . Recall that we specified the EIF model with Gaussian white noise as having dynamics: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . This is nothing more than an Ito process of the form: $$dX_t = mu(X_t,t)dt + sigma(X_t, t)dW_t $$ With standard Wiener process $W_t$. The Fokker-Planck equation gives us a probability distribution of this process $p(x,t)$ through the PDE: $$ frac{ partial}{ partial t} p(x,t) = - frac{ partial}{ partial x} left[ mu(x,t)p(x,t) right] + frac{ partial^2}{ partial x^2} left[ frac{1}{2} sigma^2(x,t)p(x,t) right] $$ This formula can also be extended to higher dimensions in an obvious way. The derivation of this formula is fairly involved so not included in this blog post, most good textbooks on stochastic analysis should have a derivation for the interested reader. . In the case of the EIF model we can thus write down: $$ frac{ partial p}{ partial t} = frac{ sigma^2}{ tau} frac{ partial^2p}{ partial V_m^2} + frac{ partial}{ partial V_m} left[ frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) right] $$ With $ psi(V_m)$ represnting the exponential firing term. . By the continuity equation we can write: $$ frac{ partial p}{ partial t} = - frac{ partial J}{ partial V_m} $$ . Where $J$ represents the flux. By using this relation in the Fokker-Planck equation and integrating over voltage we get: $$ J(V_m, t) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . We can also note that: $$J(V_{Re}^+,t) = J(V_{Re}^-, t) + r(t)$$ . Where $V_{Re}^ pm$ represents the limit from above (+) or below (-) the reset voltage. the function $r(t)$ represents the average neuron firing rate. This is due to the implementaion of the voltage reset mechanism post spike. We can also note that for $V_m &lt; V_{Re}$ we have $J(V_m, t) = 0$ and for $V_m &gt; V_{Re}$ we have $J(V_m, t) = - r(t)$. We can then solve the flux equation to give: $$P(V_m, t) = frac{r(t) tau}{ sigma^2} int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du $$ . Since the probability measure needs to integrate to 1, we can then write: $$r(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m right)^{-1} $$ . (Note under the scheme presented there is no time dependence to any of these equations. Under time dependent signals we would have to be more careful and typically further approximations are made.) . So far we have not allowed for the refractory period, we have assumed that after reset the voltage trajectories continue as normal. Given we have chosen a deterministic refractory period we can just add this to the euqation above: $$r_{ref}(t) = left( frac{ tau}{ sigma^2} int_{- infty}^{V_{Th}} left( int_{max(V_m,V_{Re})}^{V_{Th}} exp left( - sigma^2 int_{V_m}^u (x - V_L - psi(x) )dx right)du right) dV_m + T_{Ref} right)^{-1} $$ . We can see that this integral will not give rise to an analytic solution in the case of EIF neurons. The forward Euler scheme we relied upon in the past will not perform well here. Instead we will use a slightly different numerical scheme. . Numerical Integration . (This is taken from Richardson [2007] - see references for further details) Presented now is a numerical scheme for calculating the firing rate. Recall from above: $$ J(V_m, t) = - r(t) Theta(V - V_{Re}) = - frac{ sigma^2}{ tau} frac{ partial p}{ partial V_m} - frac{(V_m - V_L - psi(V_m) )}{ tau} p(V_m,t) $$ . Where $ Theta(V)$ is the Heaviside step-function. Re-arranged this gives: $$- frac{ partial p}{ partial V_m} = - frac { tau}{ sigma^2} r(t) Theta(V - V_{Re}) + sigma^{-2}(V_m - V_L - psi(V_m)) p(V_m,t) $$ . Which is of the form: $$ frac{ partial p}{ partial V_m} = G(V_m)p(V_m) + H(V_m) $$ . By applying a voltage discretization scheme: $V_k = V_{Lb} + k Delta_V $ with $V_n = V_{Th}$ we can write down: $$ p(V_{k-1}) = p(V_k) e^{ int^{V_k}_{V_{k-1}} G(V)dV} + int^{V_k}_{V_{k-1}} H(V) e^{ int^V_{V_{k-1}}G(U)dU} $$ . We can approximate this as: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V G(V_k)} + Delta_V H(V_k) left( frac{e^{ Delta_V G(V_k)} - 1}{ Delta_V G(V_k)} right) $$ . Substituting back in the necessary formulae for $G$ and $H$ gives: $$ p(V_{k-1}) = p(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2}r(t) Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right) $$ . However this still has unknown $r(t)$ in it. If we apply a transform: $q(V,t) = frac{p(V,t)}{r(t)}$ then: $ sum q(V_k) = (r(t))^{-1}$ and: $$ q(V_{k-1}) = q(V_k) e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k)) } + Delta_V frac{ tau}{ sigma^2} Theta(V_k - V_{Re}) left( frac{e^{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} - 1}{ Delta_V sigma^{-2}(V_k - V_L - psi(V_k))} right)$$ . To simplify this expression we define functions $A$ and $B$ so that: $$ q(V_{k-1}) = q(V_k) A(V_k) + Theta(V_k - V_{Re}) B(V_k) $$ . And so we can calculate the firing rate. This scheme has a much better performance than an Euler scheme. We instantiate the scheme with $q(V_n) = 0$, we also select a value $V_{Lb}$ as a cut-off to stop iterating. An implementation of this method can be seen below: . # Evaluating the solution to the Fokker-Planck Equation to calculate the firing rate of an EIF neuron subject to Gaussian white noise import numpy as np # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up additional parameters for solving Fokker-Planck. DelV (mV) and VLb (mV) DelV = 0.001 VLb = -100 Steps = int(np.ceil((Vth - VLb) / DelV)) q = np.zeros(Steps) V = np.arange(Steps)*DelV + VLb # For ease define function psi def psi(V): return DelT * np.exp((V - VT) / DelT) def A(V): return np.exp(DelV * sig**-2 *(V - VL - psi(V))) def B(V): if A(V) == 1.0: return DelV * tau * sig**-2 else: return DelV * tau * sig**-2 * (A(V) - 1) / np.log(A(V)) # Shut off numpy divide errors np.seterr(divide=&#39;ignore&#39;) for i in range(Steps -1, 0, -1): if V[i] &gt; Vr: q[i-1] = q[i]*A(V[i]) + B(V[i]) else: q[i-1] = q[i]*A(V[i]) r = 1/(q.sum()/1000000 + Tref/1000) print(&quot;Firing rate:&quot;, round(r,1), &quot;Hz&quot;) . Firing rate: 21.6 Hz . We can modify the previous EIF firing code to estimate the firing rate, the results should be similar (note: for this I used 10m time steps, it is a slow running code!): . #collapse # Implementation of a noisy EIF neuron using a forward Euler scheme # Reduce N for quicker running code import numpy as np # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 10000000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up voltage Vold (mV) and spike count Sp Vold = Vr Sp = 0 # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: Vnew = Vr Tc -= 1 else: Vtemp = Vold + dt/tau*(VL - Vold) + DelT*dt/tau*np.exp((Vold - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: Vnew = Vr Tc = np.ceil(Tref/dt) Sp += 1 else: Vnew = Vtemp Vold = Vnew print(&quot;Estimated firing rate:&quot;, round(Sp/(N*dt/1000),1), &quot;Hz&quot;) . . Estimated firing rate: 20.2 Hz . Which we can see is similar to the solution of the Fokker-Planck equation. In the limit $N to infty$ and decreasing the lattice sizes these approximations should become much closer. . Conclusion . We have seen that by using the Fokker-Planck framework we are able to calculate the mean firing rate of the EIF neuron. We can also notice that the numerical scheme to integrate the Fokker-Planck runs significantly faster than taking a Monte-Carlo approximation by simulating the EIF directly. We can also notice that the Fokker-Planck framework is easy to extend (e.g. to modulated noise or other applied signals) and further we can extend this to allow for connected networks of neurons (I may write an additional blog post on this in the future but will likely end up being quite similar to this one). . References . https://neuronaldynamics.epfl.ch/online/Ch13.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | How Spike Generation Mechanisms Determine the Neuronal Response to Fluctuating Inputs - Nicolas Fourcaud-Trocme´, David Hansel, Carl van Vreeswijk, and Nicolas Brunel [2003] | Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based synaptic drive - Magnus J Richardson [2007] | .",
            "url": "https://www.lewiscoleblog.com/biology/computational-statistics/non-linear-dynamics/neuron/2020/01/21/Neuron-Models-3.html",
            "relUrl": "/biology/computational-statistics/non-linear-dynamics/neuron/2020/01/21/Neuron-Models-3.html",
            "date": " • Jan 21, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Neuron Models 2: Exponential Integrate and Fire Model",
            "content": "Integrate and Fire Models . Throughout this blog post we will focus on integrate and fire models. This class of model has been around for a long time, in fact longer than the Hodgkin-Huxley model. The first model was presented by Lapicque in 1907. Since then many alternative formulations have been presented. We can express the models in the form: $$ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $$ . Where: $ tau$ represents membrane time constant ($C_m / g_L$ in notation used previously) $E$ represents the rest potential $ psi(V)$ is the spike generating current term $R_m$ represents membrane resistance ($1/g_L$) $I(t)$ is a function representing an applied current (in the Hodgkin-Huxley example we took a Gaussian white noise) . With integrate and fire models we have the issue that (typically) the action potential will shoot off to infinity. In order to stop this we implement a threshold ($V_{Th}$), when the process reaches this value it is reset (to $V_{Re}$) and the dynamics start again. This is not a big issue because for our purposes we are noly interested in the dynamics of an onset of an action potential, the mechanism of returning to normal levels is not of (much) interest. When modelling we may wish to hold the voltage at $V_{Re}$ following a spike for a short time ($ Delta_{T_{Rf}}$) to reflect the refractory period of a neuron. The refractory period can be modelled stochastically but usually a static value is succficient. . Integrate and fire models are thus defined by the form of the function $ psi(V)$ - this function may be linear or non-linear. Examples include the leaky-integrate and fire model (linear), Fitzhugh-Nagumo (polynomial) and the exponential integrate and fire (non-linear). . From Hodgkin-Huxley to Integrate and Fire . We now take a quick de-tour to justify the use of the integrate and fire model as an approximation to the Hodgkin-Huxley dynamics. . First we notice that gate m operates on a much faster time scale than gates n or h (and similarly much faster than the leak channel which controls the potential dynamics with all gates closed.) Given it is so much faster we can apply an instantaneous approximation, namely: $m(t) = hat{m}(V_m(t))$ that is: the dynamics are defined by the membrane voltage. From plotting gate dynamics we can also observe that gates n and h are approximately translated reflections of each other. As an approximation we can create an adaptation variable $w$ with $n = aw$ and $h = b - w$ for constants $a, b$. We can then write down the equation: $ C_m frac{dV_m}{dt} = I_p - overline{g_K}(aw)^4(V_m-V_K) - overline{g_{Na}}( hat{m}(V_m))^3(b - w)(V_m-V_{Na}) - overline{g_L}(V_m-V_L) $ . There is a corresponding equation for the adaptation variable $w$ which we shall not concern ourselves with. We are only interested in the onset of spiking not the refractory period dynamics so we will take $w = w_{rest}$ to be a fixed value. We can therefore express the voltage dynamics as: $ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) - lambda ( hat{m}(V_m))^3(V_m-V_{Na}) $ . By collecting terms and some re-arrangement. $g_{eff}, V_{eff}$ and $ lambda$ are all constant values. Therefore via the approximations outlined above we are left with an equation of the form: $ tau frac{dV}{dt} = -(V - E) + psi(V) + R_m I(t) $ . Namely an integrate and fire model. . Exponential Integrate and Fire . Continuing with the line of reasoning above we shall consider the function: $ hat{m}(V_m)$. We assume the dynamics are so rapid that they are essentially in equilibrium: $ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m = 0 $ So: $ hat{m}(V_m) = frac{ alpha_m(V_m)}{ alpha_m(V_m) + beta_m(V_m)}$ . Since Hodgkin-Huxley suggested the following forms of these equations: $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ . We can approximate $ hat{m}(V_m)$ with a logistic function: $ hat{m}(V_m) approx (1 + e^{- beta(V_m - theta)})^{-1}$ We can then express the Taylor expansion of this as: $ hat{m}(V_m) = sum (-1)^k e^{ beta(V_m - theta)(1+k)}$ Which we can see from the expansion of $1/(1+y)$ with $y = e^{- beta(V_m - theta)}$. So a first order approximation is: $ hat{m}(V_m) approx e^{ beta(V_m - theta)}$ . Then the current corresponding to the sodium channel can be expressed approximately: $I_{Na} = g_{Na}(b - w_{rest})(V_m - V_{Na}) e^{3 beta(V_m - theta)} $ . If we take $(V_m - V_{Na}) approx (V_{rest} - V_{Na}) &lt; 0$ as an approximation we get approximate voltage dynamics as: $ C_m frac{dV_m}{dt} = I_p - overline{g_{eff}}(V_m-V_{eff}) + hat{ lambda} e^{ beta (V_m - theta)} $ . This is known as the exponential integrate and fire (EIF) model.This model has been shown to fit experimental data (and the Hodgkin-Huxley model) very well in practice. Typically we use a parameterization of the spiking term: $$ psi(V) = Delta_T e^{ left( frac{V - V_T}{ Delta_T} right)} $$ We can see this model is highly non-linear and without applying the threshold mechanics the membrane potential would shoot to infinity. The 2 new parameters in this model are: $V_T$ which represents the voltage scale at which the exponential term becomes significant in the dynamics $ Delta_T$ representing the sharpness of the spike . With a Gaussian white noise term $ xi_t$ we can fully specify the dynamics using: $$ tau frac{dV_m}{dt} = (V_L - V_m) + Delta_T e^{ left( frac{V_m - V_T}{ Delta_T} right)} + sigma sqrt{2 tau} xi_t $$ . As before we can solve this using a foward Euler scheme (although the forcing term is non-linear this still provides a reasonable solution for small enough time steps.) An implementation of this can be seen below: . # Implementation of a noisy EIF neuron using a forward Euler scheme import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed for repeatability np.random.seed(123) # Set time step dt (ms) and number of steps N dt = 0.001 N = 50000 # Set model parameters # Membrane time constant tau (ms) and leak reversal potential VL (mV) tau = 30 VL = -70 # Spike sharpness DelT (mV) and exponential potential threshold VT (mV) DelT = 3 VT = -60 # Variation in gaussian noise sig sig = 25 # Set voltage spike threshold Vth (mV), reset voltage Vr (mV) and refractory period Tref (ms) Vth = 30 Vr = -70 Tref = 5 # Set up arrays for time T (ms) and voltage V (mV) T = np.arange(N) * dt V = np.zeros(N) V[0] = Vr # Set up refractory period counter Tc (ms) Tc = 0 for i in range(1, N): if Tc &gt; 0: V[i] = Vr Tc -= 1 else: Vtemp = V[i-1] + dt/tau*(VL - V[i-1]) + DelT*dt/tau*np.exp((V[i-1] - VT)/DelT) + sig*np.sqrt(2*dt/tau)*np.random.normal(0,1,1) if Vtemp &gt; Vth: V[i] = Vr Tc = np.ceil(Tref/dt) else: V[i] = Vtemp # Plot voltage trajectory plt.plot(T, V) plt.xlabel(&quot;Time (ms)&quot;) plt.ylabel(&quot;Membrane Potential (mV)&quot;) plt.title(&quot;Membrane Voltage Trajectory&quot;) plt.show() . Conclusion . The voltage trajectory displays realistic neuron dynamics for the onset of spiking. However as expected through the use of the refractory period implementation the depolarizing phase is not captured well. This is ok since we consider the information to be carried by the spike itself not the behaviour shortly afterwards. We can also see that this implementation is considerably simpler than that of Hodgkin-Huxely since there is only one ODE. This is of benefit when modelling large networks of neurons where time/computational constraints become a consideration. . It has been shown that the EIF model can predict spiking behaviour very well in practice, despite the somewhat cavalier assumptions made during its derivation from the Hodgkin-Huxley. . In a future blog post we will make use of the mathematical tractability of the EIF model to analyse neurons in more depth. . References . https://neuronaldynamics.epfl.ch/online/Ch5.S2.html - Online Neuronal Dynamics Textbook by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski | .",
            "url": "https://www.lewiscoleblog.com/biology/computational-statistics/non-linear-dynamics/neuron/2020/01/14/Neuron-Models-2.html",
            "relUrl": "/biology/computational-statistics/non-linear-dynamics/neuron/2020/01/14/Neuron-Models-2.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Neuron Firing Models: Hodgkin-Huxley Model",
            "content": "Some (very basic) Biology . We start with a very limited description of what a neuron is and the mechanism by which it fires. For sake of completeness by neuron we will refer to pyramidal neurons, they make up a large proportion of neurons within the cortex of mammals. Other types of neurons are specialised for different functions. This background will be very brief and not cover the biology in any great detail. The literature and data on this topic is vast and I will not do it justice so any interested readers should look towards biology textbooks for more detailed descriptions. . The neuron is made up of 3 main components: a soma (cell body), an axon and many dendrites. In laymans terms the axon is the &quot;output&quot; of the neuron while the dendrites are &quot;inputs&quot; to the soma. The axon is covered in a myelin sheath which acts &quot;insulation&quot; which can &quot;speed up&quot; signal flow. Dendrites can futher be broken down into the apical dendrite, basal dendrites and dendritic spines. Both dendrites and axons are highly branched and a single neuron can be linked to many 1000s of others. Signals are passed through synapses. Synaptic inputs can be either excitatory (making the target neuron more likely to fire) or inhibitory (less likely). . (Source: https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg ) . The details of axons and dendrites are not that important for our purposes right now. Instead we are interested in the soma: essentially where the signal is generated. The cell wall contains many voltage gated ion channels, most notably: sodium (Na+) and potassium (K+) channels. There are 2 forces acting on ions inside/outside of the soma: namely the electrical potential within the body and the concentration gradient. Resting membrane potential is around -40mv to -90mv, in this state there is a higher concentration of potassium ions inside the cell than outside and the reverse for sodium ions. The channel for potassium is highly permeable and so potassium ions flow into the soma, the sodium channel is semi-permeable so sodium slowly flow out of the soma, to maintain a constant negative potential the cell &quot;pumps&quot; ions in the reverse direction to maintain equilibrium. . An action potential (neuron spike) is a shift away from the negative equilibrium membrane potential to a positive potential. This is as a result of ion flows due to voltage controlled gates. There are 3 gates of interest here: . Sodium activation gate - normally closed but opens with positive potential | Sodium inactivation gate - normally open but closes with positive potential | Potassium inactivation gate - normally closed but opens with large positive potential | We can break down the events that cause the action potential then as: . Depolarisation - A depolarisation event occurs (e.g. a signal from the dendrites) which brings the cell&#39;s membrane potential to 0mv. This is as a result of positive charged ions flowing into the cell. As the potential increases to some threshold the sodium activation gate is opened which allows more positively charged sodium ions to enter. The potential then becomes positive. | Repolarisation - The positive potential causes the sodium inactivation gate to close preventing more sodium ions entering. Meanwhile the potassium inactivation gate opens, since the concentration of potassium ions inside the cell is much greater than outside this leads to an outflow of potassium. The removal of positively charged ions moves the cell back towards equilibrium. | Refractory Period - The potassium channel stays open slightly past the equilibrium point and the membrane potential becomes too negative (hyperpolarises). As the potassium channel closes the potential tends back to equilibrium. There is a short period after an action potential where the neuron is unable to fire again. | (Source: https://teachmephysiology.com/wp-content/uploads/2018/08/action-potential.png) . In the rest of this blog we will look at models of action potentials and will try and keep this real world description of action potentials in mind. . Hodgkin Huxley Model . We now consider a spiking neuron model as presented by Alan Hodgkin and Andrew Huxley in 1952, this model won them the Nobel prize for physiology in 1963. The model was developed by studying the axon of a giant squid neuron. The model itself tries to mimic the gates and ionic channels described above. Diagramatically we can represent the model as: (Source: https://vignette.wikia.nocookie.net/psychology/images/c/cf/Hodgkin-Huxley.jpg/revision/latest?cb=20061206001510) . The voltage controlled ionic gates conductances are represented in the diagram by gn (only one represented in the diagram) and there is a leak conductance represented by gl. Cm represents the membrane capacitance and there is an external stimulus Ip which represents inputs from other neurons (or a test current applied by a probe in experiment). We can then represent the model as a set of 4 interacting PDEs: $ frac{dV_m}{dt} = frac{I_p}{C_m} - frac{ overline{g_K}n^4}{C_m}(V_m-V_K) - frac{ overline{g_{Na}}m^3h}{C_m}(V_m-V_{Na}) - frac{ overline{g_L}}{C_m}(V_m-V_L) $ $ frac{dn}{dt} = alpha_n(V_m)(1-n) - beta_n(V_m)n $ $ frac{dm}{dt} = alpha_m(V_m)(1-m) - beta_m(V_m)m $ $ frac{dh}{dt} = alpha_h(V_m)(1-h) - beta_h(V_m)h $ . Where the functions $ alpha_x , beta_x$ as suggested by Hodgkin and Huxley are: $ alpha_n(V_m) = frac{0.01(10-V_m)}{e^{(1.0-0.1V_m)}-1} $ $ beta_n(V_m) = 0.125 e^{-V_m / 80} $ $ alpha_m(V_m) = frac{0.1(25-V_m)}{e^{(2.5-0.1V_m)}-1} $ $ beta_m(V_m) = 4 e^{-V_m / 18} $ $ alpha_h(V_m) = 0.07 e^{-V_m / 20} $ $ beta_h(V_m) = frac{1}{e^{(3.0-0.1V_m)}+1} $ . The following values are suggested for the model constants: $C_m = 1 mu F /cm^2$ - Capacitance per unit surface area of neuron membrane $ overline{g_{Na}} = 120 mu S / cm^2$ - Voltage controlled sodium conductance per unit surface area $ overline{g_{K}} = 36 mu S / cm^2$ - Voltage controlled potassium conductance per unit surface area $ overline{g_{L}} = 0.336 mu S / cm^2$ - Voltage controlled leak conductance per unit surface area $V_{Na} = 115mV$ - Sodium voltage gradient $V_{K} = -12mV$ - Potassium voltage gradient $V_{L} = 10.613mV$ - Leak current voltage gradient . This is a non-linear system of differential equations and as such it is not possible to study analytically. However we can simulate this numerically. For this example we will assume that the external stimulus follows a white noise (Brownian motion). Under certain conditions this can be a reasonable assumption. We will define this as follows: $I_p(t) = sigma W_t$ for some positive constant $ sigma$ with units $ mu A / cm^2$ . By introducing stochastic noise such as this we unfortunately are unable to use any of the Python pre-made ODE integrators (e.g. scipy.integrate.ode) instead we will rely on a forward Euler scheme which will perform well enough for our purposes assuming the discrete time steps are selected to be sufficiently small. An example implementation of this can be seen below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Set seed np.random.seed(123) # Set time step dt (ms) and number of steps for simulation N dt = 0.001 N = 50000 # Set up time array T T = np.arange(N) * dt # Set model inputs, using nomenclature: K - Potassium, Na - Sodium, L - Leak # Set membrane capacitance per unit area (uF/cm^2) Cm = 1.0 # Set applied current density volatility (uA/cm^2) sigma = 2 # Set channel conductance per unit area (mS/cm^2) gK = 36.0 gNa = 120.0 gL = 0.3 # Set voltage gradients (mV) VK = -12.0 VNa = 115.0 VL = 10.613 # Define ion channel rate functions def alpha_n(Vm): return (0.01 * (10.0 - Vm)) / (np.exp(1.0 - (0.1 * Vm)) - 1.0) def beta_n(Vm): return 0.125 * np.exp(-Vm / 80.0) def alpha_m(Vm): return (0.1 * (25.0 - Vm)) / (np.exp(2.5 - (0.1 * Vm)) - 1.0) def beta_m(Vm): return 4.0 * np.exp(-Vm / 18.0) def alpha_h(Vm): return 0.07 * np.exp(-Vm / 20.0) def beta_h(Vm): return 1.0 / (np.exp(3.0 - (0.1 * Vm)) + 1.0) # Define applied signal function - can be replaced to investigate different signals def Ip(sig, t, V): return np.sqrt(dt) * sig * np.random.normal(0, 1, 1) # Set up arrays for dynamic results Vm = np.zeros(N) n = np.zeros(N) m = np.zeros(N) h = np.zeros(N) Signal = np.zeros(N) # Initialize the system V0 = 0 Vm[0] = V0 n[0] = alpha_n(V0) / (alpha_n(V0) + beta_n(V0)) m[0] = alpha_m(V0) / (alpha_m(V0) + beta_m(V0)) h[0] = alpha_h(V0) / (alpha_h(V0) + beta_h(V0)) # Loop through Euler-Forward scheme for i in range(1, N): Signal[i] = Ip(sigma, i*dt, Vm[i-1])/Cm Vm[i] = Vm[i-1] + Signal[i] - gK*np.power(n[i-1],4)*(Vm[i-1]-VK)*dt/Cm - gNa*np.power(m[i-1],3)*h[i-1]*(Vm[i-1]-VNa)*dt/Cm - gL*(Vm[i-1]-VL)*dt/Cm n[i] = n[i-1] + (alpha_n(Vm[i-1])*(1 - n[i-1]) - beta_n(Vm[i-1])*n[i-1])*dt m[i] = m[i-1] + (alpha_m(Vm[i-1])*(1 - m[i-1]) - beta_m(Vm[i-1])*m[i-1])*dt h[i] = h[i-1] + (alpha_h(Vm[i-1])*(1 - h[i-1]) - beta_h(Vm[i-1])*h[i-1])*dt # Plot sample path plt.plot(T, Vm) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Membrane voltage mV&quot;) plt.title(&quot;Hodgkin-Huxley Spiking&quot;) plt.show() # Plot gate opening over time plt.plot(T, n, label=&quot;n - K&quot;) plt.plot(T, m, label=&quot;m - Na&quot;) plt.plot(T, h, label=&quot;h - Na&quot;) plt.legend(loc=&quot;upper right&quot;) plt.xlabel(&quot;Time ms&quot;) plt.ylabel(&quot;Gate Proportion&quot;) plt.title(&quot;Gate Dynamics&quot;) plt.show() # Plot Limit trajectories plt.plot(n, Vm, label=&quot;n-Vm&quot;) plt.plot(m, Vm, label=&quot;m-Vm&quot;) plt.plot(h,Vm, label=&quot;h-Vm&quot;) plt.legend() plt.title(&quot;Cycle Trajectories&quot;) plt.xlabel(&quot;Gate Proportion&quot;) plt.ylabel(&quot;Membrane Voltage mV&quot;) plt.show() .",
            "url": "https://www.lewiscoleblog.com/biology/computational-statistics/non-linear-dynamics/neuron/2020/01/07/Neuron-Models-1.html",
            "relUrl": "/biology/computational-statistics/non-linear-dynamics/neuron/2020/01/07/Neuron-Models-1.html",
            "date": " • Jan 7, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Insurance Aggregation Model",
            "content": "Note: Thoughts epressed within this workbook are my own and do not represent any prior, current nor future employers or affiliates . Background - An Overview of Modelling in Specialty Insurance . Within the specialty insurance space we are typically insuring economic interests of relatively rare events of high impact (for example: buildings damaged by hurricanes, aircraft crashes, impact of CEO wrong-doing, and so on.) These events are typically broken up into 2 broad classes: . Property | Casualty | Hence why the term &quot;P&amp;C&quot; insurer is sometimes used. Property risks are, as the name suggests, related to property - historically phyiscal property but now can include non-physical property (e.g. data). Owing to the relative simplicity of these risks there is an entire universe of quantitative models that exist for risk management purposes, in particular there are a handful of vendors that create &quot;natural catastrophe&quot; (nat-cat) models. These models are sophisticated and all essentially rely on GIS style modelling: a portfolio of insured risks are placed on a geographical map (using lat-long co-ordinates) then &quot;storm tracks&quot; representing possible hurricane paths are run through the portfolio resulting in a statistical distribution of loss estimates. For other threats such as earthquakes, typhoons and wild-fires similar methods are used. . These nat-cat models allow for fairly detailed risk management procedures. For example it allows insurers to look for &quot;hot spots&quot; of exposure and can then allow for a reduction in exposure growth in these areas. They allow for counter-factual analysis: what would happen if the hurricane from last year took a slightly different track? It allows insurers to consider marginal impacts of certain portfolios, for example: what if we take on a portfolio a competitor is giving up, with our current portfolio will it aggregate or diversify? As a result of this explanatory power natural catastrophe risks are now well understood and for all intents and purposes these risks are now commodified and have allowed insurance linked securities (ILS) to form. &lt;/br&gt; . Before this analytics boom specialty insurers made their money in natural catastrophe and property insurance, as such there has been a massive growth in recent years in the Casualty side of the business. Unfortunately the state of modelling on that side is, to put it politely, not quite at the same level. . As one would expect nat-cat model vendors have tried, and continue to try, to force the casualty business into their existing natural catastrophe models. This is a recipe for disaster as the network structure for something like the economy does not naturally lend itself to a geogprahic spatial representation. There is also a big problem of available data. Physical property risks give rise to data that is easy to cultivate. Casualty data is either hard to find or impossible - why would any corporation want to divulge all the details of their interactions? As such it does not appear that these approaches will become useful tools in this space. . To fill this void there has been an increasing movement of actuaries into casualty risk modelling roles. While this overcomes some of the problems that face the nat-cat models they also introduce a whole new set of issues. Traditional actuarial models relying on statistical curve fitting to macro-level data. Even assuming a suitable distribution function can be constructed it is of limited use for risk management as it only informs them of the &quot;what&quot; but not the &quot;why&quot;, making it hard to orient a portfolio for a specific result. More recently actuaries have slowly began to model individual deals at a micro-level and aggregate them to get a portfolio view. To do this a &quot;correlation matrix&quot; is typically employed, this aproach also has issues: . Methods don&#39;t scale well with size, adding new risks often require the entire model to be recalibrated taking time and effort. | They either require a lot of parameters or unable to capture multi-factor dependency (e.g. a double trigger policy where each trigger has its own sources of accumulation). | It is usually not possible to vary the nature of dependency (e.g. add tail dependence or non-central dependency) | Results are often meaningless in the real world, it is usually impossible to perform counter-factual analysis | To bridge this gap I have developed a modelling framework that allows for the following: . Modelling occurs at an individual insured interest level | Modelling is scalable in the sense that adding new insured interests requires relatively few new parameters and calibrations | Counter-factual analysis is possible and the model can be interpreted in terms of the real world | The framework itself is highly parallelizable, whereas nat-cat models require teams of analysts, large servers and IT infrastructure this framework lends itself to being run by multiple people on regular desktop computers with little additional workflow requirements | A First Step: A Simple Driver Method . We will now look at a very stylised model of aggregation that will form a foundation on which we can build the more sophisticated model framework. We call this method of applying dependence a &quot;driver method&quot;, it is standard practice for applying dependence in banking credit risk models where there can be many thousands of risks modelled within a portfolio. The interpretation is that there is a central &quot;driver&quot;, each individual risk is &quot;driven&quot; by this and since this is common to all risks there is an induced dependence relation between them. . The model relies on the generalised inverse transform method of generating random variates. Stated very simply: if you apply the inverse CDF of a random variable to a random number (U[0,1] variate) you will have samples distributed as that random variable. Therefore in order to apply dependence in a general form we only need to apply dependence between U[0,1] variates. We will also exploit the fact that normal distributions are closed under addition (that is the sum of normals is normal). . We can now express the model as follows: . We sample standard normal (N(0,1)) variates to represent the &quot;driver&quot; variable | For each risk sample an additional set of normal variates | Take a weighted sum of the &quot;driver&quot; and the additional normal variates to give a new (dependent) normal variate | Standardise the result from step 3) and convert to a U[0,1] variable using the standard gaussian CDF | Use an inverse transform to convert the result of step 4) to a variate as specified by the risk model | We can see that this method is completely general, it does not depend on any assumption about the stand-alone risk model distributions (it is a &quot;copula&quot; method). Another observation is that the normal variates here are in some sense &quot;synthetic&quot; and simply a tool for applying the dependence. . For clarity an example is presented below: . # Simple driver method example # We model a central driver Z # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates X1 and X2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate temporary synthetic variable X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Use normal CDF to convert X synthetic variables to uniforms U U1 = norm.cdf(X1) U2 = norm.cdf(X2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.4628059800990357 . The example above shows we have correlated gamma variates with around a 50% correlation coefficient (in this case we could calculate the correlation coefficient analytically but it is not necessary for our purposes, as we create more sophisticated models the analytic solutions become more difficult/impossible). . Even from this example we can see how models of this form provide superior scalability: for each additional variable we only need to specify 1 parameter: the weight given to the central driver. In contrast a &quot;matrix&quot; method requires each pair-wise combination to be specified (and then we require a procedure to convert the matrix to positive semi-definite form in order to apply it). Say our model requires something more sophisticated: say the sum of a correlated gamma and a weibull distribution - the number of parameters in a matrix representation grows very quickly. However it is worth noting we do lose some control, by reducing the number of parameters in this way we lose the ability to express every possible correlation network. However in most cases this is not a big problem as there is insufficient data to estimate the correlation matrix anyway. . It is worth pointing out that the type of dependency applied here is a &quot;rank normal&quot; dependency - this is the same dependency structure as in a multi-variate normal distribution, albeit generalised to any marginal distribution. . An Extension to the Simple Driver Method . We can extend the model above by noticing the following: there is nothing stopping the &quot;synthetic&quot; variables being considered drivers in their own right. Gaussians being closed under addition does not require that each variable needs to be independent, sums of rank correlated normals are still normal! We can thus extend the model to: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence import numpy as np from scipy.stats import gamma, norm import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Use normal CDF to convert sX synthetic variables to uniforms U U1 = norm.cdf(sX1) U2 = norm.cdf(sX2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7851999480298125 . As before we have ended up with rank-normal correlated gamma variates. This time we have 3 potential &quot;driver&quot; variables Z, X1, X2 - all correlated with each other. It is not hard to see how this procedure can be iterated repeatedly to give arbitrarily many correlated driver variables. Further we can imagine these variables being oriented in a hierarchy, Z being at the bottom layer, X1 and X2 being a layer above, and so on. . What is a Driver? . We should now take a step back and think about the implications for the insurance aggregation problem. As stated previously this method allows us to define dependency with far fewer parameters than using a matrix approach. When you start getting into the realms of 100,000s of modelled variables this becomes increasingly important from a calibration perspective. . However there are other benefits: for example we can look at how the model variables relate to the driver variables. For example we can ask questions such as: &quot;What is the distribution of modelled variables when driver Z is above the 75th percentile&quot; and so on. This is a form of counter-factual analysis that can be performed using the model, with the matrix approaches you get no such ability. For counter-factual analysis to be useful however we require real-world interpretations of the drivers themselves. By limiting ourselves to counter-factual analysis based on driver percentiles (e.g. after the normal cdf is applied to Z, X1, X2 - leading to uniformly distributed driver variables) we make no assumption about the distribution about the driver itself, only its relationship with other drivers. . By not making a distributional assumption a driver can represent any stochastic process. This is an important but subtle point. For example we could create a driver for &quot;global economy&quot; (Z) and by taking weighted sums of these create new drivers &quot;US economy&quot; (X1) and &quot;european economy&quot; (X2). In this example there may be data driven calibrations for suitable weights to select (e.g. using GDP figures) however it is also relatively easy to use expert judgement. In my experience it is actually easier to elicit parameters in this style of model compared to &quot;correlation&quot; parameters given this natural interpretation. . Given this natural interpretation we can quite easily begin to answer questions such as: &quot;What might happen to the insurance portfolio in the case of a european economic downturn?&quot; and so on. Clearly the detail level of the driver structure controls what sort of questions can be answered. . As stated previously we can repeat the mechanics of creating drivers to create new &quot;levels&quot; of drivers (e.g. moving from &quot;european economy&quot; to &quot;French economy&quot;, &quot;UK economy&quot; and so on). We can also create multiple &quot;families&quot; of driver, for example in addition to looking at economies we may consider a family relating to &quot;political unrest&quot;, again this could be broken down into region then country and so on. Other driver families may not have a geographic interpretation - for example commodity prices. In some cases the families may be completely independent of each other, in other cases they can depend on each other (e.g. commodity prices will have some relationship with the economy). . In the examples so far we have presented a &quot;top down&quot; implementation in our examples: we start by modelling a global phenomena and then build &quot;smaller&quot; phenomena out of these. There is nothing special about this, we could have just as easily presented a &quot;bottom up&quot; implementation: take a number of &quot;Z&quot; variables to represent regions and combine these to form an &quot;X&quot; representing a global variable. Neither implementation is necessarily better than another and mathematically they lead to equivalent behaviours (through proper calibration). In practice however I have found the &quot;top down&quot; approach works better, typically you will start with a simple model and through time it can iterate and become more sophisticated. The top down approach makes it easier to create &quot;backward compatability&quot; which is a very useful feature for any modelling framework (e.g. suppose the first iteration of the framework only considers economic regions, next time a model is added which requires country splits - with top down adding new country variables keeps the economic regions identical without requiring any addtional thought.) . The need for more Sophistication: Tail Dependence . Unfortunately the model presented so far is still quite a way from being useful. We may have found a way of calibrating a joint distribution using relatively few (O(N)) parameters and can (in some sense) perform counter-factual analysis, but there is still a big issue. . So far the method only allows for rank-normal joint behaviour. From the analysis of complex systems we know that this is not necessarily a good assumption (please see other blog posts for details). We are particularly interested in &quot;tail dependence&quot;, in layman&#39;s terms: &quot;when things go bad, they go bad together&quot;. Tail dependence can arise for any number of reasons: . Structual changes in the system | Feedback | State space reduction | Multiplicative processes | Herd mentality/other human behaviours | And many others | . Given the framework we are working within we are not particularly interested in how these effects occur, we are just interested in replicating the behaviour. . To do this we will extend the framework to cover a multivariate-student-T dependence structure. To do this we note the following: $$ T_{ nu} sim frac{Z} { sqrt{ frac{ chi^2_{ nu}} { nu}}} $$ Where: $ T_{ nu} $ follows a student-t distribution with $ nu$ degrees of freedom $ Z $ follows a standard normal $N(0,1)$ $ chi^2_{ nu} $ follows Chi-Square with $ nu$ degrees of freedom . Therefore we can easily extend the model to allow for tail dependence. . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu = 3 Chi = chi2.rvs(nu, size=SIMS) sX1 /= np.sqrt(Chi / nu) sX2 /= np.sqrt(Chi / nu) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu) U2 = t.cdf(sX2, df=nu) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7907911109201866 . Adding Flexibility . We can further extend this model by allowing each model variate to have its own tail-dependence. Why is this important one might ask? In the case of this framework we are spanning many different models, selecting a single degree of tail dependence might not be suitable for all variables. We can do this via applying another inverse transform: $$ T_{ nu} sim frac{Z} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ As before but where: $U$ follows a uniform U[0,1] distribution $F^{-1}_{ chi^2_{ nu}}$ is the inverse cdf of $ chi^2_{ nu}$ . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 U = np.random.rand(SIMS) Chi1 = chi2.ppf(U,df=nu1) Chi2 = chi2.ppf(U, df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7703228652641819 . There is a small practical issue relating to multivariate student-t distributions: namely that we lose the ability to assume independence. This is a direct result of allowing for tail dependence. In many situations this is not an issue, however within this framework we have models covering very disperate processes some of which may genuinely exhibit independence. To illustrate this issue we will re-run the existing model with zero driver weights (&quot;attempt to model independence&quot;): . Estimated Pearson Correlation Coefficient: 0.08220534833363176 . As we can see there is a dependence between Y1 and Y2 0 clearly through the chi-square variates. We can overcome this issue by &quot;copying&quot; the driver process. The common uniform distribution is then replaced a number of correlated uniform distributions. We can then allow for independence. An implemntation of this can be seen in the code sample below: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers import numpy as np from scipy.stats import gamma, norm, chi2, t import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Use t CDF to convert sX synthetic variables to uniforms U U1 = t.cdf(sX1, df=nu1) U2 = t.cdf(sX2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7406745557389065 . To show this allows full independence we repeat the zero-weight example: . Estimated Pearson Correlation Coefficient: -0.01456173215652803 . We can see that this is a much better scatter plot if we are looking for independence! . Non-Centrality . We now extend this model yet further. So far we have allowed for tail dependence however it treats both tails equally. In some instances this can be problematic. For example if we rely on output from the framework to do any kind of risk-reward comparison the upisde and downside behaviour are both important. While it is easy to think of structural changes leading to a downside tail dependence an upside tail dependence is typically harder to justify. We can allow for this with a simple change to the model, namely: $$ T_{ nu, mu} sim frac{Z + mu} { sqrt{ frac{F^{-1}_{ chi^2_{ nu}}(U)} { nu}}} $$ The addition of the $ mu$ parameter means that $T_{ nu, mu}$ follows non-central student-t distribution with $ nu$ degrees of freedom and non-centrality $ mu$. Details of this distribution can be found on wikipedia. By selecting large positive values of $ mu$ we can create tail dependence in the higher percentiles, large negative values can create tail dependence in the lower percentiles and a zero value leads to a symmetrical dependency. Adjusting the code futher we get: . # Simple driver method example # We model a central driver Z # 2 additional drivers X1 and X2 are calculated off these # We want to model 2 risks: Y1 and Y2 which follow a gamma distribution # Synthetic normal variates sX1 and sX2 are used to apply dependence # Tail dependence is added through Chi1 and Ch2 with varying degrees # Chi1 and Chi2 are driven by X1tail and X2tail which are copies of X1 and X2 drivers # We add non-centrality through an additive scalar import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Simulate driver variables Z = np.random.normal(0, 1, SIMS) # Simulate copy of driver for tail process Ztail = np.random.normal(0, 1, SIMS) # Simulate additional driver variables X1, X2 and standardise X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate additional tail drivers X1tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2tail = (0.5 * Ztail + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) # Simulate Synthetic Variables sX and standardize sX1 = (0.5 * X1 + 0.25 * X2 + 0.25 * np.random.normal(0, 1, SIMS)) sX1 = (sX1 - sX1.mean()) / sX1.std() sX2 = (0.5 * X2 + 0.25 * X1 + 0.25 * np.random.normal(0, 1, SIMS)) sX2 = (sX2 - sX2.mean()) / sX2.std() # Simulate Synthetic Variables for tail process sX1tail = (0.5 * X1tail + 0.25 * X2tail + 0.25 * np.random.normal(0, 1, SIMS)) sX1tail = (sX1tail - sX1tail.mean()) / sX1tail.std() sX2tail = (0.5 * X2tail + 0.25 * X1tail + 0.25 * np.random.normal(0, 1, SIMS)) sX2tail = (sX2tail - sX2tail.mean()) / sX2tail.std() # Simulate Chi-Square for tail-dependence nu1 = 2 nu2 = 4 Chi1 = chi2.ppf(norm.cdf(sX1tail),df=nu1) Chi2 = chi2.ppf(norm.cdf(sX2tail), df=nu2) sX1 /= np.sqrt(Chi1 / nu1) sX2 /= np.sqrt(Chi2 / nu2) # Specify the non-centrality values nc1 = -2 nc2 = -2 # Use non-central t CDF to convert sX synthetic variables to uniforms U U1 = nct.cdf(sX1+nc1, nc=nc1, df=nu1) U2 = nct.cdf(sX2+nc2, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 Y1 = gamma.ppf(U1, 2) Y2 = gamma.ppf(U2, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1, Y2) plt.xlabel(&#39;Y1&#39;) plt.ylabel(&#39;Y2&#39;) plt.show() correl = np.corrcoef(Y1, Y2) print(&quot;Estimated Pearson Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Correlation Coefficient: 0.7100911602838634 . In the code example we have selected a non-centrality of -2 which is a fairly large negative value, we can see the dependency increasing in the lower percentiles (clustering around (0,0) on the plot). . Temporal Considerations . So far we have essentially considered a &quot;static&quot; model, we have modelled a number of drivers which represent values at a specific time period. For the majority of insurance contracts this is sufficient: we are only interested in losses occuring over the time period the contract is active. However in some instances the contracts relate to multiple time periods and it does not make sense to consider losses over the entire lifetime. Moreover it is not ideal to model time periods as independent from one another, to take the US economy example: if in 2020 the US enters recession it is (arguably) more likely that the US will also stay in recession in 2021. Clearly the dynamics of this are very complex and constructing a detailed temporal model is very difficult, however for the sake of creating the drivers we do not need to know the exact workings. Instead we are looking for a simple implementation that gives dynamics that are somewhat justifiable. . Fortunately it is relatively easy to add this functionality to the model framework we have described so far. Essentially we will adopt a Markovian assumption whereby a driver in time period t+1 is a weighted sum of its value at time t and an idiosyncratic component. Of course this is not a perfect description of the temporal behaviour of every possible driver but it shouldn&#39;t be completely unjustifiable in most instances and the trajectories shouldn&#39;t appear to be totally alien (e.g. US economy being in the top 1% one year immediately followed by a bottom 1% performance very frequently). . To illustrate this please see the code example below, for brevity I will change the model code above to a functional definition to avoid repeating blocks of code. . # Creating temporally dependent variables import numpy as np from scipy.stats import gamma, norm, chi2, nct import matplotlib.pyplot as plt %matplotlib inline # Set number of simulations and random seed SIMS = 1000 SEED = 123 np.random.seed(SEED) # Define function to create correlated normal distributions def corr_driver(): # Create driver Z Z = np.random.normal(0, 1, SIMS) # Create drivers X1, X2 X1 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) X2 = (0.5 * Z + 0.5 * np.random.normal(0, 1, SIMS)) / np.sqrt(0.5**2 + 0.5**2) return np.array([X1, X2]) # Create drivers variables for time periods t0 and t1 driver_t0 = corr_driver() driver_t1 = 0.5 * driver_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Create copy of drivers for tail process time periods t0 and t1 tail_t0 = corr_driver() tail_t1 = 0.5 * tail_t0 + 0.5 * corr_driver() / np.sqrt(0.5**2 + 0.5**2) # Define a standardise function def standardise(x): return (x - x.mean()) / x.std() # Create sythetic variables sX1 sX2 for variable 1 and 2 at times t0 and t1 # Note depending on the model idiosyncratic components may also be dependent sX1t0 = standardise(0.25*driver_t0[0] + 0.5*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1t1 = standardise(0.25*driver_t1[0] + 0.5*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t0 = standardise(0.5*driver_t0[0] + 0.25*driver_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2t1 = standardise(0.5*driver_t1[0] + 0.25*driver_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Repeat synthetic variable construction for tail process sX1tailt0 = standardise(0.25*tail_t0[0] + 0.5*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX1tailt1 = standardise(0.25*tail_t1[0] + 0.5*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt0 = standardise(0.5*tail_t0[0] + 0.25*tail_t0[1] + 0.25*np.random.normal(0, 1, SIMS)) sX2tailt1 = standardise(0.5*tail_t1[0] + 0.25*tail_t1[1] + 0.25*np.random.normal(0, 1, SIMS)) # Simulate Chi-Square for tail-dependence t0 and t1 nu1 = 2 nu2 = 4 Chi1t0 = chi2.ppf(norm.cdf(sX1tailt0),df=nu1) Chi2t0 = chi2.ppf(norm.cdf(sX2tailt0), df=nu2) sX1t0 /= np.sqrt(Chi1t0 / nu1) sX2t0 /= np.sqrt(Chi2t0 / nu2) Chi1t1 = chi2.ppf(norm.cdf(sX1tailt1),df=nu1) Chi2t1 = chi2.ppf(norm.cdf(sX2tailt1), df=nu2) sX1t1 /= np.sqrt(Chi1t1 / nu1) sX2t1 /= np.sqrt(Chi2t1 / nu2) # Specify the non-centrality values nc1 = 2 nc2 = 2 # Use non-central t CDF to convert sX synthetic variables to uniforms U for t0 and t1 U1t0 = nct.cdf(sX1t0+nc1, nc=nc1, df=nu1) U2t0 = nct.cdf(sX2t0+nc2, nc=nc2, df=nu2) U1t1 = nct.cdf(sX1t1+nc1, nc=nc1, df=nu1) U2t1 = nct.cdf(sX2t1+nc1, nc=nc2, df=nu2) # Use inverse transforms to create dependent samples of Y1 and Y2 at t0 and t1 Y1t0 = gamma.ppf(U1t0, 2) Y2t0 = gamma.ppf(U2t0, 3) Y1t1 = gamma.ppf(U1t1, 2) Y2t1 = gamma.ppf(U2t1, 3) # Plot a basic scatter to show dependence has been applied and calculate pearson coefficient plt.scatter(Y1t0, Y1t1) plt.xlabel(&#39;Y1(t=t0)&#39;) plt.ylabel(&#39;Y1(t=t1)&#39;) plt.show() correl = np.corrcoef(Y1t0, Y1t1) print(&quot;Estimated Pearson Auto-Correlation Coefficient:&quot;, correl[0,1]) . Estimated Pearson Auto-Correlation Coefficient: 0.37600307233845764 . In this code example we created to variables Y1 and Y2, each one taking a value from a Gamma distribution at times t0 and t1. Y1 and Y2 have a dependency between eachother but also temporally. . As with any temporal model the time period chosen is very important, typically for insurance contracts yearly time periods make sense. However in one particular model I developed there was a need for monthly simulations, rather than re-parameterising the entire central driver structure to work on a monthly basis (creating lots of extra data that will not be used by the vast majority of the models) I applied a &quot;Brownian Bridge&quot; type argument to interpolate driver simulations for each month. . Notes on Implementation . In this blog post I have not included the code exactly as it is implemented in production since this is my employer&#39;s IP. The implementation presented here is not very efficient and trying to run large portfolios in this way will be troublesome. In the full production implementation I used the following: . Strict memory management as the this is a memory hungry program | Certain aspects of the implementation are slow in pure python (and even Numpy) Cython and Numba are used for performance | The Scipy stats module is convenient but restrictive, it is better to either use the Cython address for Scipy special functions or implement functions from scratch. By implementing extended forms of some of the distribution functions one is also able to allow for non-integer degrees of freedom which is useful | The model naturally lends itself to arrays (vectors, matrices, tensors) however these tend to be sparse in nature, it is often better to construct &quot;sparse multiply&quot; type operations rather than inbuilt functions like np.dot | Conclusion . This blog posts represents the current iteration of the aggregation framework I have developed. It is considered a &quot;version 0.1&quot; implementation and is expected to develop as we use it more extensively and uncover further properties or issues. For example it is clear regardless of parameters selected the joint behaviour will always be (approximately) elliptical, as presented it is not possible to implement non-linearities (e.g. the price of some asset will only attain a maximum/minimum value dependent on some other driver indicator). It is not difficult to implement ideas like this when the need arises, the difficulty becomes more around how to implement the idea in a seamless way. . There are a couple of additional benefits to this framework which we have not mentioned, I will outline these here briefly: . It is possible to parallelise this process quite effectively as there are minimal bottlenecks/race conditions | The driver variables can be generated centrally and models can link to this central variable repository. From a work-flow perspective this means that individual underwriting teams can run models independently (quickly) leaving the risk teams to collate an analyse the results. (Sometimes called a federated workflow.) | The federated workflow means no specialist hardware is required, even very large portfolios can be run on standard desktops/laptops. | The current production version of this framework has around 5-10,000 driver variables (&quot;X1, X2&quot;) over 5 different hierarchical layers. These influence dependence between around 500,000 individual modelled variables (&quot;Y1, Y2&quot;) with 20 time periods (&quot;t0, t1&quot;). The quality of risk management analysis and reporting has increased dramatically as a result. . There are still some things left to do in relation to this framework and the work is on-going. These include: . Work relating to calibration and how to do this as efficiently as possible | Further work on increasing code efficiency | Further mathematical study of the framework&#39;s parameters | Study of the implied network behaviour: since we&#39;re placing risks on a network (driver structure) can we gain additional insight by considering contagion, critical nodes, etc.? | Further improvements to the workflow, how the model data is stored/collated etc. |",
            "url": "https://www.lewiscoleblog.com/insurance/computational-statistics/probability/copula/2020/01/01/Insurance-Aggregation-Model.html",
            "relUrl": "/insurance/computational-statistics/probability/copula/2020/01/01/Insurance-Aggregation-Model.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Welcome to my Blog . Lewis Cole 2020 . Hello, and welcome to my little corner of the internet. I have never written a blog before and so this is likely to be a bit of a work in progress slowly evolving through time. . Within this blog I aim to talk about some ideas that are of interest to me. My interests are quite broad and so there will likely be a wide variety of topics discussed. I suppose most, if not all, of these interests could be placed under the broad umbrella of “models” or “simulation”. Some particular areas or topics I wish to write about include: . Non-linear dynamics | Systems out of equilibrium | Fat-tail and extreme value statistics | Non-ergodicity | Path dependence | Individual vs Collective phenomena | Agent based modelling | Networks | Machine Learning / Data Analysis | Mathematics, Probability, Computer Science generally | Inter-disciplinary study | . Many of these areas could be considered “complex” or “complex systems” although I am not a big fan of the term due to lack of a consistent definition. In many cases we must rely on computational methods since traditional analytic methods tend to fall down. As a result most blog posts will contain sample code, I will write this in Python (with a variety of libraries/packages) owing to ease of understanding and it’s ubiquity. As such I will also write about more computational considerations such as: . Python packages | Other languages | Writing performant python | Optimization techniques | and so on | . From time to time I might also include more “thought pieces” on news/recent research or book reviews or similar. . Where applicable I will try and assume no specific knowledge of a particular subject and try and build up to a somewhat sophisticated level of understanding. However I will be forced to assume a certain level of mathematical/statistical/programming understanding as I would rather not write articles on the fundamentals, where possible I will try and mention the name of techniques used so if they appear unfamiliar it will at least be possible to search for resources online. . To create this blog I used the fast.ai fastpages which has made the job much easier. I have a great deal of gratitude to Jeremy Howard, Hamel Husain and the folks at fast.ai for making this simple as possible so I can focus on creating content instead of worrying about the technicalities of creating the blog. . Thank you for your time, I hope you enjoy my blog posts. . If you would like to get in contact with me you can via my twitter account or via this blog’s email: .",
          "url": "https://www.lewiscoleblog.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}