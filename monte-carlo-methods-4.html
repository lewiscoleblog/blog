<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Monte Carlo Methods 4: Integration | Lewis Cole Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Monte Carlo Methods 4: Integration" />
<meta name="author" content="Lewis Cole (2020)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this blog post we turn our attention away from sampling from generic distributions to the problem of Monte-Carlo integration. Building up from a simple estimator we look at various variance reduction methods and approximations." />
<meta property="og:description" content="In this blog post we turn our attention away from sampling from generic distributions to the problem of Monte-Carlo integration. Building up from a simple estimator we look at various variance reduction methods and approximations." />
<link rel="canonical" href="https://www.lewiscoleblog.com/monte-carlo-methods-4" />
<meta property="og:url" content="https://www.lewiscoleblog.com/monte-carlo-methods-4" />
<meta property="og:site_name" content="Lewis Cole Blog" />
<meta property="og:image" content="https://github.com/lewiscoleblog/blog/raw/master/images/Monte-Carlo/integral.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-01T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Lewis Cole (2020)"},"description":"In this blog post we turn our attention away from sampling from generic distributions to the problem of Monte-Carlo integration. Building up from a simple estimator we look at various variance reduction methods and approximations.","@type":"BlogPosting","headline":"Monte Carlo Methods 4: Integration","dateModified":"2020-06-01T00:00:00-05:00","datePublished":"2020-06-01T00:00:00-05:00","image":"https://github.com/lewiscoleblog/blog/raw/master/images/Monte-Carlo/integral.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.lewiscoleblog.com/monte-carlo-methods-4"},"url":"https://www.lewiscoleblog.com/monte-carlo-methods-4","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.lewiscoleblog.com/feed.xml" title="Lewis Cole Blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Monte Carlo Methods 4: Integration | Lewis Cole Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Monte Carlo Methods 4: Integration" />
<meta name="author" content="Lewis Cole (2020)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this blog post we turn our attention away from sampling from generic distributions to the problem of Monte-Carlo integration. Building up from a simple estimator we look at various variance reduction methods and approximations." />
<meta property="og:description" content="In this blog post we turn our attention away from sampling from generic distributions to the problem of Monte-Carlo integration. Building up from a simple estimator we look at various variance reduction methods and approximations." />
<link rel="canonical" href="https://www.lewiscoleblog.com/monte-carlo-methods-4" />
<meta property="og:url" content="https://www.lewiscoleblog.com/monte-carlo-methods-4" />
<meta property="og:site_name" content="Lewis Cole Blog" />
<meta property="og:image" content="https://github.com/lewiscoleblog/blog/raw/master/images/Monte-Carlo/integral.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-01T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Lewis Cole (2020)"},"description":"In this blog post we turn our attention away from sampling from generic distributions to the problem of Monte-Carlo integration. Building up from a simple estimator we look at various variance reduction methods and approximations.","@type":"BlogPosting","headline":"Monte Carlo Methods 4: Integration","dateModified":"2020-06-01T00:00:00-05:00","datePublished":"2020-06-01T00:00:00-05:00","image":"https://github.com/lewiscoleblog/blog/raw/master/images/Monte-Carlo/integral.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.lewiscoleblog.com/monte-carlo-methods-4"},"url":"https://www.lewiscoleblog.com/monte-carlo-methods-4","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://www.lewiscoleblog.com/feed.xml" title="Lewis Cole Blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Lewis Cole Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/welcome/">Welcome</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Monte Carlo Methods 4: Integration</h1><p class="page-description">In this blog post we turn our attention away from sampling from generic distributions to the problem of Monte-Carlo integration. Building up from a simple estimator we look at various variance reduction methods and approximations.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-01T00:00:00-05:00" itemprop="datePublished">
        Jun 1, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Lewis Cole (2020)</span></span>
       <!-- • <span class="read-time" title="Estimated read time">
    
    
      22 min read
    
</span>-->
    </p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Monte-Carlo">Monte-Carlo</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Statistics">Statistics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Probability">Probability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Computational-Statistics">Computational-Statistics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Theory">Theory</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Computation">Computation</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Naïve-Monte-Carlo-Integration">Naïve Monte-Carlo Integration </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Central-Limit-Theorem-for-Monte-Carlo-Integration">Central Limit Theorem for Monte Carlo Integration </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Riemann-Approximation">Riemann Approximation </a></li>
<li class="toc-entry toc-h2"><a href="#Importance-Sampling">Importance Sampling </a></li>
<li class="toc-entry toc-h2"><a href="#Rao-Blackwell-Method">Rao-Blackwell Method </a></li>
<li class="toc-entry toc-h2"><a href="#Antithetic-Sampling">Antithetic Sampling </a></li>
<li class="toc-entry toc-h2"><a href="#Laplace-Approximations">Laplace Approximations </a></li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-01-Monte-Carlo-Methods-4.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p>This is the fourth blog post in a series - you can find the previous blog post <a href="https://lewiscoleblog.com/monte-carlo-methods-3">here</a></p>
<hr>
<p>In the previous few blog posts we have looked at how to sample from both univariate and multi-variate distributions. We now move onto another of main "themes" of Monte-Carlo method: the integration problem.</p>
<h2 id="Naïve-Monte-Carlo-Integration">
<a class="anchor" href="#Na%C3%AFve-Monte-Carlo-Integration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Naïve Monte-Carlo Integration<a class="anchor-link" href="#Na%C3%AFve-Monte-Carlo-Integration"> </a>
</h2>
<p>Recall from our first blog post in this series we looked at how to estimate $\pi$ using a PRNG. This is an example of a Monte-Carlo integration. Instead of using a counting argument based around covering the shape with small simple shapes of known volume (i.e. traditional analytic integration) instead we simply fire "paintballs" at the shape and count the proportion landing inside. We can do this for generic functions too. Suppose we have a function $f(.)$ and we want to calculate: $I = \int_0^1 f(x) dx$. Then for a sequence of uniform variates $(u_1,...,u_N)$ we can take the estimator: $\hat{I} = \frac{1}{N} \sum_{i=1}^N f(u_i)$. The similarities to the $\pi$ example should be clear. Let's consider a specific example:</p>
<p>
$$f(x) = \frac{\log(\Gamma(e^{\sin(x)+\cos(x)}))}{\log(\Gamma(e^{\sin(x)}))+\log(\Gamma(e^{\cos(x)}))} $$
</p>
<p>Which we want to integrate over one full period $x \in [0, 2\pi]$:</p>
<p>
$$ I = \int_0^{2\pi} \frac{\log(\Gamma(e^{\sin(x)+\cos(x)}))}{\log(\Gamma(e^{\sin(x)}))+\log(\Gamma(e^{\cos(x)}))} dx $$
</p>
<p>There is no real meaning to this function, I just wanted to pick something sufficiently complicated that the integral would not exist in analytic form (even Wolfram Alpha does not give me an approximate integral value!). We can create a Monte-Carlo estimator of this integral below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Basic Monte-Carlo Integration Estimator</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">gamma</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="c1"># Define function for integration</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span> <span class="o">/</span> <span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span> <span class="p">)</span>

<span class="c1"># Create samples function</span>
<span class="k">def</span> <span class="nf">samples</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">):</span>
    <span class="c1"># Create uniform variates</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">+</span><span class="n">a</span>
    
    <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>

<span class="c1"># Define integrator function</span>
<span class="k">def</span> <span class="nf">integrate_1d</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">):</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">samples</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    
    <span class="c1"># Return integral estimate</span>
    <span class="k">return</span> <span class="n">vals</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">steps</span>

<span class="c1"># Plot function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'f(x)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Plot of f(x)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Print integral estimate</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Estimated area under curve for 1 million samples:"</span><span class="p">,</span> <span class="n">integrate_1d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Estimated area under curve for 1 million samples: 5.222711387883453
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(Note: this function does not have a discontinuity - however the peak is very high c.80 so the full range is not shown in the graph above.)</p>
<h3 id="Central-Limit-Theorem-for-Monte-Carlo-Integration">
<a class="anchor" href="#Central-Limit-Theorem-for-Monte-Carlo-Integration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Central Limit Theorem for Monte Carlo Integration<a class="anchor-link" href="#Central-Limit-Theorem-for-Monte-Carlo-Integration"> </a>
</h3>
<p>We can see that our estimated integral is around $5.2$, however at this stage we have no idea whether this is a "good" estimate or a "bad" estimate. In the (rather contrived) $\pi$ estimate example we had some idea of what the value "should" be but in this case we do not have any real intuition. We can look at the graph and try and convince ourselves the estimate is reasonable but how do we know? Thankfully we can rely on some theory to help us gain a better understanding.</p>
<p>To better understand the properties of our estimator we rely on the central limit theorem (CLT):</p>
<hr>
<p><strong>Theorem: Central Limit Theorem (for Monte Carlo)</strong></p>
<p>Suppose we wish to estimate the integral:</p>
<p>
$$\mathbb{E}_f[h(X)] = \int h(x)f(x) dx$$
</p>
<p>For some variable $X$ with pdf $f$. We can approximate this with a uniform sample via:</p>
<p>
$$\overline{h}_N = \frac{1}{N} \sum_{i=1}^N h(u_i)$$
</p>
<p>For $u_i$ iid samples from the uniform distribution. We let:</p>
<p>
$$\overline{v}_N = \frac{1}{N^2} \sum_{i=1}^N (h(u_i) - \overline{h}_N)^2$$
</p>
<p>Then assuming $\mathbb{E}_f[h(X)]$ and $\operatorname{Var}_f[h(X)]$ both exist and are finite then:</p>
<p>
$$ \frac{\overline{h}_N - \mathbb{E}_f[h(X)] }{\sqrt{\overline{v}_N}} \xrightarrow[N \to \infty]{\text{D}} \mathcal{N}(0,1) $$
</p>
<p>That is the sample mean $\overline{h}_N$ converges in distribution to a Gaussian distribution.</p>
<hr>
<p>We can use this result in a number of ways, for example we can calculate a 95% confidence interval using our estimator above:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Number of Samples</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>

<span class="c1"># Create Samples</span>
<span class="n">samp</span> <span class="o">=</span> <span class="n">samples</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="c1"># Calculate h_N and v_N</span>
<span class="n">h_N</span> <span class="o">=</span> <span class="n">samp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">N</span>
<span class="n">v_N</span> <span class="o">=</span> <span class="p">(</span><span class="n">samp</span> <span class="o">-</span> <span class="n">h_N</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">v_N</span> <span class="o">=</span> <span class="n">v_N</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">N</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Find 95th percentile of standard normal</span>
<span class="n">pt</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>

<span class="c1"># Create confidence interval</span>
<span class="n">CI</span> <span class="o">=</span> <span class="p">[</span> <span class="n">h_N</span> <span class="o">-</span> <span class="n">pt</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_N</span><span class="p">),</span> <span class="n">h_N</span> <span class="o">+</span> <span class="n">pt</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_N</span><span class="p">)]</span>

<span class="c1"># Print results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Integral Estimate:"</span><span class="p">,</span> <span class="n">h_N</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"95% Confidence Interval:"</span><span class="p">,</span> <span class="n">CI</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Integral Estimate: 5.212267819546782
95% Confidence Interval: [5.184822438263345, 5.239713200830219]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This gives us some idea of how "good" our estimator is, we would expect to see a random estimate using this method to fall outside this range only 1 in 20 attempts. Depending on our application this may or may not be acceptable. We can of course increase the number of samples to improve this estimate if we wanted to. Further we know that if we increase the number of samples to $N' = \Delta N$ - then the variance should decrease by a factor $\Delta$ and so we can use this to work out how many samples should be required to get to a specific level of desired accuracy.</p>
<p>To check the confidence interval makes sense we can repeatedly sample and create an empirical 95% confidence interval:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># N is number of samples per estimate</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>

<span class="c1"># M is the number of samples of h_N</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">h_N_M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">h_N_M</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">integrate_1d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    
<span class="n">CI_emp</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">h_N_M</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">h_N_M</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">)]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Empirical Confidence Interval:"</span><span class="p">,</span> <span class="n">CI_emp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"CLT Confidence Interval"</span><span class="p">,</span> <span class="n">CI</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Empirical Confidence Interval: [5.2009155048412, 5.256399451162318]
CLT Confidence Interval [5.184822438263345, 5.239713200830219]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the empirical and CLT derived confidence intervals are reasonably close, which is what we expect. It is always worth remembering that in calculating any quantity witha Monte-Carlo method (or working with "real" data) any summary statistic or quantity calculated is itself nothing more than a sample from a distribution! We therefore need to treat it only as an estimate (not realising this is one of the most significant causes of incorrect conclusions being drawn from models/data!)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Riemann-Approximation">
<a class="anchor" href="#Riemann-Approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Riemann Approximation<a class="anchor-link" href="#Riemann-Approximation"> </a>
</h2>
<p>Thinking once again about our integral in general terms:</p>
<p>
$$ I = \mathbb{E}_f(h(X)) = \int_D h(x)f(x) dx$$
</p>
<p>Let's not see if we can improve upon this estimate. If we were to approach this in a Riemann (analytic) way we would consider a sequence of ordered sequences $(a_{i,n})_{i=1}^n$ such that: $|a_{i+1,n} - a_{i,n}| \xrightarrow{n \to \infty}  0$. We then approximate the integral via:</p>
<p>
$$ I_n = \sum_{i=0}^{n-1} h(a_{i,n})f(a_{i,n})(a_{i+1,n} - a_{i,n}) $$
</p>
<p>We can mimic this approach in a Monte-Carlo setting by sampling $(x_1,...,x_N)$ iid rom $f(.)$ such that: $x_1 \leq ... \leq x_N$ and taking:</p>
<p>
$$ I_N \approx \sum_{i=0}^{N-1} h(x_i)f(x_i)(x_{i+1} - x_i) $$
</p>
<p>This estimator has variance of order $\mathcal{O}(N^{-2})$ which we can improve further by taking the average value:</p>
<p>
$$ I_N \approx \sum_{i=0}^{N-1} \frac{h(x_{i+1}) - h(x_i)}{2}f(x_i)(x_{i+1} - x_i) $$
</p>
<p>If the second derivative of $h(.)$ is bounded then we have an estimator with variance $\mathcal{O}(N^{-4})$. On the surface this appears to be an incredibly powerful method since the simple Monte-Carlo estimator we looked at initially has variance $\mathcal{O}(N^{-1})$. However things are not that easy, the Riemann approximation method essentially "covers" the entire space, so in 1d this is very efficient. However as soon as we increase the dimensionality of the space this becomes less efficient. In fact for dimension $d$ the Riemann approximation (assuming bounded second derivative) is: $\mathcal{O}(N^{-4/d})$ - this is called the curse of dimensionality and is why Monte-Carlo methods can be so useful in estimating complex high dimensional integrals! Say we are looking at an integral in $12$ dimensions then the Riemann estimator would have variance  $\mathcal{O}(N^{-1/3})$ - whereas the naïve approach would still have variance $\mathcal{O}(N^{-1})$. The naïve approach having a significant improvement in variance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Importance-Sampling">
<a class="anchor" href="#Importance-Sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Importance Sampling<a class="anchor-link" href="#Importance-Sampling"> </a>
</h2>
<p>With the naïve Monte-Carlo example above we have attacked the problem in quite a "natural" way, we have some sense of how "good" the estimator of our integral is thanks to calculating $\overline{v}_N$ the sample variance of the estimator. This statistic allows us to "compare" estimators, an estimator with a lower variance being "better". We also note that we could have parameterized this integral in any number of ways, for example we could take a change of variable (e.g. $y = \log(x)$) or in some cases we could rely on a geometric argument (e.g. if the integral is symmetrical around some value) - this can allow us to sample points more "efficiently" to create a better estimate for a given number of samples. However practically we always have to weigh this up against how long it takes to create a sample (e.g. if there is a transform that leads to a better variance but each sample takes 1000 times longer to generate it may not be wise to take the "better" estimator!)</p>
<p>We now look further at this idea through the concept of Importance sampling:</p>
<hr>
<p><strong>Definition: Importance Sampling</strong></p>
<p>Let $(x_1,...,x_N)$ be samples according to some distribution function $g(.)$. The the importance sampling method can be represented:</p>
$$\mathbb{E}_f[h(X)] \approx \frac{1}{N} \sum_{i=1}^N \frac{f(x_i)}{g(x_i)}h(x_i) $$<p></p>
<p>This follows from re-expressing the expectation as:</p>
<p>
$$\mathbb{E}_f[h(X)] = \int h(x)f(x) dx = \int h(x) \frac{f(x)}{g(x)} g(x) dx $$
</p>
<hr>
<p>Notice how we do not have many constraints on the choice of distribution $g(.)$, importance sampling is therefore a very powerful method. We can also generate our samples and use them repeatedly for different choices of $f(.)$ and $h(.)$ which can be useful in certain circumstances.</p>
<p>To illustrate this power we look at an example of estimating the frequency of a rare event. Suppose $Z \sim \mathcal{N}(0,1)$ and we wish to estimate $\mathbb{P}(Z &gt; 5)$. The naive approach would be to use:</p>
<p>
$$ \mathbb{P}(Z &gt; 5) \approx \frac{1}{N} \sum_{i=1}^N \mathbb{1}(z_i &gt; 5) $$
</p>
<p>For $z_i$ iid realisations of the standard Gaussian. If we try this even with $N=10,000$ we're not likely to find any samples above the threshold and so our estimate will most likely be zero. If instead we take $Y \sim \mathcal{TE}(5,1)$ - that is an exponential distribution truncated at 5 with scale 1. The density function being:</p>
<p>
$$ f_Y(y) = \frac{e^{-(y-5)}}{\int_{5}^{\infty} e^{-(x-5)}dx} $$
</p>
<p>With $y_i$ iid samples from this distribution we can use the estimator:</p>
<p>
$$ \mathbb{P}(Z &gt; 5) \approx \frac{1}{N} \sum_{i=1}^N \frac{\phi(y_i)}{f_Y(y_i)} \mathbb{1}(y_i &gt; 5) $$
</p>
<p>Which should lead to better performance. Let's take a look at this in action:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Importance Sampling Example</span>
<span class="c1"># Estimating tail probability of a standard normal</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">expon</span>

<span class="c1"># Set number samples</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">thresh</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Naive estimate</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">thresh</span>
<span class="n">naive_est</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">N</span>

<span class="c1"># Importance Sampling estimate</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">expon</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">thresh</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="n">expon</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">thresh</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">)</span>
<span class="n">IS_est</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">N</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Naive Tail Probability Estimate:"</span><span class="p">,</span> <span class="n">naive_est</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Importance Sampling Tail Probability Estimate:"</span><span class="p">,</span> <span class="n">IS_est</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Naive Tail Probability Estimate: 0.0
Importance Sampling Tail Probability Estimate: 2.8430113423144186e-07
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that with just $10,000$ samples we have been able to create an estimate for a probability that has a return period of the order of $10,000,000$ - far in excess of the number of samples. This is an incredibly powerful concept. To find a suitable instrumental distribution we relied on the theory of large deviations - we will not cover this here but it may form a future blog post.</p>
<p>Although there is (next to) no restrictions on the instrumental distribution $g(.)$ used for importance sampling, clearly some distributions will work better than others. How can we select a "good" distribution? Firstly we have that the variance of the importance sampling estimator is finite only when:</p>
<p>
$$ \mathbb{E}_g\left[h^2(X) \frac{f^2(X)}{g^2(X)}\right] = \mathbb{E}_f\left[h^2(X) \frac{f(X)}{g(X)}\right] &lt; \infty$$
</p>
<p>We thus have that $g(.)$ with lighter tails than $f(.)$ are not appropriate for importance sampling since they lead to estimators of infinite variance. This leads to two sufficient conditions for finite variance estimators:</p>
<ol>
<li>$\frac{f(x)}{g(x)} &lt; M$ for some $M$ for all $x$ and $\operatorname{Var}_f(h)&lt;\infty$</li>
<li>$X$ has compact support with $f(x) &lt; F$ for some $F$ and $g(x)&gt; \epsilon$ for all $x$</li>
</ol>
<p>An alternate form of the importance sampling estimator that sometimes performs better than the form above is:</p>
<p>
$$ I \approx \frac{\sum\limits_{i=1}^N h(x_i) \frac{f(x_i)}{g(x_i)}}{\sum\limits_{i=1}^N \frac{f(x_i)}{g(x_i)}} $$
</p>
<p>This works since: $\frac{1}{N} \sum_{i=1}^N \frac{f(x_i)}{g(x_i)} \xrightarrow{\text{a.s}} 1$. The estimator is itself biased, albeit with small bias. The reduction in variance can be worth the reduction in bias.</p>
<hr>
<p><strong>Theorem:</strong></p>
<p>The selection of $g(.)$ that minimises the variance of an importance sampling estimator is:</p>
<p>
$$ g^*(x) = \frac{|h(x)|f(x)}{\int |h(z)| f(z) dz} $$
</p>
<hr>
<p><strong>Proof:</strong></p>
<p>Recall overall variance of the estimator can be expressed as sums of variances of the form:</p>
<p>
$$\operatorname{Var}\left[ \frac{h(X)f(X)}{g(X)} \right] = \mathbb{E}_g\left[h^2(X) \frac{f^2(X)}{g^2(X)}\right] - \left( \mathbb{E}_g\left[\frac{h(X)f(X)}{g(X)}\right]\right)^2 $$
</p>
<p>We note that the second term of this sum does not depend on $g(.)$ at all and so it suffices to minimize the first term. By Jensen's inequality we have:</p>
<p>
$$ \mathbb{E}_g\left[h^2(X) \frac{f^2(X)}{g^2(X)}\right] \geq \left( \mathbb{E}_g\left[\frac{h(X)f(X)}{g(X)}\right]\right)^2 = \left( \int |h(z)| f(z) dz \right)^2 $$
</p>
<p>Which provides us with the lower bound $g^*(.)$.  $\square$</p>
<hr>
<p>Unfortunately our bound requires us to know about: $\int h(x)f(x) dx$ which is preciesly the integral we're looking to evaluate! Practically we look for $g(.)$ such that $\frac{|h(.)|f(.)}{g(.)}$ is almost constant with finite variance. Further importance sample will perform poorly if:</p>
<p>
$$ \int \frac{f^2(x)}{g(x)} dx = \infty $$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Rao-Blackwell-Method">
<a class="anchor" href="#Rao-Blackwell-Method" aria-hidden="true"><span class="octicon octicon-link"></span></a>Rao-Blackwell Method<a class="anchor-link" href="#Rao-Blackwell-Method"> </a>
</h2>
<p>We now look at another method for reducing the variance of an estimator through the use of the Rao-Blackwell theorem. Suppose we have some estimator: $\delta(X)$ some function of a random-variable $X$ that estimates some parameter $\theta$. Given a sufficient statistic for the parameter: $T(X)$ then we can define the Rao-Blackwell estimator as:</p>
<p>
$$ \delta_1(X) = \mathbb{E}[\delta(X) | T(X)] $$
</p>
<p>We can note that the variance of $\delta_1(X)$ has be less than (or equal to) the variance of estimator $\delta$ - why is this? By looking at the mean square error we have:</p>
<p>
$$ \mathbb{E} [(\delta _{1}(X)-\theta )^{2}] = \mathbb{E} [(\delta (X)-\theta )^{2}]- \mathbb{E} [\operatorname {Var} (\delta (X)\mid T(X))] $$
</p>
<p>Since variance is always positive this leads to:</p>
<p>
$$ \mathbb{E} [(\delta _{1}(X)-\theta )^{2}] \leq \mathbb{E} [(\delta (X)-\theta )^{2}] $$
</p>
<p>Which means the variance must also decrease via the Rao-Blackwell estimate.</p>
<p>In the context of Monte-Carlo integration however we can "drop" the requirement of sufficient statistics on the conditioning variable. Suppose again our quantity of interest is:</p>
<p>
$$I = \mathbb{E}_f[h(X)] $$
</p>
<p>If we have a joint distribution function $g(x,y)$ for variables $(X,Y)$ such that:</p>
<p>
$$ \int g(x,y) dy = f(x) \quad \forall x $$
</p>
<p>Then we can take the Rao-Blackwell estimator to be:</p>
<p>
$$ \delta_1(X) = \mathbb{E}_f[\delta(X)|Y] $$
</p>
<p>And the argument above still holds leading to a reduced variance estimator. However the use of this method is itself rather limited due to the need to find suitable joint distribution functions and for the conditional expectations to have a convenient form (i.e. we would not want to have a Rao-Blackwell estimator that itself needed a Monte-Carlo method to evaluate!). In spite of this Rao-Blackwell methods can be quite powerful when combined with accept-reject style algorithms.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Antithetic-Sampling">
<a class="anchor" href="#Antithetic-Sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Antithetic Sampling<a class="anchor-link" href="#Antithetic-Sampling"> </a>
</h2>
<p>Another way to reduce the variance of an estimator is to use an antithetic sampling procedure. The intuition behind this is fairly simple. So far we have talked about the need for "independent" samples for use in Monte-Carlo methods. There are times however when this is not always ideal. Let's suppose we are estimating two integrals: $I_1$ and $I_2$. To do this we create estimators: $\delta_1(.)$ and $\delta_2(.)$ respectively. Notice that if we are concerned with the quantity: $I_1 + I_2$, using our estimators we have:</p>
<p>
$$ \operatorname{Var}[ \delta_1(X) + \delta_2(Y) ] = \operatorname{Var}[\delta_1(X)] + \operatorname{Var}[\delta_2(X)] + 2 \operatorname{Cov}[\delta_1(X), \delta_2(Y)] $$
</p>
<p>We can then see that our estimator for $I_1+I_2$ has lowest variance when we have negative correlation between $\delta_1(X)$ and $\delta_2(Y)$!</p>
<p>But what options do we have to ensure this? Clearly we could rely on the multi-variate generation methods (e.g. copulae) from a previous blog post, but this is often overkill and the computational cost outweighs the reduction in variance. Fortunately we can overcome this problem very easily when using an inverse transform method - we generate a uniform variate: $u_i$ for use in the $\delta_1(.)$ estimator and then use $(1-u_i)$ for use in estimator $\delta_2(.)$.</p>
<p>For our integration problem this has the following interpretation:</p>
<p>
$$ J_1 = \frac{1}{2N} \sum_{i=1}^N [h(F^{-1}(u_i)) + h(F^{-1}(1-u_i))] $$
</p>
<p>Should achieve a better convergence (lower variance) than the estimator:</p>
<p>
$$ J_2 = \frac{1}{2N} \sum_{i=1}^{2N} h(F^{-1}(u_i)) $$
</p>
<p>Before considering the implications of generating an extra $N$ pseudo-random numbers. Let's take a look at an example of this in action. We will look to estimate the integral:</p>
<p>
$$ \int_{0}^{1} \frac{1}{1+x} dx $$
</p>
<p>We know that this can be evaluated analytically as $\ln(2)$ but we shall use Monte-Carlo to calculate it here. By taking $1,000$ estimators each with $N=1,000$ we can get an idea of the variance in the estimators:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># An example of antithetic sampling</span>
<span class="c1"># Note this is a slow running highly non-optimal code!</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">gamma</span>

<span class="c1"># Fix number of estimators M and number of samples per estimate N/2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Set up arrays of estimates</span>
<span class="n">regular</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="n">antithetic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="c1"># Define h(x)</span>
<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Non-antithetic approach</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">)</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">regular</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">)</span>

<span class="c1"># Antithetic approach</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">u</span><span class="p">)</span>
    <span class="n">antithetic</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">)</span>

<span class="c1"># Calculate mean and variance of regular vs antithetic sample estimates</span>
<span class="n">regular_mean</span> <span class="o">=</span> <span class="n">regular</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">regular_var</span> <span class="o">=</span> <span class="n">regular</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">**</span><span class="mi">2</span>

<span class="n">ant_mean</span> <span class="o">=</span> <span class="n">antithetic</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">ant_var</span> <span class="o">=</span> <span class="n">antithetic</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Print results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Independent Sampling:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Bias:"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">regular_mean</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)),</span> <span class="s2">"Variance:"</span><span class="p">,</span> <span class="n">regular_var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Antithetic Sampling:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Bias:"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ant_mean</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)),</span> <span class="s2">"Variance:"</span><span class="p">,</span> <span class="n">ant_var</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Independent Sampling:
Bias: 3.633843451245067e-05 Variance: 9.813137160375326e-06


Antithetic Sampling:
Bias: 9.54802630492857e-06 Variance: 5.864033213624619e-07
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see here that both sampling procedures produce reasonable estimates in terms of bias - the variance for the antithetic sampler is noticably less.</p>
<p>Of course this method is not without its issues: it is not necessarily a "pre-canned" solution and we have to think about the problem at hand. For very complicated models with many interacting components, implementing antithetic sampling can become so complicated that it makes the code too difficult to understand. Further we are generally unable to combine use this method when relying on accept/reject type methods of sampling - we are essentially limited to using inverse-transform methods which themselves can have their own set of issues (as noted in previous blog posts). Nonetheless antithetic sampling is a useful tool in the modellers arsenal.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Laplace-Approximations">
<a class="anchor" href="#Laplace-Approximations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Laplace Approximations<a class="anchor-link" href="#Laplace-Approximations"> </a>
</h2>
<p>We now move on to look at Laplace approximations to an integral. Unlike the methods shown previously they are not technically "Monte-Carlo" methods in the sense that they do not rely on pseudo-random number samples. Instead we apply some analytic approximations to our integrand in order to simplify its evaluation. This can be useful for problems where Monte-Carlo is prohibitively expensive and the approximations calculated can be used as a guide or "ballpark estimate" for sense checking the results of a more sophisticated Monte-Carlo simulation engine. We can also use the method as a way of generating proposal distributions when no obvious proposal exists for our Monte-Carlo method.</p>
<p>We will outline the Laplace procedure below (some of the more subtle details of the derivation are excluded). Suppose we have an integral of the form:</p>
<p>
$$ I = \int_D f_{\theta}(x) dx $$
</p>
<p>Where $D$ is some sub-domain of function $f_{\theta}$ with $\theta$ being some (fixed) parameter vector for the function (e.g $\mu$ and $\sigma$ for the Gaussian pdf). We will assume that $f(.)$ is non-negative and integrable (some extensions have been proposed to drop these requirements but we shall make this assumption for now). If we re-write $f(.)$ in the form: $f_{\theta}(x) = e^{n h_{\theta}(x)}$ (hence for the need for $f(.)$ to be non-negative). Then we can write:</p>
<p>
$$ I = \int_D e^{n h_{\theta}(x)} dx $$
</p>
<p>We can write a Taylor expansion of $h_{\theta}(.)$ about the point $x_0$ via:</p>
<p>
$$ h_{\theta}(x) \approx h_{\theta}(x_0) + (x-x_0)h'_{\theta}(x_0) + \frac{(x-x_0)^2}{2!}h''_{\theta}(x_0) + \frac{(x-x_0)^3}{3!}h'''_{\theta}(x_0) + \mathcal{O}((x-x_0)^3) $$
</p>
<p>Where: $\mathcal{O}(.)$ is big-O notation. If we pick $x_0$ to be an attained maximum of the function $h_{\theta}(.)$ then the first derivative is zero and so we have (ignoring the big-O term):</p>
<p>
$$ h_{\theta}(x) \approx h_{\theta}(x_0) + \frac{(x-x_0)^2}{2!}h''_{\theta}(x_0) + \frac{(x-x_0)^3}{3!}h'''_{\theta}(x_0)$$
</p>
<p>We can thus re-write an approximation to our integral as:</p>
<p>
$$ I = e^{n h_{\theta}(x_0)} \int_D e^{n \frac{n(x-x_0)^2}{2}h''_{\theta}(x_0)} e^{\frac{n(x-x_0)^3}{3!}h'''_{\theta}(x_0)} dx $$
</p>
<p>With $h'_{\theta}(x_0) = 0$ ($x_0$ attaining the maximum). We can now apply a second Taylor expansion to the term: $e^{\frac{n(x-x_0)^3}{3!}h'''_{\theta}(x_0)}$ - by noting $e^y \approx 1 + y + \frac{y^2}{2!} + \mathcal{O}(y^2)$. By taking the expansion around $x_0$ again we get:</p>
<p>
$$ e^{\frac{n(x-x_0)^3}{3!}h'''_{\theta}(x_0)} \approx 1 + \frac{n(x-x_0)^3}{3!}h'''_{\theta}(x_0) + \frac{n^2(x-x_0)^6}{2!(3!)^2} (h'''_{\theta}(x_0))^2 + \mathcal{O}((x-x_0)^6) $$
</p>
<p>By excluding the big-O term, we can plug this back into our integral approximation to give:</p>
<p>
$$ I \approx e^{n h_{\theta}(x_0)} \int_D e^{n \frac{n(x-x_0)^2}{2}h''_{\theta}(x_0)} \left[  1 + \frac{n(x-x_0)^3}{3!}h'''_{\theta}(x_0) + \frac{n^2(x-x_0)^6}{2!(3!)^2} (h'''_{\theta}(x_0))^2 \right] dx $$
</p>
<p>This is the 3rd order Laplace approximation to the integral. If we limit ourselves to the first order approximation:</p>
<p>
$$ I \approx e^{n h_{\theta}(x_0)} \int_D e^{n \frac{n(x-x_0)^2}{2}h''_{\theta}(x_0)} $$
</p>
<p>We see that the integrand is nothing more than the kernel of the Gaussian density with mean $x_0$ and variance $\frac{-1}{nh''_{\theta}(x_0)}$. If our integral domain is: $D = [a,b]$ - some line segment on the real line. we can write the Laplace approximation in the form:</p>
<p>
$$ I = \int_a^b f_{\theta}(x) dx \approx e^{n h_{\theta}(x_0)} \sqrt{\frac{-2\pi}{nh''_{\theta}(x_0)}} \left[ \Phi\left[\sqrt{-nh''_{\theta}(x_0)}(b-x_0)\right] -  \Phi\left[\sqrt{-nh''_{\theta}(x_0)}(a-x_0)\right]  \right] $$
</p>
<p>Where $\Phi[.]$ is the CDF of the standard Gaussian. This provides us with a convenient way of estimating an integral without needing to produce samples from a distribution. Note: that by choosing $x_0$ to be the maximum we ensure: $h''_{\theta}(x_0) \leq 0$ and so all square-roots are defined in the approximation above.</p>
<p>Let's look at an example of this in action. We will take an example where we can calculate an exact analytic solution so we can see how the approximation behaves. We can take a Gamma integral:</p>
<p>
$$ \int_a^b \frac{x^{\alpha-1}}{\Gamma(\alpha)\beta^{\alpha}} e^{-\frac{x}{\beta}} dx $$
</p>
<p>We have:</p>
<p>
$$ h_{\theta}(x) = - \frac{x}{\beta}+(\alpha -1) \log{x}$$
</p>
<p>This function is maximized with:</p>
<p>
$$x_0 = (\alpha-1)\beta$$
</p>
<p>For $\alpha, \beta &gt; 1$. We therefore get the approximation:</p>
<p>
$$ \int_a^b \frac{x^{\alpha-1}}{\Gamma(\alpha)\beta^{\alpha}} e^{-\frac{x}{\beta}} dx  \approx \frac{x_0^{\alpha-1} e^{-\frac{x_0}{\beta}}}{\Gamma(\alpha)\beta^{\alpha}} \sqrt{\frac{2\pi x_0^2}{\alpha -1}} \left[ \Phi\left[ \sqrt{\frac{\alpha-1}{x_0^2}}(b-x_0) \right] - \Phi\left[ \sqrt{\frac{\alpha-1}{x_0^2}}(a-x_0) \right]\right]  $$
</p>
<p>We know that the exact value of this integral is:</p>
<p>
$$ \int_a^b \frac{x^{\alpha-1}}{\Gamma(\alpha)\beta^{\alpha}} e^{-\frac{x}{\beta}} dx = \frac{ {\gamma\left(\alpha,{\frac {b}{\beta }}\right)} - {\gamma\left(\alpha,{\frac {a}{\beta }}\right)} }{\Gamma(\alpha)}$$
</p>
<p>Where: $\gamma\left(\alpha,{\frac {(.)}{\beta }}\right)$ is the incomplete beta function. Let's look at the example with $\alpha = 3$ and $\beta = 4$ this leads to the maximum being attained at $x_0=8$. We will consider the the integral of the Gamma desity in the range: $[7,9]$ (in the general vicinity of the maxima) and the range $[25,27]$ (outside the general vicinity of the maxima).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Integral of the Gamma Density</span>
<span class="c1"># Exact solution using Gamma CDF</span>
<span class="c1"># Approximation using first order Laplace approximation</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">scipy.special</span>

<span class="c1"># Set parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Define function to evaluate exact integral</span>
<span class="k">def</span> <span class="nf">exact_integral</span><span class="p">(</span><span class="n">alp</span><span class="p">,</span> <span class="n">bet</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">bottom</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gamma</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">top</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">alp</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">bet</span><span class="p">)</span> <span class="o">-</span> <span class="n">gamma</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">alp</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">bet</span><span class="p">)</span>

<span class="c1"># Define function to evaluate Laplace approx. integral</span>
<span class="k">def</span> <span class="nf">laplace_integral</span><span class="p">(</span><span class="n">alp</span><span class="p">,</span> <span class="n">bet</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">bottom</span><span class="p">):</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="p">(</span><span class="n">alp</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">bet</span>
    <span class="k">return</span> <span class="n">x0</span><span class="o">**</span><span class="p">(</span><span class="n">alp</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x0</span><span class="o">/</span><span class="n">bet</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">alp</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">alp</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">x0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">top</span><span class="o">-</span><span class="n">x0</span><span class="p">))</span><span class="o">-</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">alp</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">x0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">bottom</span><span class="o">-</span><span class="n">x0</span><span class="p">)))</span> <span class="o">/</span> <span class="p">(</span><span class="n">bet</span><span class="o">**</span><span class="n">alp</span> <span class="o">*</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">alp</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Exact Integral in range [7,9]:"</span><span class="p">,</span> <span class="n">exact_integral</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Laplace Integral in range [7,9]:"</span><span class="p">,</span> <span class="n">laplace_integral</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Exact Integral in range [25,27]:"</span><span class="p">,</span> <span class="n">exact_integral</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">25</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Laplace Integral in range [25,27]:"</span><span class="p">,</span> <span class="n">laplace_integral</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">25</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Exact Integral in range [7,9]: 0.13463042839894013
Laplace Integral in range [7,9]: 0.13463370379908948


Exact Integral in range [25,27]: 0.015951556413567713
Laplace Integral in range [25,27]: 0.0008976580106864617
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the approximation is very accurate around $x_0$ but performs much worse in when further away from this value. This is worth keeping in mind when relying on a Laplace approximation procedure.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
<p>In this blog post we have looked at the basics of using Monte-Carlo to solve integration problems. We have seen how unlike "traditional" methods Monte-Carlo does not suffer from the "curse of dimensionality" in quite the same way. Further we have seen that via construction we have methods for quantifying the variance of our estimators and so have the ability to "judge" whether an estimator will have suitable performance for our needs. Further we have seen a few various techniques that we can use to reduce the variance of our estimators in order to improve performance. Finally we looked at the Laplace transform as a method of estimating complex integrals when Monte-Carlo methods do not perform well and a "rough" estimate is required quickly. The Laplace estimator can also be used as a piece of validation to see whether our Monte-Carlo integral estimators are working as we would like.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/monte-carlo-methods-4" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog about maths, probability, modelling and computing.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/lewiscoleblog" title="lewiscoleblog"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jazzcoffeestuff" title="jazzcoffeestuff"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
